var store = [{
        "title": "Beyond the Basic Stuff with Python [Introduction]",
        "excerpt":"   포스팅 계기   1) 클린 코드에 대한 고민   연구연수생 과정에서 프로젝트를 진행하며 코드 규칙에 대한 고민이 들었습니다.  학부 과정에서 팀 프로젝트를 진행할 때에는 코드의 품질보다는 구현에 집중하였고, 이 때문에 코드에 대한 기준이 없었습니다.  이 때문에 연구연수생 과정에서 코드의 품질을 높이기 위한 학습과 프로젝트 진행을 병행하다보니 처음 작성한 코드 기준과 마지막에 작성한 코드의 기준이 차이가 났습니다.  이를 겪으며 협업 과정에서 서로 규칙을 정하는 데 기준이 되는 것이 필요하다고 생각이 들었고, 우선은 나의 기준을 먼저 명확하게 세워야 다른 사람과 함께 규칙을 정하는 것이 가능할 것이라 생각이 들었습니다.   2) 개발자 북클럽 [클린 코드 이제는 파이썬이다]   채널 박재호 - https://www.youtube.com/@devjaypark   처음에는 많이 알려진 책인 Clean Code[로버트 C. 마틴]의 책을 통해 학습을 진행하려 했지만 Java 언어를 기반으로 작성된 책이라 고민이 있었습니다. 그러던 중에 유데미 큐레이션 x 책만이 진행하는 [클린 코드 이제는 파이썬이다] 개발자 북클럽에 참가하게 되었습니다. Clean Code 도서의 역자를 맡으신 박재호 개발자님이 번역한 도서이고 이번 북클럽의 멘토로 참여하여 질의응답과 학습 가이드를 제공해 주기 때문에 집중을 할 수 있을 것이라 생각하여 선택하게 되었습니다.   마치며…      블로그에 리뷰하는 내용은  [Beyond the Basic Stuff with Python]의 내용을 기반으로 작성할 예정입니다.  상세한 내용은 [클린 코드 이제는 파이썬이다] 도서를 통해 학습하기를 추천합니다.   Reference   [Beyond the Basic Stuff with Python_Al Sweigart] - [https://inventwithpython.com/beyond/  CC License - [CC BY-NC-SA 3.0]  ","categories": ["CleanCode_Python"],
        "tags": ["CleanCode_Python"],
        "url": "/cleancode_python/CleanCode_Introduction/",
        "teaser": null
      },{
        "title": "Beyond the Basic Stuff with Python [Chapter 01 ~ 02]",
        "excerpt":"   Chapter 01 - DEALING WITH ERRORS AND ASKING FOR HELP [오류 처리 및 도움 요청]   1.1_ How to Ask for Programming Help [프로그래밍 도움을 요청하는 방법]   - 도움을 요정할 때 하지 말아야 할것 11가지     1) 질문하기 전에 질문을 해도 되는지 묻는 것      온라인상에서는 즉각적으로 피드백을 하기가 어려울 수가 있다. 질문을 해도 되는지는 예의 바르게 묻는 것이지만 서로 시간의 텀이 있으므로 질문을 바로 하자  [Not Recommend]  혹시 질문 드려도 될까요?  [Recommend]  oo한 부분이 안되는 데 이 부분에 대해서 질문해도 될까요?    2) 직접적으로 질문하지 않고 질문을 암시하는 것      문제를 설명할 때 상대가 말하는 내용을 알고 있다고 가정하기 쉽다.  “코드가 작동하지 않습니다”는 질문이 무엇인지 암시할 수는 있지만 물음표로 끝나는 명시적인 질문을 포함하는 것을 추천한다.    3) 상황에 맞지 않는 포럼이나 웹 사이트에 질문 하는 것      상황에 맞는 사이트에 방문해서 질문하자    4) “문제가 있습니다” 또는 “도움을 주세요”와 같은 구체적이지 않은 게시물 제목 또는 이메일 제목 작성을 하는 것      [Example] python 3.7 : 0000 오류를 어떻게 해결할 수 있을까요?    5) “내 프로그램이 작동하지 않습니다”라고 하면서 어떤 것을 원하는지 설명 안하는 것   6) 전체 오류 메시지 포함 안하는 것   7) 전체 코드 공유를 안하는 것   8) 잘못된 포맷팅의 코드를 공유하는 것      [Wrong Example]  def add(a, b):  result=a+b  return result    9) 이미 시도한 과정을 설명하지 않는 것   10) 운영 체제 또는 버전 정보를 제공하지 않는 것   11) 다른 사용자에게 프로그램을 작성해 달라고 요청하는 것         Chapter 02 - ENVIRONMENT SETUP AND THE COMMAND LINE [환경 설정 및 명령행]   2.1_ Command Line Interface      Windows : C:\\Windows\\System32\\cmd.exe   macOS : /bin/bash   Ubuntu Linux : /bin/bash         Reference     [Beyond the Basic Stuff with Python_Al Sweigart] - https://inventwithpython.com/beyond/  [Chapter 1 - Dealing with Errors and Asking for Help] - https://inventwithpython.com/beyond/chapter1.html  [Chapter 2 - Environment Setup and the Command Line] - https://inventwithpython.com/beyond/chapter2.html  CC License - [CC BY-NC-SA 3.0]   Translator - ChatGPT    ","categories": ["CleanCode_Python"],
        "tags": ["CleanCode_Python"],
        "url": "/cleancode_python/CleanCode_week_01/",
        "teaser": null
      },{
        "title": "Beyond the Basic Stuff with Python [Chapter 03 ~ 04]",
        "excerpt":"   Chapter 3 - Code Formatting with Black [Black과 함께하는 코드 포매팅]   3.1_ Style Guides and PEP 8 [스타일 가이드 및 PEP 8]   - PEP 8 (Python Enhancement Proposal 8)      Python 핵심 개발 팀이 작성한 스타일 가이드 중 하나  Link - https://peps.python.org/pep-0008/    - PEP 8 Code Lay-out           Indentation [들여 쓰기]              들여쓰기 레벨당 4개의 공백을 사용하자            # Correct:    # 여는 구분자와 맞추어 정렬 foo = long_function_name(var_one, var_two,                          var_three, var_four)    # 함수 인자들을 나머지 코드와 구분하기 위해 4칸의 공백(들여쓰기)를 추가 def long_function_name(         var_one, var_two, var_three,         var_four):     print(var_one)    # 행을 붙이는 들여쓰기는 한 단계 더 추가 foo = long_function_name(     var_one, var_two,     var_three, var_four)           # Wrong:    # 세로 맞춤을 사용하지 않을 때 첫 번째 줄에 인수를 작성하는 것은 금지 foo = long_function_name(var_one, var_two,     var_three, var_four)    # 들여쓰기가 구별되지 않으므로 추가적인 들여쓰기가 필요 def long_function_name(     var_one, var_two, var_three,     var_four):     print(var_one)                  4칸 공백 큐직은 연속된 줄에 대해 선택 사항이다.            # 들여쓰기가 된 문단에서는 4칸이 아닌 다른 간격으로도 들여쓰기 가능 foo = long_function_name(   var_one, var_two,   var_three, var_four)                  if 문의 조건부분이 여러 줄에 걸쳐 쓰여져야 할만큼 길 경우, 2개의 문자 키워드(예: if)와 공백 하나, 그리고 “(“를 조합하면 다음 줄의 조건부문에 자연스러운 4칸의 들여쓰기가 생성된다.  그러나 이렇게 되면 if 문 안에 중첩된 들여쓰기된 코드 블록도 같은 4칸의 들여쓰기를 하게 되어 시각적으로 충돌이 발생할 수 있다.  PEP은 이러한 조건부문과 중첩된 if 문 내부의 코드 블록을 시각적으로 구분하는 방법(또는 구분하지 않는 방법)에 대해 명시적인 입장을 표하지 않는다. 이러한 상황에서 적용 가능한 대안은 다음과 같다(하지만 이에 국한되지 않는다)            # 추가 들여쓰기 없이 현재의 들여쓰기 유지 if (this_is_one_thing and     that_is_another_thing):     do_something()    # 구문 하이라이트를 지원하는 편집기에서 구분이 가능한 주석 추가. if (this_is_one_thing and     that_is_another_thing):     # 두 조건이 모두 참이므로, do_something() 할 수 있습니다.     do_something()    # 조건문의 연속된 줄에 들여쓰기를 추가합니다. if (this_is_one_thing         and that_is_another_thing):     do_something()                  여러 줄 구성에서의 } , ] , )            # 목록의 마지막 줄에서 공백이 아닌 첫 번째 문자 아래에 정렬될 수 있습니다. my_list = [     1, 2, 3,     4, 5, 6,     ] result = some_function_that_takes_arguments(     'a', 'b', 'c',     'd', 'e', 'f',     )    # 다음과 같이 여러 줄 구성을 시작하는 줄의 첫 번째 문자 아래에 정렬될 수 있습니다. my_list = [     1, 2, 3,     4, 5, 6, ] result = some_function_that_takes_arguments(     'a', 'b', 'c',     'd', 'e', 'f', )                Tabs or Spaces? [탭 또는 스페이스?]              스페이스가 선호되는 들여쓰기 방법이며 탭은 이미 탭으로 들여쓰기가 된 코드와 일관성을 유지하기 위해서만 사용해야 한다.  파이썬은 들여쓰기에 탭과 스페이스를 혼용하는 것을 허용하지 않는다.                 Maximum Line Length [최대 줄 길이]              Python 표준 라이브러리는 보수적이며, 각 줄을 79자로 제한한다. (그리고 docstrings/주석은 72자까지)  긴 줄을 감싸는 선호되는 방법은 (), [] 및 {} 안에 Python의 묵시적 줄 연속을 사용하는 것이다.  식을 괄호로 묶으면 긴 줄을 여러 줄로 나눌 수 있다. 줄 연속을 위해 백슬래시를 사용하는 것보다 우선적으로 사용해야 한다.  때때로 백슬래시는 적절할 수 있다.  예를 들어 길고 여러 개의 ‘with’-문은 Python 3.10 이전에는 암시적 연속을 사용할 수 없었으므로 백슬래시가 허용되었다.            with open('/path/to/some/file/you/want/to/read') as file_1, \\      open('/path/to/some/file/being/written', 'w') as file_2:     file_2.write(file_1.read())                Should a Line Break Before or After a Binary Operator? [이항 연산자 앞 또는 뒤에서 줄 바꿈을 해야하나요?]              수십 년 동안 권장되는 스타일은 이진 연산자 다음에 중단하는 것이었다.  그러나 이것은 두 가지 방식으로 가독성을 손상시킬 수 있다.  연산자는 화면의 여러 열에 흩어져 있는 경향이 있고 각 연산자는 피연산자에서 이전 줄로 이동된다.  여기서 눈은 어떤 항목이 더해지고 어떤 항목이 뺄 것인지를 구분하기 위해 추가 작업을 수행해야 한다.            # Wrong: # 연산자는 피연산자와 멀리 떨어져 있다. income = (gross_wages +           taxable_interest +           (dividends - qualified_dividends) -           ira_deduction -           student_loan_interest)    # Correct: # 연산자와 피연산자를 쉽게 매치할 수 있다. income = (gross_wages           + taxable_interest           + (dividends - qualified_dividends)           - ira_deduction           - student_loan_interest)                Blank Lines [공백 라인]              두 개의 빈 줄로 최상위 함수 및 클래스 정의를 둘러싼다.  클래스 내부의 메서드 정의는 하나의 빈 줄로 둘러싸여 있다.  관련 기능 그룹을 구분하기 위해 여분의 빈 줄이 (드물게) 사용될 수 있다.  관련된 한 줄짜리 묶음(예: 더미 구현 집합) 사이에 빈 줄을 생략할 수 있다.  논리 섹션을 나타내기 위해 함수에서 빈 줄을 (드물게) 사용한다.                 Source File Encoding [소스 파일 인코딩]              핵심 Python 배포판의 코드는 항상 UTF-8을 사용해야 하며 인코딩 선언이 없어야 한다.  표준 라이브러리에서 UTF-8이 아닌 인코딩은 테스트 목적으로만 사용해야 한다.  ASCII가 아닌 문자는 가급적이면 장소와 사람 이름을 표시할 때만 사용, ASCII가 아닌 문자를 데이터로 사용하는 경우 z̯̯͡a̧͎̺l̡͓̫g̹̲o̡̼̘ 및 바이트 순서 표시와 같은 잡음이 많은 유니코드 문자를 사용하지 말아야 한다.  Python 표준 라이브러리의 모든 식별자는 ASCII 전용 식별자를 사용해야 하며 가능할 때마다 영어 단어를 사용해야 한다(대부분의 경우 영어가 아닌 약어 및 기술 용어가 사용됨).         전 세계 사용자를 대상으로 하는 오픈 소스 프로젝트는 유사한 정책을 채택하도록 권장된다.                 Imports [임포트]              import는 일반적으로 별도의 줄에 있어야 한다.            # Correct: import os import sys    # Wrong: import sys, os    # Corrent: from subprocess import Popen, PIPE                  import는 다음 순서로 구분되어야 한다.                  표준 라이브러리 import         관련 서드파티 import         로컬 애플리케이션/라이브러리 import               각 import 그룹 사이에 빈줄을 넣어야 한다.                   절대 경로 가져오기가 권장된다. 가독성이 좋으며  가져오기 시스템이 잘못 구성된 경우(예: 패키지 내의 디렉터리가 ‘sys.path’로 끝나는 경우) 적어도 더 나은 오류 메시지를 제공하는) 경향이 있으므로 권장한다            import mypkg.sibling from mypkg import sibling from mypkg.sibling import example                  명시적 상대 경로 가져오기는 절대경로 가져오기의 허용 가능한 대안이며, 불필요하게 복잡한 패키지 레이아웃을 처리할 땐 절대경로 가져오기를 사용하는 것 보다 나을 수 있다.            from . import sibling from .sibling import example                  표준 라이브러리 코드는 복잡한 패키지 레이아웃을 피하고 항상 절대 가져오기를 사용해야 한다.                   클래스 포함 모듈에서 클래스를 가져올 때 일반적으로 다음과 같이 작성해도 된다.            from myclass import MyClass from foo.bar.yourclass import YourClass                  로컬 이름이 충돌하는 경우, 명시적으로 작성한다.            import myclass import foo.bar.yourclass                  와일드카드 가져오기(from &lt;module&gt; import *)는 피해야한다.  네임스페이스에 어떤 이름이 있는지 명확하지 않게 만들어 사용자와 많은 자동화된 도구 모두를 혼란스럽게 만들기 때문에 피해야 한다.  와일드 카드 가져오기를 사용할 수 있는 사례가 있는 데, 이는 공용 API의 일부로 내부 인터페이스를 다시 republish하는 것이다.                 Module Level Dunder Names [모듈 레벨 던더(Dunder) 이름]              던더는 앞뒤로 두개의 밑줄( __ )로 감싸진 이름이다. ex) __all__ , __author__ , __version__ , …  던더는 docstring 뒤에 배치하되 from __future__을 모듈을 제외하고 모든 import 구문 전에 있어햐 한다.  파이썬의 future-imports 명령은 docstrings을 제외한 다른 코드보다 앞에 위치해야 한다.            \"\"\"This is the example module.    This module does stuff. \"\"\"    from __future__ import barry_as_FLUFL    __all__ = ['a', 'b', 'c'] __version__ = '0.1' __author__ = 'Cardinal Biggles'    import os import sys                  __future__ 모듈은 파이썬 2.x 환경에서 파이썬 3.x의 특정한 기능들을 사용할 수 있게 해주는 모듈이다.            - String Quotes [문자열 따옴표]      파이썬에서는 작은 따옴표와 큰 따옴표로 둘러싸인 문자열이 동일하게 처리됨  하지만 문자열이 작은 따옴표 또는 큰 따옴표 문자를 포함하는 경우 가독성을 높이기 위해 반대의 따옴표를 사용하는 것이 좋다  docstring일 경우 PEP 257 – Docstring Conventions에 따라 큰 따옴표를 사용하는 것을 권장    - Blank [공백]   불필요한 공백을 피해야 할 상황           괄호, 대괄호 또는 중괄호 바로 안쪽:       # Correct: spam(ham[1], {eggs: 2}) # Wrong: spam( ham[ 1 ], { eggs: 2 } )           후행 쉼표와 다음 닫는 괄호 사이:     # Correct: foo = (0,) # Correct: foo = (0,)           쉼표, 세미콜론 또는 콜론 바로 앞:     # Correct: if x == 4: print(x, y); x, y = y, x # Wrong: if x == 4 : print(x , y) ; x , y = y , x                그러나 슬라이스에서 콜론은 이진 연산자처럼 작동하며 양쪽에 동일한 양이 있어야 한다. (우선 순위가 가장 낮은 연산자로 취급).  확장된 슬라이스에서는 두 콜론에 동일한 양의 간격이 적용되어야 한다.  예외: 슬라이스 매개변수가 생략되면 공백이 생략된다.       # Correct: ham[1:9], ham[1:9:3], ham[:9:3], ham[1::3], ham[1:9:] ham[lower:upper], ham[lower:upper:], ham[lower::step] ham[lower+offset : upper+offset] ham[: upper_fn(x) : step_fn(x)], ham[:: step_fn(x)] ham[lower + offset : upper + offset] # Wrong: ham[lower + offset:upper + offset] ham[1: 9], ham[1 :9], ham[1:9 :3] ham[lower : : upper] ham[ : upper]           함수 호출의 인수 목록을 시작하는 여는 괄호 바로 앞:     # Correct: spam(1) # Wrong: spam (1)           인덱싱 또는 슬라이싱을 시작하는 여는 괄호 바로 앞:     # Correct: dct['key'] = lst[index] # Wrong: dct ['key'] = lst [index]           대입(또는 기타) 연산자 주위에 두 개 이상의 공백을 두어 다른 연산자와 정렬:     # Correct: x = 1 y = 2 long_variable = 3 # Wrong: x             = 1 y             = 2 long_variable = 3           기타 권장 사항           후행 공백은 피하기     이항 연산자는 항상 양쪽에 하나의 공백으로 둘러싼다. ex) 할당(  = ), 증강 할당( +=, -= 등), 비교(==, &lt;, &gt;, !=, &lt;&gt;, &lt;=, &gt;=, in, not in, is, is not), Booleans(and, or, not)     우선 순위가 다른 연산자를 사용하는 경우 우선 순위가 가장 낮은 연산자 주위에 공백을 추가하는 것이 좋다.  그러나 공백을 두 개 이상 사용하지 말고 항상 이항 연산자의 양쪽에 같은 양의 공백을 사용      # Correct: i = i + 1 submitted += 1 x = x*2 - 1 hypot2 = x*x + y*y c = (a+b) * (a-b) # Wrong: i=i+1 submitted +=1 x = x * 2 - 1 hypot2 = x * x + y * y c = (a + b) * (a - b)      함수 주석은 콜론에 대한 일반 규칙을 사용해야 하며 화살표가 -&gt;있는 경우 항상 화살표 주변에 공백이 있어야 한다.    # Correct: def munge(input: AnyStr): ... def munge() -&gt; PosInt: ... # Wrong: def munge(input:AnyStr): ... def munge()-&gt;PosInt: ...      키워드 인수를 나타내는 데 사용되거나 주석이 없는 함수 매개 변수 =의 기본값을 나타내는 데 사용되는 경우 기호 주위에 공백을 사용하지 말기    # Correct: def complex(real, imag=0.0):     return magic(r=real, i=imag) # Wrong: def complex(real, imag = 0.0):     return magic(r = real, i = imag)      그러나 인수 주석을 기본값과 결합할 때 =부호 주위에 공백을 사용    # Correct: def munge(sep: AnyStr = None): ... def munge(input: AnyStr, sep: AnyStr = None, limit=1000): ... # Wrong: def munge(input: AnyStr=None): ... def munge(input: AnyStr, limit = 1000): ...      복합문(같은 줄에 여러 문)은 일반적으로 사용하는 것을 권장하지 않음    # Correct: if foo == 'blah':     do_blah_thing() do_one() do_two() do_three() # Wrong: if foo == 'blah': do_blah_thing() do_one(); do_two(); do_three()      경우에 따라 같은 줄에 작은 본문이 있는 if/for/while을 넣어도 되지만 다중 절 문에는 지양, 또한 긴 줄을 접는 것도 지양    # Wrong: if foo == 'blah': do_blah_thing() for x in lst: total += x while t &lt; 10: t = delay() # Wrong: if foo == 'blah': do_blah_thing() else: do_non_blah_thing()  try: something() finally: cleanup()  do_one(); do_two(); do_three(long, argument,                              list, like, this)  if foo == 'blah': one(); two(); three()   - 후행 쉼표를 사용하는 경우      후행 쉼표는 한 요소의 튜플을 만들 때 필수인 경우를 제외하고 일반적으로 선택 사항    # Correct: FILES = ('setup.cfg',) # Wrong: FILES = 'setup.cfg',      후행 쉼표가 중복되면 값, 인수 또는 가져온 항목 목록이 시간이 지남에 따라 확장될 것으로 예상되는 버전 제어 시스템을 사용할 때 종종 유용  패턴은 각 값(등)을 한 줄에 단독으로 놓고 항상 뒤에 쉼표를 추가하고 다음 줄에 닫는 괄호/대괄호/중괄호를 추가   그러나 닫는 구분 기호와 같은 줄에 후행 쉼표가 있는 것은 의미가 없다(위의 싱글톤 튜플의 경우 제외).    # Correct: FILES = [     'setup.cfg',     'tox.ini',     ] initialize(FILES,            error=True,            ) # Wrong: FILES = ['setup.cfg', 'tox.ini',] initialize(FILES, error=True,)   3.2_  Black: Code Formatter      블랙의 코드 스타일 - Link : The Black Code Style     $ pip install black으로 설치 가능  $ black yourScript.py으로 실행 가능          Chapter 4 - Choosing Understandable Names [이해 가능한 이름 선택]   4.1_ PEP 8 Naming Conventions [PEP 8의 명명 규칙]      Link - https://www.python.org/dev/peps/pep-0008/#naming-conventions            모든 문자는 ASCII 문자여야 한다. 즉, 대문자와 소문자 영어 문자에는 악센트 기호가 없어야 한다     모듈 이름은 짧아야 하고 소문자로 표기     클래스 이름은 PascalCase로 작성해야 한다     상수 변수는 대문자 SNAKE_CASE로 작성해야 한다     함수, 메소드, 변수 이름은 소문자 snake_case로 작성해야 한다     메서드의 첫 번째 인수는 항상 소문자로  self라고 이름을 지정해야 한다     클래스 메서드의 첫 번째 인수는 항상 소문자로 cls 이름을 지정해야 한다     클래스의 Private 속성은 항상 밑줄( _)로 시작     클래스의 Public 속성은 밑줄( _)로 시작해서는 안 된다      4.2 Naming Conventions [명명 규칙]           적절한 이름 길이 사용            너무 짧은 이름 지양              month, monster, monitor -&gt; mon [약식 이름 사용 지양]         start [모호한 단어 지양]                 너무 긴 이름              쓰이는 범위가 넓을수록 더 설명적인 이름이 되어야한다.         짧은 단일 함수 내부의 지역 변수 - payment         프로그램 전체에서의 전역 변수 - salesClientMonthlyPayment or annual_electric_bill_payment         상황에 맞추어 유동적으로 적용                 파이썬 내장 함수 이름 사용 금지       &gt;&gt;&gt; list(range(5)) [0, 1, 2, 3, 4] &gt;&gt;&gt; list = ['cat', 'dog', 'moose'] &gt;&gt;&gt; list(range(5)) Traceback (most recent call last):   File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;                 Reference     [Beyond the Basic Stuff with Python_Al Sweigart] - https://inventwithpython.com/beyond/  [Chapter 3 - Code Formatting with Black] - https://inventwithpython.com/beyond/chapter3.html  [Chapter 4 - Choosing Understandable Names] - https://inventwithpython.com/beyond/chapter4.html  CC License - [CC BY-NC-SA 3.0]   PEP 8 – Style Guide for Python Code - https://peps.python.org/pep-0008/  Copyright - This document has been placed in the public domain.   Translator - ChatGPT    ","categories": ["CleanCode_Python"],
        "tags": ["CleanCode_Python"],
        "url": "/cleancode_python/CleanCode_week_02/",
        "teaser": null
      },{
        "title": "Beyond the Basic Stuff with Python [Chapter 05]",
        "excerpt":"   Chapter 5 - FINDING CODE SMELLS [코드 악취 감지]      코드 악취 : 잠재적인 버그를 나타내는 소스 코드 패턴, 코드 냄새가 난다고 해서 반드시 문제가 있다는 의미는 아니지만 프로그램을 조사해야 한다는 의미     나중에 버그를 발견하고 이해하고 수정하는 것보다 버그를 방지하는 데 훨씬 적은 시간과 노력이 듭니다. 모든 프로그래머는 디버깅에만 시간을 소비한 이야기를 가지고 있습니다. 한 줄의 코드 변경과 관련된 수정 사항을 찾았습니다. 이러한 이유로 잠재적인 버그의 조그마한 냄새라도 잠시 멈추어 향후 문제를 일으키지 않는지 다시 확인하라는 메시지를 표시해야 합니다.     물론 코드 냄새가 반드시 문제가 되는 것은 아닙니다. 궁극적으로 코드 냄새를 처리할지 아니면 무시할지 여부는 사용자가 판단해야 합니다.    5.1_ Duplicate Code [중복 코드]   중복 코드 : 프로그램에 다른 코드를 복사하여 붙여넣어 생성할 수 있는 모든 소스 코드   print('Good morning!') print('How are you feeling?') feeling = input() print('I am happy to hear that you are feeling ' + feeling + '.') print('Good afternoon!') print('How are you feeling?') feeling = input() print('I am happy to hear that you are feeling ' + feeling + '.') print('Good evening!') print('How are you feeling?') feeling = input() print('I am happy to hear that you are feeling ' + feeling + '.')      중복 코드는 코드 변경을 어렵게 만들기 때문에 문제입니다. 중복 코드의 복사본 하나를 변경하면 프로그램의 모든 복사본에 변경 사항이 적용되어야 합니다. 어딘가에서 변경하는 것을 잊어버리거나 다른 복사본을 다르게 변경하면 프로그램에 버그가 생길 수 있습니다.     중복 코드에 대한 해결책은 중복을 제거하는 것    해결법 1 : 중복 코드를 함수로 이동한 다음 해당 함수를 반복해서 호출   def askFeeling():     print('How are you feeling?')     feeling = input()     print('I am happy to hear that you are feeling ' + feeling + '.')  print('Good morning!') askFeeling() print('Good afternoon!') askFeeling() print('Good evening!') askFeeling()   해결법 2 : 중복 코드를 루프로 이동   for timeOfDay in ['morning', 'afternoon', 'evening']:     print('Good ' + timeOfDay + '!')     print('How are you feeling?')     feeling = input()     print('I am happy to hear that you are feeling ' + feeling + '.')   해결법 3 : 두 기술을 결합하여 함수와 루프를 사용   def askFeeling(timeOfDay):     print('Good ' + timeOfDay + '!')     print('How are you feeling?')     feeling = input()     print('I am happy to hear that you are feeling ' + feeling + '.')  for timeOfDay in ['morning', 'afternoon', 'evening']:     askFeeling(timeOfDay)   5.2_ Magic Numbers [매직 넘버]   프로그래밍에 숫자가 포함되는 일이 있지만, 소스 코드에 나타나는 일부 숫자는 다른 프로그래머(또는 작성한 후 몇 주 후에)를 혼란스럽게 할 수 있다   # before expiration = time.time() + 604800  # after expiration = time.time() + 604800  # Expire in one week.      다음과 같이 주석을 사용하여 expiration(만료기간)이 일주일 뒤인것을 나타 낼 수 있다    # Set up constants for different time amounts: SECONDS_PER_MINUTE = 60 SECONDS_PER_HOUR   = 60 * SECONDS_PER_MINUTE SECONDS_PER_DAY    = 24 * SECONDS_PER_HOUR SECONDS_PER_WEEK   = 7  * SECONDS_PER_DAY  --snip--  expiration = time.time() + SECONDS_PER_WEEK  # Expire in one week.      다음과 같이 Magic Number를 상수로 대체하는 것이 더 좋은 해결 법이다    NUM_CARDS_IN_DECK = 53 NUM_WEEKS_IN_YEAR = 52      지금은 값이 같더라도 다르게 사용이 될 수 있다면 분리해서 선언해야 된다    - Magic number는 숫자가 아닌 값(문자열, …)등에도 적용 될 수 있다   while True:     print('Set solar panel direction:')     direction = input().lower()     if direction in ('north', 'south', 'east', 'west'):         break  print('Solar panel heading set to:', direction) if direction == 'nrth': # &lt;- 오타     print('Warning: Facing north is inefficient for this panel.')      ‘north’를 입력 받으면 경고를 출력 하려했지만 조건문에서 오타로 ‘nrth’가 입력 되었다  오타는 여전히 문법적으로 올바르기 때문에 감지하기 어려울 수 있다.    # Set up constants for each cardinal direction: NORTH = 'north' SOUTH = 'south' EAST = 'east' WEST = 'west'  while True:     print('Set solar panel direction:')     direction = input().lower()     if direction in (NORTH, SOUTH, EAST, WEST):         break  print('Solar panel heading set to:', direction) if direction == NRTH: # &lt;- 오타     print('Warning: Facing north is inefficient for this panel.')   # --------------- output --------------- # Set solar panel direction: west Solar panel heading set to: west Traceback (most recent call last):   File \"panelset.py\", line 14, in &lt;module&gt;     if direction == NRTH: NameError: name 'NRTH' is not defined      NameError 이 있는 코드 라인에서 발생한 예외는 이 프로그램을 실행할 때 버그를 명확하게 표시해준다    매직 넘버는 목적을 전달하지 않기 때문에 코드 악취이며, 코드 가독성을 떨어뜨린다. 추가로 업데이트하기 어렵게 만들고 감지할 수 없는 오타가 발생하기 쉽다. 해결 방법은 상수 변수를 대신 사용하는 것이다.   5.3_ Commented-Out Code and Dead Code [주석 코드 &amp; 죽은 코드]           주석 코드              함수나 코드 설명을 위한 주석을 하는 것은 나쁜것은 아니다. 하지만 흔히 테스트 과정에서 다른 기능을 테스트하고 싶을 때 해당 코드를 주석처리하면 나중에 쉽게 되돌릴 수 있다.  하지만 주석 처리된 코드가 그대로 남아있는 것은 좋지않다. 그렇게 되면 왜 코드에서 제거됐는지, 나중에 어떤 조건에서 다시 필요해지는 지 모르는 상태가 된다. 사용하지 않는 코드는 지우는 것을 권장한다.            create_dataframe() #preprocess_dataframe() create_new_dataframe() preprocess_dataframe()                  예시 코드지만 극단적으로 표현하자면, preprocess_dataframe()가 왜 두 번 나오는지, 그리고 뒤에 create_new_dataframe()을 호출한 뒤에 preprocess_dataframe()을 호출하는데 왜 앞에서 create_dataframe()일경우에는 preprocess_dataframe()를 호출하지 않는지 와 같은 의문이 생길 수 있다.                 죽은 코드              죽은 코드는 “도달할 수 없거나 논리적으로 실행 될 수 없는 코드”이다.            def true_or_false(input: bool):   if input == True:     return '참입니다'   else:     return '거짓입니다'   return '아무것도 아닙니다' #죽은 코드                  return '아무것도 아닙니다'는 논리적으로 실행 될 수 없는 죽은 코드이다.            - 예외              stub은 코드 악취 판단에서 예외합니다.  stub : 아직 구현되지 않은 함수나 클래스처럼 향후 코드가 작성될 위치를 나타내는 플레이스홀더이다.            # 예시 1 def preprocess_dataframe(df):   pass # 예시 2 def preprocess_dataframe(df):   raise NotImplementedError                  2가지 방법으로 표현할 수 있지만, “예시 2”번의 방식으로 사용을 한다면 의도치 않게 호출 되었을 때 에러를 표시하여 실수를 예방할 수 있습니다.            5.4_ Print Debugging [디버깅 출력]      print debugging은 코드에 임시로 print() 호출을 넣어 중간에 값들을 출력하는 것을 의마한다.  처음에는 문제가 없는 것 처럼 보일 수 있지만 print()문이 많아지다 보면 호출 일부를 제거하는 것을 잊는 다면 다시 찾아서 제거를 해야되는 문제가 있다.    import logging logging.basicConfig(filename='log_filename.txt', level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s') logging.debug('This is a log message.')      위와 같이 logging 모듈을 가져와 print() 대신 logging.debug()로 호출하면 텍스트 파일에 정보를 저장할 수 있다.  또한 로그 파일은 프로그램에서 대량의 정보를 기록 할 수 있으므로 이전에 실행된 코드와 비교도 가능하다.    5.5_ Variables with Numeric Suffixes [숫자 접미사가 있는 변수]      예를 들어 오타를 방지하기 위해 비밀번호를 2번 입력하는 경우가 있다고 할 때, password1, password2이런 변수명은 지양하고, password, confirm_password이런 방식으로 이름을 짓는 것을 권장한다.    5.6_ Classes That Should Just Be Functions or Modules [함수 또는 모듈이어야 하는 클래스]   import random class Dice:   def __init__(self, sides=6):     self.sides = sides   def roll(self):     return random.randint(1, self.sides)  d = Dice() print('You rolled a', d.roll())  &gt; You rolled a 1   print('You rolled a', random.randint(1, 6)) &gt; You rolled a 6      같은 기능을 하지만 파이썬은 다른 언어에 비해 코드를 구성하는 것이 좀 더 자유롭다.  파이썬은 함수를 그룹화하기 위해 클래스가 아닌 모듈을 사용하기 때문에 위와 같은 경우는 오히려 파이썬 코드를 복잡하게 만들 수 있다.    5.7_ List Comprehensions Within List Comprehensions [리스트 컴프리헨션 내의 리스트 컴프리헨션]      과도한 리스트 컴프리헨션은 오히려 코드 해석을 복잡하게 만들 수 있다.    nestedList = [[0, 1, 2, 3], [4], [5, 6], [7, 8, 9]] flatList = [num for sublist in nestedList for num in sublist] flatList &gt; [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]   nestedList = [[0, 1, 2, 3], [4], [5, 6], [7, 8, 9]] flatList = [] for sublist in nestedList:   for num in sublist:     flatList.append(num) flatList &gt; [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]      첫번째 코드는 한 리스프 컴르리헨션안에 for 문이 2개가 들어가면서 코드 해석이 더 어렵게 되었다.  컴프리헨션은 구문론적으로 코드를 간결하게 만들 수 있지만, 남용하면 오히려 해석을 어렵게 만든다.    5.8_ Empty except Blocks and Poor Error Messages [빈 예외처리 &amp; 부족한 에러 메세지]   try:   num = input('Enter a number: ')   num = int(num) except ValueError:   pass  Enter a number: forty two &gt;&gt;&gt; num 'forty two'      다음과 같이 pass로 구성하면 코드 자체는 계속 실행이 되지만, num이 문자열로 출력이 되어, 의도하지 않은 결과를 가져올 수 있습니다.    try:   num = input('Enter a number: ')   num = int(num) except ValueError:   print('An incorrect value was passed to int()')  Enter a number: forty two An incorrect value was passed to int()      이전 코드보다는 나아졌지만, 해당 출력만으로는 사용자가 무슨 일이 일어났고, 어떤 일을 해야할지 파악이 어렵습니다.  에러 메세지는 무슨 일이 일어났는 지, 사용자가 해결을 위해 어떤 조치를 해야하는 지가 포함되는 것을 권장합니다.    마무리      저자는 이 장에 설명된 코드 악취도 프로젝트나 개인의 기본 설정에 따라 위에 정리한 내용을 지킨다고 무조건 좋은 코드가 나오는 것은 아니라고 마무리를 했습니다.  하지만 이 챕터를 공부하면서 느낀것은 클린코드는 다른 사람이 이해하기 좋은 코드라는 의미도 있지만, 코드를 작성하는 과정에서 발생하는 실수를 방지하고 유지 보수를 수월하게 할 수 있도록 하는 과정이라고 느꼈습니다.  그래서 클린코드는 기술 부채를 줄이기 위한 중요한 내용이라고 생각하지만, 예전에 배달의 민족 CEO분의 개발에 대한 인터뷰를 보았을 때가 생각이 났었습니다.  기술 부채는 서비스가 개발이 되었기에 생기는 것이니 우선은 주어진 문제를 해결하는 것도 중요하게 생각을 한다고 했었는데, 이러한 점을 보면 클린코드에 대해 정답을 없는 것 같고 앞으로 경험을 쌓으며 균형을 맞추어야 할 것 같습니다.        Reference     [Beyond the Basic Stuff with Python_Al Sweigart] - https://inventwithpython.com/beyond/  [Chapter 5 - Code Formatting with Black] - https://inventwithpython.com/beyond/chapter5.html  CC License - [CC BY-NC-SA 3.0]   Translator - ChatGPT    ","categories": ["CleanCode_Python"],
        "tags": ["CleanCode_Python"],
        "url": "/cleancode_python/CleanCode_week_03/",
        "teaser": null
      },{
        "title": "Beyond the Basic Stuff with Python [Chapter 06]",
        "excerpt":"   Chapter 06 - WRITING PYTHONIC CODE [파이썬스러운 코드 작성히기]      개인의 관점에 따라 다르지만 python다운 코드를 작성하는 일반적인 방법을 제시합니다.    6.1_ Commonly Misused Syntax [일반적으로 잘못 사용되는 구문]           range()보다는 enumerate()를 사용하자       animals = ['cat', 'dog', 'moose'] for i in range(len(animals)):   print(i, animals[i])    &gt; 0 cat &gt; 1 dog &gt; 2 moose           # Pythonic Example animals = ['cat', 'dog', 'moose'] for i, animal in enumerate(animals):   print(i, animal)    &gt; 0 cat &gt; 1 dog &gt; 2 moose                  index를 출력하거나 사용하기 위해 range(len())을 사용하는 것보다 enumerate가 더 간단히다.                 open() 및 close() 대신 with문 사용하기       # Unpythonic Example try:   fileObj = open('spam.txt', 'w')   eggs = 42 / 0    # A zero divide error happens here.   fileObj.close()  # This line never runs.   except:     print('Some error occurred.')    &gt; Some error occurred.                  eggs에서 에러가 생기고 close()가 실행되지 않은 상태로 끝나 파일은 열린 상태로 남아 있게 되고, 파일 손상 버그가 발생하면 try블록까지 추적하기 어렵다.            # Pythonic Example with open('spam.txt', 'w') as fileObj:   fileObj.write('Hello, world!')                  다음과 같이 with문으로 작성하면, 코드를 벗어날 경우 자동으로 close()를 호출한다.                 None과 비교할 때 == 대신 is를 사용하자       class SomeClass:   def __eq__(self, other):     if other is None:       return True        spam = SomeClass()    spam == None &gt; True    spam is None &gt; False                  ==는 두 개체의 값을 비교하지만, is는 두 개체의 ID를 비교한다.  예시 코드처럼 클래스가 == 연산자를 오버로드할 가능성은 희박하지만, 만약을 대비해서 == None보다는 is None을 사용해야 한다.            6.2_ Formatting Strings [문자열 포매팅]           문자열에 \\가 많은 경우에는 원시 문자열 사용하기       # Unpythonic Example print('The file is in C:\\\\Users\\\\Al\\\\Desktop\\\\Info\\\\Archive\\\\Spam')    &gt; The file is in C:\\Users\\Al\\Desktop\\Info\\Archive\\Spam           # Pythonic Example print(r'The file is in C:\\Users\\Al\\Desktop\\Info\\Archive\\Spam')    &gt; The file is in C:\\Users\\Al\\Desktop\\Info\\Archive\\Spam                  아래의 예시와 같이 원시 문자열을 사용할 때 이스케이프 처리를 간단하게 할 수 있다.                 f-strings을 통한 문자열 포매팅              원시 문자열 처리에 r이 붙는 것처럼 f가 붙는다.            name, day, weather = 'Al', 'Sunday', 'sunny' f'Hello, {name}. Today is {day} and it is {weather}.'    &gt; 'Hello, Al. Today is Sunday and it is sunny.'           width, length = 10, 12 f'A {width} by {length} room has an area of {width * length}.'    &gt; 'A 10 by 12 room has an area of 120.'                  위의 2가지 예시와 같이 변수를 간단하게 삽입하여 출력 할 수 있다.            spam = 42 f'This prints the value in spam: {spam}' &gt; 'This prints the value in spam: 42'    f'This prints literal curly braces: ' &gt; 'This prints literal curly braces: {spam}'                  내부에 중괄호를 사용해야 할 경우에는 중괄호를 추가로 사용하면 된다.            6.3_ Making Shallow Copies of Lists [리스트 얕은 복사본 만들기]   spam = ['cat', 'dog', 'rat'] eggs = spam print(id(spam) == id(eggs)) &gt; True  eggs.append('eggs') print(eggs, spam) &gt; ['cat', 'dog', 'rat', 'eggs'] ['cat', 'dog', 'rat', 'eggs']      spam 리스트를 eggs 변수에 할당할 때, 실제로 새로운 리스트 객체를 생성하는 것이 아니라, eggs 변수도 spam 변수가 가리키고 있는 같은 리스트 객체를 참조하게 된다. 따라서 spam과 eggs는 동일한 객체를 가리키게 되므로 id(spam)과 id(eggs)의 결과가 같게 되고, 둘 중 하나의 리스트에 변화가 생기면 동일한 객체를 가리키게 되면서 연동이 된것처럼 작동하게 된다. 이처럼 의도하지 않은 변화를 예방하기 위해 리스트를 복사 할 때, 주의해야한다.    spam = ['cat', 'dog', 'rat', 'eel'] eggs = spam[:] print(id(spam) == id(eggs)) &gt; False      리스트를 다음과 같이 shallow copy(얕은 복사)를 하게 되면 서로 다른 리스트 객체를 참조하게 된다.    # Pythonic Example import copy spam = ['cat', 'dog', 'rat', 'eel'] eggs = copy.copy(spam) print(id(spam) == id(eggs)) &gt; False      다음과 같이 copy 모듈의 copy() 함수를 사용하는 [:]보다 가독성이 좋다.    6.4_ Pythonic Ways to Use Dictionaries [파이썬다운 딕셔너리 사용]           딕셔너리에서는 get()과 setdefault()를 사용       - get()       # Unpythonic Example numberOfPets = {'dogs': 2} if 'cats' in numberOfPets: # Check if 'cats' exists as a key.   print('I have', numberOfPets['cats'], 'cats.') else:   print('I have 0 cats.')    &gt; I have 0 cats.           # Pythonic Example numberOfPets = {'dogs': 2} print('I have', numberOfPets.get('cats', 0), 'cats.') &gt; I have 0 cats.                  딕셔너리에 get() 메소드로 접슨하면 키가 없을 때 지정한 기본값을  반환하도록 할 수 있다.            - setdefault()       # Unpythonic Example numberOfPets = {'dogs': 2} if 'cats' not in numberOfPets:   numberOfPets['cats'] = 0    numberOfPets['cats'] += 10 print(numberOfPets['cats']) &gt; 10           # Pythonic Example numberOfPets = {'dogs': 2} numberOfPets.setdefault('cats', 0) # Does nothing if 'cats' exists. &gt; 0 numberOfPets['cats'] += 10 print(numberOfPets['cats']) &gt; 10                  위의 조건문 대신 setdefault() 메소드를 통해 기본값을 지정할 수 있다.                 기본 값을 위해서 collections.defaultdict을 사용       import collections scores = collections.defaultdict(int)    print(scores) &gt; defaultdict(&lt;class 'int'&gt;, {})    scores['Al'] += 1 # No need to set a value for the 'Al' key first.    print(scores) &gt; defaultdict(&lt;class 'int'&gt;, {'Al': 1})    print(scores['Zophie']) # No need to set a value for the 'Zophie' key first. &gt; 0 scores['Zophie'] += 40    print(scores) &gt; defaultdict(&lt;class 'int'&gt;, {'Al': 1, 'Zophie': 40})                  collections.default 클래스를 사용하여 KeyError를 방지할 수 있다.  아래처럼 int 타입을 지정하면 초기값이 0으로 설정된다.            import collections booksReadBy = collections.defaultdict(list) booksReadBy['Al'].append('Oryx and Crake') booksReadBy['Al'].append('American Gods')    print(len(booksReadBy['Al'])) &gt; 2 print(len(booksReadBy['Zophie'])) # The default value is an empty list. &gt; 0                  다음과 같이 빈 리스트를 초기값으로 설정할 수 있다.                 switch 문 대신 딕셔너리 사용       # All of the following if and elif conditions have \"season ==\": if season == 'Winter':     holiday = 'New Year\\'s Day' elif season == 'Spring':     holiday = 'May Day' elif season == 'Summer':     holiday = 'Juneteenth' elif season == 'Fall':     holiday = 'Halloween' else:     holiday = 'Personal day off'           holiday = {'Winter': 'New Year\\'s Day',            'Spring': 'May Day',            'Summer': 'Juneteenth',            'Fall':   'Halloween'}.get(season, 'Personal day off')                  switch 문 대신 딕셔너리를 사용하여 코드를 간결하게 작성할 수 있다.  get() 메소드를 통해 key가 존재하지 않으면 'Personal day off' 를 반환한다.  하지만 switch 문 대신 딕셔너리를 사용할 경우 가독성이 떨어질 수 있어, 사용 여부는 사용자에게 달려있다.            6.5_ 변수값 작업           체이닝 할당과 비교 연산자       # Unpythonic Example if 42 &lt; spam and spam &lt; 99:    # Pythonic Example if 42 &lt; spam &lt; 99:                  파이썬에는 chaining 비교 연산자가 있기 때문에 and를 사용할 필요가 없다.            # Pythonic Example spam = eggs = bacon = 'string' print(spam, eggs, bacon) &gt; string string string    # Pythonic Example spam = eggs = bacon = 'string' print(spam == eggs == bacon == 'string') &gt; True                  = 할당 연산자도 체이닝이 가능하다. 또한 == 비교 연산자도 체이닝 형식으로 사용할 수 있다.                 변수가 여러 값 중 하나인지 확인하기       # Pythonic Example spam = 'cat' print(spam in ('cat', 'dog', 'moose')) &gt; True                  spam == 'cat' or spam == 'dog' or spam == 'moose' 같은 기능을 간단하게 구현할 수 있다.                Reference     [Beyond the Basic Stuff with Python_Al Sweigart] - https://inventwithpython.com/beyond/  [Chapter 6 - Choosing Understandable Names] - https://inventwithpython.com/beyond/chapter6.html  CC License - [CC BY-NC-SA 3.0]   Translator - ChatGPT    ","categories": ["CleanCode_Python"],
        "tags": ["CleanCode_Python"],
        "url": "/cleancode_python/CleanCode_week_04/",
        "teaser": null
      },{
        "title": "Boostcamp Week 01 [TIL]",
        "excerpt":"03.09 (목)    Coding Test Study   이진 탐색 알고리즘에 대해서 학습한 후 코드 리뷰를 하는 시간을 가졌다.   이진 탐색은 시간 복잡도가 O(log n)의 알고리즘이며, 이진 탐색 트리(BST)와 유사한 점이 많다.  하지만 이진 탐색 트리가 구조를 저장하고 탐색하는 자료구조라면 이진검색은 정렬된 배열에서 값을 찾아내는 알고리즘 자체를 의미한다.   이진 탐색 문제를 풀고 코드리뷰를 하는 방식으로 진행하였고 추가적으로는 이진 검색 알고리즘 버그에 대해 간단한 tech talk을 진행하였다.      이진 탐색 알고리즘 버그    mid = (left + right) // 2   위의 코드의 경우처럼 보통 left와 right를 더하고 결과를 반으로 나눠 가운데를 계산한다.  수학적으로는 문제가 없지만 컴퓨터의 자료형에는 한계가 있으므로 문제를 일으킬 수 있다.  예를 들어 두 개의 값의 합이 int일 경우 2^31 - 1을 넘어간다면 C, java에서는 의도하지 않는 결과나 오버플로를 발생시킨다.   mid = left + (right - left) // 2   위의 코드의 경우에는 같은 같을 반환하면서 더한 값은 항상 right보다 작으므로 오버플로의 위험이 없다.  파이썬의 경우 임의 정밀도 정수형을 지원하므로 해당 사항이 없지만, 자료형이 엄격한 언어에서는 문제를 발생 시킬 수 있다.   오늘의 회고   AI 수학에 대해 어려움을 겪고있다. 예를 들어 경사하강법의 알고리즘의 계산식 같은 부분이다.  하지만 실제로 개발 환경에서는 직접 계산하는 일이 거의 없고 라이브러리가 지원한다.  계산식을 완벽하게 이해하는 것도 좋지만 실제로 사용해보면서 어떻게 영향을 주는지 공부하는 것도 중요하다고 느꼈고,  AI 수학에서 수학에만 빠져 AI를 놓치는 일이 없도록 잘 조율해야겠다고 생각하게 되는 하루였다.   03.10 (금)     Dacon 준비   팀원들과의 피어세션에서 대회를 함께 하기로하고 우선적으로 Dacon 플랫폼에 익숙해지기 위해 간단한 연습 대회를 선정하였습니다      Link : 와인 품질(Quality) 분류 경진대회    일단 간단하게 데이터 파악을 목표로 하였기 때문에 위해 간단하게 결측값 확인을 한 후에 Sicikit-Learn의 RobustScaler 스케일러를 사용하여  정규화를 진행한 후 별도의 파라미터 수정 없이 RandomForestClassifier 모델을 사용하여 예측 모델을 생성했습니다.      sklearn.preprocessing -&gt; Link          데이터의 결측치 여부만 확인하고 학습을 시켰기 때문에 이상치의 영향이 적은 RobustScaler로 전처리하였습니다.       리더보드에서는 상위 25% 정도의 정확도를 보였지만, 주말 동안 어떻게 해야 발전 시킬 수 있을 지 생각해보아야 할 것 같습니다.   오늘의 회고   AI Math가 중심이 되는 하루였습니다.  경사하강법이나 확률론, 통계확과 같은 개념들도 중요하지만 우선 순위를 매긴다면 벡터와 행렬 개념은 확실히 학습하는 것이 중요하다는 것을 알 수 있었습니다.  예를 들어 합성곱 신경망(CNN)의 합성곱 연산이 행렬연산이기 때문입니다.  주말 동안 벡터와 행렬 개념은 확실히 학습해야겠다고 계획하게 되었습니다.  ","categories": ["TIL"],
        "tags": ["BoostCamp_AI"],
        "url": "/til/Week_01_TIL/",
        "teaser": null
      },{
        "title": "[Scikit-learn] sklearn.preprocessing",
        "excerpt":"- import sklearn.preprocessing                  종류       설명                       StandardScaler       평균과 표준편차를 이용하여 스케일링하는 방법 입력 데이터의 분포가 평균이 0이고, 표준편차가 1인 정규분포 형태로 변환 [0, 1, 2, 3, 4, 5] -&gt; [-1.46385, -0.87831, -0.29277, 0.29277, 0.87831, 1.46385] 각 feature의 분포가 동일하게 변환되므로, 모델의 학습이 더욱 안정적 력 데이터가 정규분포 형태에 가까울 때 가장 잘 작동 또한, 이상치(outlier)에 민감하게 반응                 MinMaxScaler       최솟값과 최댓값을 이용하여 스케일링 입력 데이터의 분포를 0과 1 사이로 변경 [0, 1, 2, 3, 4, 5] -&gt; [0.0, 0.17, 0.33, 0.50, 0.67, 0.83] 입력 데이터의 분포를 0과 1 사이로 매핑함으로써, 각 feature의 절대적인 크기를 동일하게 만들어준다 MinMaxScaler는 입력 데이터가 비교적 균일한 분포를 가지고 있을 때 적합 예를 들어, 이미지 데이터나 픽셀 값 등에서 주로 사용 MinMaxScaler는 이상치(outlier)에 민감하게 반응하므로, 이상치가 포함되어 있는 데이터에서는 RobustScaler나 StandardScaler와 같은 다른 스케일링 방법을 사용하는 것이 더 안정적일 수 있다.                 MaxAbsScaler       입력 데이터에서 각각의 feature(특성)에 대해 절대값이 가장 큰 값을 1로 만들고,  나머지 값들은 절대값이 최댓값에 대한 비율로 변환하는 스케일링 기법 [-10, -5, 0, 5, 10] -&gt;  [-1, -0.5, 0, 0.5, 1] 입력 데이터의 분포를 보존하면서 각 feature의 절대적인 크기를 동일하게 만들어주는 효과                 RobustScaler       입력 데이터에서 각각의 feature(특성)에 대해 중앙값(median)과 IQR(Interquartile Range)를 이용하여 스케일링하는 방법 [-10, -5, 0, 5, 10, 100] -&gt; [-0.5, -0.25, 0, 0.25, 0.5, 18.5] 중앙값과 IQR을 사용하여 스케일링하기 때문에, 이상치가 포함된 데이터에서도 안정적인 스케일링 결과를 얻을 수 있다. 중앙값과 IQR을 사용하여 스케일링하기 때문에, 이 값들이 outlier에 의해 영향을 받을 수 있다는 점을 주의                 Normalizer       L1 또는 L2 norm으로 정규화하는 기능을 제공. L1 norm은 각 데이터의 절대값을 합산한 값으로 나누어 정규화하며, L2 norm은 각 데이터의 제곱을 합산한 값의 제곱근으로 나누어 정규화 [[1, 2], [3, 4], [5, 6], [7, 8]] – 디폴트 : L2 norm –&gt; [[0.22975292 0.97324899] [0.45584231 0.89096089] [0.56630692 0.82462113] [0.62791145 0.77847621]] Normalizer 클래스는 fit 메서드를 사용하여 데이터에 맞는 정규화 파라미터를 계산하고, transform 메서드를 사용하여 데이터를 정규화합니다. 주로, 텍스트 분류나 클러스터링에서 희소 행렬(sparse matrix)을 다룰 때 사용됩니다.          ","categories": ["BoostCamp_AI"],
        "tags": ["Scikit-learn"],
        "url": "/boostcamp_ai/sklearn.preprocessing/",
        "teaser": null
      },{
        "title": "[itertools] Combinatoric iterators",
        "excerpt":"import itertools   - product() [중복 순열]   &gt;&gt;&gt; import itertools &gt;&gt;&gt; a_li = [1, 2, 3] &gt;&gt;&gt; b_li = [4, 5] &gt;&gt;&gt; list(itertools.product(a_li, b_li)) [(1, 4), (1, 5), (2, 4), (2, 5), (3, 4), (3, 5)]  &gt;&gt;&gt; ab_li = [[1, 2, 3], [4, 5]] &gt;&gt;&gt; list(itertools.product(*ab_li)) [(1, 4), (1, 5), (2, 4), (2, 5), (3, 4), (3, 5)]    - permutations() [순열]   &gt;&gt;&gt; import itertools &gt;&gt;&gt; li = [0, 1, 2] &gt;&gt;&gt; list(itertools.permutations(li, 2)) [(0, 1), (0, 2), (1, 0), (1, 2), (2, 0), (2, 1)]    - combination() [조합]   &gt;&gt;&gt; import itertools &gt;&gt;&gt; li = [0, 1, 2] &gt;&gt;&gt; list(itertools.combinations(li, 2)) [(0, 1), (0, 2), (1, 2)]    ","categories": ["Python_Module"],
        "tags": ["Python_Module"],
        "url": "/python_module/Combinatoric_iterators/",
        "teaser": null
      },{
        "title": "Boostcamp Week 02 [TIL]",
        "excerpt":"03.13 (월)     Coding Test Study   그리디 알고리즘이란 바로 눈앞의 이익만을 생각하는 알고리즘을 말한다.  대부분의 경우에는 뛰어난 결과를 반환하지 못하지만, 드물게 최적해를 보장하는 경우도 있다.  그리디 알고리즘이 잘 동작하는 문제는 탐욕 선택 속성을 같고 있는 최적 부분 구조인 문제들이다.  탐욕 선택 속성이란 앞의 선택이 이후 선택에 영향을 주지 않는 것을 말한다.  그래서 그리디 알고리즘은 선택을 다시 고려하지 않는다.   그리디 문제를 해결하는 과정에서 itertools 라이브러리의 conbinations(조합) 모듈을 사용 했는데 itertools의 Combinatoric iterators의 사용 예시를 추가로 작성하였다.      Link - Combinatoric iterators    오늘의 회고   Pytorch의 기본에 대해 학습하였다. Pytorch에는 행렬 연산을 지원해주는 모듈들이 많았고,  내가 원하는 결과를 얻기 위해 임의의 tensor를 곱하는 과정을 아는 것이 중요했다.  일단은 numpy, 행렬 부분을 확실히 학습하는 것이 중요하다고 크게 느껴졌다.       03.14 (화)     Pytorch Custom Model 제작   pytorch의 nn.module을 커스텀하여 원하는 모델을 제작하는 학습을 하였습니다.   pytorch 레퍼런스 문서를 읽으면서 함께 학습 하는 과정이라 학습량이 매우 많아서 1 ~ 2일정도 계속 열심히 해야 할것 같습니다.   오늘의 회고   모델을 구성하는 과정에서 Python Object Oriented Programming 개념이 많이 사용되어서 확실하게 학습을 하고 넘어가야 할 것 같습니다.       03.15 (수)     Pytorch Custom Model 과제 완료   pytorch의 nn.module을 커스텀하여 원하는 모델을 제작하는 과제를 완료했습니다   hook 개념이 어려움을 겪었습니다.   문제가 해결되고 생각해보니 hook자체 개념보다 oop개념을 어려워해서 어렵게 느껴지게 되었던것 같습니다.   오늘의 회고   주말에 custuom model 복습을 하며 oop부분을 정리 해야 될것 같습니다.       03.16 (목)     Custom Dataset 및 Custom DataLoader 생성 과제 완료   Dataset을 로드해서 class CustomDataset(Dataset)를 생성하는 과정을 학습하였습니다.      class CustomDataSet(Dataset):          def __init__(self, ): - 데이터의 위치나 파일명과 같은 초기화 작업을 하는 구역     def __len__(self): - Data의 최대 요소 수를 반환     def __getitem__(self, idx): - 데이터셋의 idx번째 데이터를 반환      전체적으로는 이런 구성이였습니다.   [오류 해결]           RuntimeError: DataLoader worker (pid(s) 12696) exited unexpectedly              torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=None,                             sampler=None, batch_sampler=None, num_workers=0,                             collate_fn=None, pin_memory=False, drop_last=False,                             timeout=0, worker_init_fn=None,                             multiprocessing_context=None, generator=None, *,                             prefetch_factor=None, persistent_workers=False,                             pin_memory_device='')               num_workers ( int , optional ) – 데이터 로드에 사용할 하위 프로세스 수입니다. 0데이터가 기본 프로세스에서 로드됨을 의미합니다. (기본값:0)         num_worker = 4 * num_GPU을 권장하는 것 같지만 과제를 진행하면서 colab의 환경과 맥북의 cpu차이 때문에 오류가 발생하던것이였다.         정리하자면 위의 오류는 작업자(worker)가 강제 종료되어 생긴 오류이기 때문에, num_workers 매개 변수를 낮추거나, 더 좋은 하드웨어를 사용하여 해결할 수 있다.         또한 DataLoader의 문제가 아닌 데이터 로드 과정에서 발생한 오류일 수도 있으므로, 데이터 로드 과정을 다시 확인해 보는 것도 권장된다.            오늘의 회고   과제는 길고 주어진 시간은 짧게 느꼈졌던 4일이 끝났습니다.   Custom Model과 Custom Dataset 및 Custom DataLoader 생성을 완벽하게 이해하는 데에는 짧은 시간이었던 것 같습니다.   과제를 통해 배워가면서 모르는게 더 많아 진것 같습니다. 특히 oop개념이 취약한 것을 느꼈습니다.   그 부분에 대해서 따로 코드를 가져와서 중단점 디버깅을 통해서 코드의 흐름을 파악하려고 했지만, 클래스, 객체, 상속 개념을 완벽히 이해하는 데 부족한 점이 있었고 이 때문에 흐름의 이해에 더 어려움을 겪었던 것 같습니다.   나중에 정답 코드가 제공되면 내가 작성했던 코드와 비교해보며 학습을 다시 해보아야 할 것 같습니다.   과제가 끝내고 조금이나마 여유를 찾으니 이제야 이상적인(?) TIL이 쓰이는 것 같습니다. 🥕🥕🥕       03.17(금)     한 주 정리 및 회고   이번주 과제🦆 해설시간을 오피스 아워에서 가졌습니다.   gather()에 관한 부분 설명은 이해하기 어려운 부분이 있었습니다. 2차원 개념에서는 이해가 수월했지만 3차원 이상일 경우에는 좀 어렵게 느껴지는 것 같습니다.   마지막 조교님께서 인공지능을 바텀업으로 공부하는 것도 나쁜것은 아니지만 모든 것을 바텀업으로 하기에는 너무 어렵고 시간이 많이 필요하므로, 일단은 내가 사용해보고 유의미한 성능 증가 등을 이루게 되면 어떻게 작동하는 지 공부하는 탑 다운 방식도 중요하다고 한것이 기억에 남았습니다.   블로그에 대해   과제를 진행하며 pytorch에서 발생하는 에러들을 찾아가며 해결했습니다. 이러한 해결 과정을 담은 게시판을 만드는 것도 좋을 것이라는 생각이 들었고 가까운 시일내에 오류 정리 게시판을 만들어야겠습니다 🔥🔥🔥     주말에 정리할 개념 :   OOP   hook()   squeeze()   gather()  ","categories": ["TIL"],
        "tags": ["BoostCamp_AI"],
        "url": "/til/Week_02_TIL/",
        "teaser": null
      },{
        "title": "Deep Learning's Most Important Ideas",
        "excerpt":"   Histrorical Review            2012 - AlexNet              AlexNet은 5개의 컨볼루션(convolutional) 레이어와 3개의 완전 연결(fully connected) 레이어로 구성된 딥 컨볼루션 신경망                 2013 - DQN              DQN은 신경망으로 상태(state)를 입력으로 받아서 행동(action)을 출력하는 구성으로, Replay Memory, Target Network, Experience Replay 등의 기술을 사용                 2014 - Encoder/Decoder              Encoder/Decoder는 입력 데이터를 고정 길이 벡터로 인코딩하는 인코더 신경망과 이를 디코딩하여 출력을 생성하는 디코더 신경망으로 구성됩니다. Transformer과 함께 셀프 어텐션(self-attention) 메커니즘이 활용                 2014 - Adam Optimizer              Adam Optimizer는 경사 하강법(gradient descent) 알고리즘의 한 종류로, 운동량(momentum) 방법과 적응적 학습률(adaptive learning rate) 방법을 결합한 최적화 알고리즘                 2015 - Generative Adversarial Network              GAN은 생성자(generator)와 판별자(discriminator) 신경망으로 구성됩니다. 생성자는 무작위 노이즈에서 샘플을 생성하고, 판별자는 생성된 샘플과 실제 데이터를 구분하는 이진 분류기 역할을 한다                 2015 - Residual Networks              ResNet은 특정 레이어에서 입력 데이터를 그대로 출력으로 전달하는 잔차 연결(residual connection)을 사용하여 깊은 신경망의 학습을 용이하게 한다                 2017 - Transformer              Transformer는 인코더와 디코더로 구성되며, 입력 데이터를 셀프 어텐션 메커니즘으로 인코딩하고 디코딩한다. 또한 Multi-Head Attention, Positional Encoding 등의 기술을 활용                 2018 - BERT(fine_tuned NLP models)              BERT는 Transformer 아키텍처를 사용하여 사전 훈련된 언어 모델을 파인튜닝하는 방식으로 작동. 입력 문장의 양쪽 방향의 컨텍스트를 동시에 고려하는 Bidirectional Transformer Encoder 구조를 사용                 2019 - BIG Language Models              대부분 Transformer 아키텍처를 기반으로 하며, GPT-2와 같이 인코더와 디코더가 없는 모델도 있다                 2020 - Self Supervised Learning              셀프 슈퍼바이즈드 러닝은 사전 훈련된 신경망 모델을 사용하며, 이 모델은 주어진 입력 데이터로부터 유용한 표현을 학습. 이를 위해 다양한 자연어 처리와 컴퓨터 비전 작업에서 적용되는 사전 훈련(pre-training) 기술 중 하나로, 대표적인 예로는 Masked Language Modeling(MLM), Contrastive Predictive Coding(CPC), SimCLR 등이 있다.         이러한 사전 훈련 기술들은 레이블이 없는 대규모 데이터셋을 이용하여 모델을 사전 훈련하고, 이후에 특정 작업(Task)에 맞게 파인튜닝하여 성능을 향상시킨다. 이러한 방식은 레이블링에 대한 비용 및 시간을 절약할 수 있고, 더 큰 데이터셋을 사용하여 높은 정확도를 달성할 수 있어 자연어 처리 및 컴퓨터 비전 분야에서 널리 사용되고 있다.           ","categories": ["BoostCamp_AI"],
        "tags": ["Deep Learing"],
        "url": "/boostcamp_ai/Deep_Learing's_Most_Important_Ideas/",
        "teaser": null
      },{
        "title": "Boostcamp Week 03 [TIL]",
        "excerpt":"03.20 (월)     딥러닝 기본과 최적화      Deep Learning’s Most Important Ideas -&gt; Link       Bagging vs Boosting                           Bagging       Boosting                       방법       여러 개의 동일한 모델을 생성 후 집계       여러 개의 약한 모델을 순차적으로 생성 및 가중치 조정                 목적       분산 감소       편향 감소                 모델       복잡한 모델에 적용하기 적합       단순한 모델에 적용하기 적합                 데이터       복원 추출       비복원 추출                 대표모델       Random Forest       Gradient Boosting Machine (GBM), XGBoost, LightGBM 등                 장점       Overfitting 감소, 안정적인 성능       예측 성능 향상, 다양한 알고리즘 적용 가능                 단점       모델 해석 어려움, 계산 복잡도 상승 가능       Overfitting 가능성, 계산 시간 증가           오늘의 회고   AI 지식들을 BoostCamp_AI 게시판으로 분리하였습니다. 작성한 내용이 길어지면 앞으로도 TIL게시판에서 분리할 예정이다.   기본적인 신경망 구조를 배우고 있는데, 데이터의 흐름을 이해하는 데 어려움이 있어 특히 신경써서 학습을 해야 될것 같다.       03.21 (화) ~ 03.24(금)     CNN, RNN, Generative Model   여러 네트워크에 대해서 학습을 진행하였습니다.   학습을 하면서 과제 코드를 완성시켜가면서 내가 확실히 잘 알지 못하는 채로 여러 함수들을 사용해가며 과제를 하고 있다는 점이 크게 느껴졌고, 밑바닥부터 시작하는 딥러닝, 김기현의 딥러닝 부트캠프 with 파이토치를 통해 기초 부분을 확실히 파악하고 넘어가야 할것 같다고 느꼈습니다.   그래서 Pytorch 게시판을 새로 만들었고 지금까지 학습한 내용과 책을 함께 참고하여 정리하기로 했습니다.      ","categories": ["TIL"],
        "tags": ["BoostCamp_AI"],
        "url": "/til/Week_03_TIL/",
        "teaser": null
      },{
        "title": "[Pytorch] Tensor",
        "excerpt":"- Pytorch Tensor   import torch import numpy as np          torch.Tensor : torch.FloatTensor - [-3.4 x 10^38 ~ 3.4 x 10^38]   ft = torch.Tensor([[1, 2],                    [3, 4]]) ft   tensor([[1., 2.],         [3., 4.]])          torch.LongTensor - 64-bit integer (signed) [-9223372036854775808 ~ 9223372036854775807]   lt = torch.LongTensor([[0.5, 1.5],                        [-0.5, -1.5],                        [-9223372036854775808, 9223372036854775807]]) lt   tensor([[                   0,                    1],         [                   0,                   -1],         [-9223372036854775808,  9223372036854775807]])          torch.ByteTensor - torch.uint8 [이미지 처리와 같은 분야에서, 픽셀 값이 0~255 범위에 있을 때 자주 사용]   bt = torch.ByteTensor([[-255, 255],                        [255, -255]]) bt   tensor([[  1, 255],         [255,   1]], dtype=torch.uint8)          임의 크기의 텐서 만들기   print(torch.Tensor(4,2)) print(torch.Tensor(2,4)) print(torch.Tensor(2,3,4))   tensor([[0., 0.],         [0., 0.],         [0., 0.],         [0., 0.]]) tensor([[0., 0., 0., 0.],         [0., 0., 0., 0.]]) tensor([[[0., 0., 0., 0.],          [0., 0., 0., 0.],          [0., 0., 0., 0.]],          [[0., 0., 0., 0.],          [0., 0., 0., 0.],          [0., 0., 0., 0.]]])          NumPy &lt;-&gt; Pytorch   x = np.array([[1, 2],               [3, 4]]) print(x, type(x))   [[1 2]  [3 4]] &lt;class 'numpy.ndarray'&gt;   # numpy -&gt; tensor x = torch.from_numpy(x) print(x, type(x))   tensor([[1, 2],         [3, 4]]) &lt;class 'torch.Tensor'&gt;   # tensor -&gt; numpy x = x.numpy() print(x, type(x))   [[1 2]  [3 4]] &lt;class 'numpy.ndarray'&gt;          텐서 타입 변환   # FloatTensor -&gt; LongTensor ft.long()   tensor([[1, 2],         [3, 4]])   # LongTensor -&gt; FloatTensor lt.float()   tensor([[ 0.0000e+00,  1.0000e+00],         [ 0.0000e+00, -1.0000e+00],         [-9.2234e+18,  9.2234e+18]])          텐서 크기 구하기   st = torch.Tensor([[[1,2,3],                     [4,5,6]]]) print(st.size()) print(st.shape)   torch.Size([1, 2, 3]) torch.Size([1, 2, 3])          텐서 차원 개수 구하기   print(st.dim()) print(len(st.size()))   3 3   ","categories": ["Pytorch"],
        "tags": ["Pytorch"],
        "url": "/pytorch/pytorch_tensor/",
        "teaser": null
      },{
        "title": "[Pytorch] Tensor 기본연산",
        "excerpt":"- Pytorch Tensor 기본연산   \\[a= \\begin{bmatrix}     1 &amp; 2 \\\\     3 &amp; 4 \\end{bmatrix},\\quad b= \\begin{bmatrix}     5 &amp; 6 \\\\     7 &amp; 8 \\end{bmatrix}\\]  import torch a = torch.Tensor([[1,2], [3,4]]) b = torch.Tensor([[5,6], [7,8]])           \\[a + b = \\begin{bmatrix}   1 + 5 &amp; 2 + 6\\\\   3 + 7 &amp; 4 + 8   \\end{bmatrix}\\]      print(a + b)   tensor([[ 6.,  8.],         [10., 12.]])           \\[a - b = \\begin{bmatrix}   1 - 5 &amp; 2 - 6\\\\   3 - 7 &amp; 4 - 8   \\end{bmatrix}\\]      print(a - b)   tensor([[-4., -4.],         [-4., -4.]])          \\[a * b = \\begin{bmatrix}   1 * 5 &amp; 2 * 6\\\\   3 * 7 &amp; 4 * 8   \\end{bmatrix}\\]      print(a * b) print(a.mul(b))   tensor([[ 5., 12.],         [21., 32.]]) tensor([[ 5., 12.],         [21., 32.]])          \\[a / b = \\begin{bmatrix}   1 / 5 &amp; 2 / 6\\\\   3 / 7 &amp; 4 / 8   \\end{bmatrix}\\]      print(a / b)   tensor([[0.2000, 0.3333],         [0.4286, 0.5000]])          \\[a + b = \\begin{bmatrix}   1 ^ 5 &amp; 2 ^ 6\\\\   3 ^ 7 &amp; 4 ^ 8   \\end{bmatrix}\\]      print(a ** b)   tensor([[1.0000e+00, 6.4000e+01],         [2.1870e+03, 6.5536e+04]])          \\[a == b = \\begin{bmatrix}   1 == 5 &amp; 2 == 6\\\\   3 == 7 &amp; 4 == 8   \\end{bmatrix}\\]      print(a == b)   tensor([[False, False],         [False, False]])           \\[a != b = \\begin{bmatrix}   1 != 5 &amp; 2 != 6\\\\   3 != 7 &amp; 4 != 8   \\end{bmatrix}\\]      print(a != b)   tensor([[True, True],         [True, True]])   ","categories": ["Pytorch"],
        "tags": ["Pytorch"],
        "url": "/pytorch/pytorch_tensor_%EA%B8%B0%EB%B3%B8%EC%97%B0%EC%82%B0/",
        "teaser": null
      },{
        "title": "[Pytorch] Tensor Broadcasting",
        "excerpt":"- Pytorch Tensor 브로드캐스팅     크기가 다른 두 텐서의 산술 연산    import torch   - 텐서 + 스칼라   \\[\\begin{bmatrix}     1 &amp; 2\\\\     3 &amp; 4 \\end{bmatrix} \\oplus      1  = \\begin{bmatrix}     1 &amp; 2\\\\     3 &amp; 4 \\end{bmatrix} + \\begin{bmatrix}     1 &amp; 1 \\\\     1 &amp; 1 \\end{bmatrix} = \\begin{bmatrix}     1 + 1 &amp; 2 + 1 \\\\     3 + 1 &amp; 4 + 1 \\end{bmatrix}\\]  x = torch.Tensor([[1,2],                   [3,4]]) x + 1   tensor([[2., 3.],         [4., 5.]])   - 텐서 + 벡터   \\[\\begin{bmatrix}     1 &amp; 2\\\\     3 &amp; 4 \\end{bmatrix} \\oplus \\begin{bmatrix}     1 &amp; 2 \\end{bmatrix} = \\begin{bmatrix}     1 &amp; 2\\\\     3 &amp; 4 \\end{bmatrix} + \\begin{bmatrix}     1 &amp; 2 \\\\     1 &amp; 2 \\end{bmatrix} = \\begin{bmatrix}     1 + 1 &amp; 2 + 2 \\\\     3 + 1 &amp; 4 + 2 \\end{bmatrix}\\]  x = torch.Tensor([[1,2],                   [3,4]]) y = torch.Tensor([1,2]) print(x.shape) print(y.shape) print(x + y, (x+y).shape)   torch.Size([2, 2]) torch.Size([2]) tensor([[2., 4.],         [4., 6.]]) torch.Size([2, 2])       [1, 1, 2]     [1 ,1, 2]  [      2] --&gt; [1 ,1, 2]   x = torch.Tensor([[[1,2]]]) y = torch.Tensor([1,2]) print(x.shape) print(y.shape) print(x + y, (x+y).shape)   torch.Size([1, 1, 2]) torch.Size([2]) tensor([[[2., 4.]]]) torch.Size([1, 1, 2])   - 텐서 + 텐서   \\(\\begin{bmatrix}     1 &amp; 2\\\\     3 &amp; 4 \\end{bmatrix} \\oplus \\begin{bmatrix}     1\\\\     2 \\end{bmatrix} = \\begin{bmatrix}     1 &amp; 2\\\\     3 &amp; 4 \\end{bmatrix} + \\begin{bmatrix}     1 &amp; 1 \\\\     2 &amp; 2 \\end{bmatrix} = \\begin{bmatrix}     1 + 1 &amp; 2 + 1 \\\\     3 + 2 &amp; 4 + 2 \\end{bmatrix}\\)  [2,2]     [2,2] [2,1] --&gt; [2,2]   x = torch.Tensor([[1,2],                   [3,4]]) y = torch.Tensor([[1],                   [2]]) print(x.shape) print(y.shape) print(x + y, (x+y).shape)   torch.Size([2, 2]) torch.Size([2, 1]) tensor([[2., 3.],         [5., 6.]]) torch.Size([2, 2])   - General Broadcasting Rules      Reference - NumPy.org    Image  (3d array): 256 x 256 x 3 Scale  (1d array):             3 Result (3d array): 256 x 256 x 3  A      (4d array):  8 x 1 x 6 x 1 B      (3d array):      7 x 1 x 5 Result (4d array):  8 x 7 x 6 x 5   - Broadcastable arrays   - broadcasting이 되는 경우   A      (2d array):  5 x 4 B      (1d array):      1 Result (2d array):  5 x 4  A      (2d array):  5 x 4 B      (1d array):      4 Result (2d array):  5 x 4  A      (3d array):  15 x 3 x 5 B      (3d array):  15 x 1 x 5 Result (3d array):  15 x 3 x 5  A      (3d array):  15 x 3 x 5 B      (2d array):       3 x 5 Result (3d array):  15 x 3 x 5  A      (3d array):  15 x 3 x 5 B      (2d array):       3 x 1 Result (3d array):  15 x 3 x 5   - broadcasting이 안되는 경우   A      (1d array):  3 B      (1d array):  4 # trailing dimensions do not match  A      (2d array):      2 x 1 B      (3d array):  8 x 4 x 3 # second from last dimensions mismatched  ","categories": ["Pytorch"],
        "tags": ["Pytorch"],
        "url": "/pytorch/pytorch_tensor_%EB%B8%8C%EB%A1%9C%EB%93%9C%EC%BA%90%EC%8A%A4%ED%8C%85/",
        "teaser": null
      },{
        "title": "[Python Library] nbconvert - ipynb파일을 markdown으로 변환하는 방법",
        "excerpt":"- nbconvert 설치      Installation - https://nbconvert.readthedocs.io/en/latest/install.html    pip install nbconvert  # OR  conda install nbconvert   - Using as a command line tool      Using as a command line tool - https://nbconvert.readthedocs.io/en/latest/usage.html    $ jupyter nbconvert --to FORMAT notebook.ipynb      FORMAT          HTML     LaTeX     PDF     WebPDF     Reveal.js HTML slideshow     Markdown     Ascii     reStructuredText     executable script     notebook         Markdown 외에 pdf, webpdf 등은 추가적인 확장 설치가 필요할 수 있다  참고 링크 - [nbconvert Installation]    - *.ipynb -&gt; *.md   # 변환할 파일의 위치에서 실행 jupyter nbconvert --to markdown notebook.ipynb   ","categories": ["DevTools"],
        "tags": ["DevTools"],
        "url": "/devtools/nbconvert/",
        "teaser": null
      },{
        "title": "Boostcamp Week 04 [TIL]",
        "excerpt":"03.27 (월)     추천 시스템 Basic   추천 시스템의 기본에 대해서 학습 했습니다.  Precision@K, Recall@K, AP@K, MAP@K, NDCG와 같은 여러 지표를 학습하였습니다.  추천 시스템을 공부하면서도 계속 사용될 개념이라 학습 정리를 블로그에 작성하기로 계획을 하였습니다.       03.28 (화)     생활코딩[이고잉]님의 Git특강   Github 사용하는 방법에 대한 특강이 있었습니다.  이 블로그를 만들 때에도 Github를 기반으로 데이터를 관리하기 때문에 예전 녹화 강의를 본적이 있는 데,  실시간 강의를 들을 수 있어서 좋았습니다.       03.29(수)     Collaborative Filtering                  Aspect       Description                       정의       협업 필터링은 유사한 사용자의 행동을 분석하여 사용자의 선호도를 예측하는 기술                 Types       사용자 기반 협업 필터링 및 항목 기반 협업 필터링                 User-based CF       사용자와 유사한 다른 사용자의 선호도를 기반으로 사용자에게 항목을 추천                 Item-based CF       과거에 평가한 항목의 유사성을 기반으로 사용자에게 항목을 추천                 장점       개인화된 추천을 제공할 수 있으며, 신규 및 비인기 항목에 효과적이며 콘텐츠 정보가 필요하지 않다                 단점       Cold-start problem, 희소성 및 확장성 문제                 Data required       평가, 리뷰 및 구매 내역과 같은 사용자 항목 상호 작용 데이터                 Algorithm       협업 필터링 알고리즘은 유사성 측정을 사용하여 유사한 사용자 또는 항목을 식별               03.30(목)     MBCF                  측면       설명                       정의       기계 학습 알고리즘을 사용하여 사용자-항목 상호 작용 모델을 학습하는 추천 기술                 모델 유형       Matrix factorization, Bayesian personalized ranking, and neural networks.                 장점       Sparsity / Scalability 문제 개선 크고 희소한 데이터 세트를 처리할 수 있고 제한된 사용자 데이터로도 정확한 예측을 할 수 있으며 암시적 및 명시적 피드백과 같은 다양한 유형의 데이터를 처리 이웃 기반 CF의 경우 공통의 유저/아이템을 많이 공유해야 유사도 값이 정확해지지만 이러한 Limited Coverage를 극복                 단점       다른 협업 필터링 기술보다 더 많은 계산 리소스가 필요하며 적절한 모델을 선택하고 최적화하려면 전문 지식이 필요              Explicit feedback      평점, 별점 등 item에 대한 user의 선호도를 직접적으로 알수 있는 문제       Implict feedback      클릭 여부, 시청 여부 등 item에 대한 user 선호도를 간접적으로 알 수 있는 데이터     1(positive)을 원소로 가지는 행렬로  표현가능    Singular Value Decomposition(SVD)                  Aspect       Description                       Definition       사용자 항목 등급 매트릭스를 3개의 매트릭스로 분해하기 위해 협업 필터링에 사용되는 매트릭스 분해 기술                 Matrix decomposition       U, S 및 V 행렬 여기서 U는 user factors, S는 singular values, V는 item factors을 나타냄                 Prediction formula       U, S 및 V 행렬의 내적을 통해 unknown ratings을 추정                 장점       크고 sparse한 데이터 세트를 처리하는 데 효과적이며 누락된 등급을 처리하고 정확한 권장 사항을 제공할 수 있다.                 단점       분해 하려는 행렬의 knowledge가 불완전할 때 정의 안됨 정확하지 않은 Imputation은 데이터를 왜곡 시키고 성능을 떨어뜨린다 행렬의 entry가 적을때 과적합 되기 쉬움 -&gt; MF 등장           Matrix Factorization(MF)      정의  User-Item 행렬을 저차원의 User와 Item의 latent factor 행렬의 곱으로 분해  SVD와 유사하지만 관측된 선호도(평점)만 모델링에 활용    \\[Rating Matrix를 P와 Q로 분해하여 R과 최대한 유사하게 \\hat{R}을 추론 \\\\ R \\approx P * Q^T = \\hat{R}\\]    Alternative Least Square (ALS)           정의  Implict Feedback 데이터에 적합하도록 MF 기반 모델을 설계하여 성능을 향상            Basic Concept  User와 Item 매트릭스를 번갈아 가면서 업데이트  두 매트릭스 중 하나를 상수로 놓고 나머지 매트릭스를 업데이트  p_u, q_i 가운데 하나를 고정하고 다른 하나로 least-square문제를 해결           03.31(금)     Item2Vec / ANN   Recommender Systerm with Deep Learning      DL / MLP / AE   ","categories": ["TIL"],
        "tags": ["BoostCamp_AI"],
        "url": "/til/Week_04_TIL/",
        "teaser": null
      },{
        "title": "Boostcamp Week 05 [TIL]",
        "excerpt":"04.03 (월)     Recommender System with GNN                  Feature       GNN       NGCF       LightGCN                       Basic Concept       Neural networks operating on graph data       Collaborative filtering using graph neural networks       Simplified version of NGCF                 Primary Use Cases       Node classification, link prediction, graph classification       추천 시스템, 사용자 항목 상호 작용 예측       추천 시스템, 사용자 항목 상호 작용 예측                 Key Components       Graph convolutions, aggregating neighbor features       Graph convolutions, 유저와 아이템의 임베딩, 상호작용 모델링       Graph convolutions, embedding propagation                 Complexity       아키텍처에 따라 다름       LightGCN에 비해 더 높은 복잡성       NGCF에 비해 복잡성 감소                 Interpretability       보통, 아키텍처에 따라 다름       여러 구성 요소로 인해 보통       단순화로 인한 높은 해석성           Recommender System with RNN                  기능       GRU4Rec                       기본 개념       세션 기반 추천을 위한 GRU(Gated Recurrent Unit) 기반 모델                 주요 사용 사례       특히 세션 기반 추천을 위한 추천 시스템                 주요 구성 요소       GRU(Gated Recurrent Unit), 세션 병렬 미니 배치 학습, 순위 손실 함수                 복잡성       다른 RNN에 비해 중간 정도의 복잡성                 해석 가능성       GRU의 특성으로 인한 중간 정도의 해석 가능성                 세션 시퀸스 처리       추천 시스템에서 세션 기반 시퀀스를 처리하도록 설계               04.04 (화)     생활코딩[이고잉]님의 Git특강   Github를 사용하는 방법에 대해서 2번째 특강이 있었습니다.   간단하게 명령어를 정리하였습니다.      링크 : [GitHub] Command summary        04.05 (수) ~ 04.06 (목)     Factorization Machine(FM) &amp; Field-aware Factorization Machine(FFM)                  Feature       Factorization Machine (FM)       Field-aware Factorization Machine (FFM)                       Objective       Model pairwise feature interactions with lower complexity       Model pairwise feature interactions considering field information                 Embedding dimension       모든 기능에 대한 전역       각 피쳐의 필드별                 임베딩 수       기능당 1개       (필드 수 - 1) 기능당                 모델 복잡성       Lower       Higher                 해석 가능성       Moderate       Lower                 Pairwise interaction       All features       Features from different fields                 계산 복잡성       O(k * n)       O(k * n * f)                 매개변수 크기       O(k * n)       O(k * n * f)                 Latent factor vectors       모든 필드에서 공유       Unique for each field                 적합한 경우       적은 필드의 Sparse 데이터       더 많은 필드와 복잡한 상호 작용이 있는 Sparse 데이터                 교육 시간       Faster       Slower (due to increased complexity)                   FM 수식     \\[y(x) = w_0 + \\sum_{i=1}^{n}(w_ix_i) + \\sum_{i=1}^{n}\\sum_{j=i+1}^{n}&lt;v_i,v_j&gt;x_ix_j\\]         FMM 수식     \\[y(x) = w_0 + \\sum_{i=1}^{n}(w_ix_i) + \\sum_{i=1}^{n}\\sum_{j=i+1}^{n}&lt;v_{i,{f_j}},v_{j,{f_i}}&gt;x_ix_j\\]       04.07(금)     오피스 아워      torch.mm과 torch.matmul의 차이                  Property       torch.mm       torch.matmul                       기능       행렬 곱셈만 수행       행렬, 벡터 및 스칼라 곱셈 수행                 Input Types       2D tensors only       Tensors with any dimensions                 Broadcasting       Not supported       Supported                 Output       Always 2D tensor       Output can be tensor with any dimensions depending on input                 Use case       2D 텐서로 엄격한 행렬 곱셈을 수행하려는 경우       입력 차원, 브로드캐스팅 또는 스칼라 작업에 더 많은 유연성이 필요한 경우          ","categories": ["TIL"],
        "tags": ["BoostCamp_AI"],
        "url": "/til/Week_05_TIL/",
        "teaser": null
      },{
        "title": "[GitHub] Command summary",
        "excerpt":"- Git 명령어 정리   # GitHub에서 로컬 머신으로 리포지토리를 복제 git clone [리포지토리 URL] # 로컬 저장소의 현재 상태 출력 git status # 커밋된 변경 사항을 GitHub의 원격 저장소로 푸시 git push # 원격 저장소에서 변경 사항을 가져와 로컬 저장소에 병합 git pull # 지정된 분기를 현재 분기에 병합 git merge [분기 이름]   # 로그 보기 git log --oneline --all #그래프로 로그 보기 git log --oneline --all --graph # 'git log --online --all --graph'를 단축키로 가능하게 함 git config --global alias.l \"log --oneline --all --graph\"  # commit하지 않은 파일이 있는지 확인 git status # add : untracked -&gt; tracked [스테이징 영역에 파일을 추가] git add *  # 깃 커밋 -m은 뒤에 커밋 메세지 작성을 의미 [커밋 메세지로 새 커밋 생성] git commit -m \"[커밋 메시지]\" # a 옵션은 tracked 상태의 파일만 adding 해준다 git commit -am \"*\" # 마지막으로 커밋했던 메세지 이름을 *으로 변경 [커밋 id는 변경됨 -&gt; 복원 가능] git commit --amend -m *  # .gitignore에 무시하고자 하는 파일을 추가, 파일이 status에 더 이상 나타나지 않는다. .gitignore 파일   # checkout = HEAD를 변경 [지정된 분기로 전환] git checkout [분기 이름] # reset = HEAD의 Branch를 변경 [commit을 취소하고 돌아감\\ git reset --hard [commit_ID]  # 지금까지 했던 로그들이 궁금할 때 git reflog  # 저장소의 모든 분기 목록을 표시 git branch # Git에서 새 분기를 만들기 git branch [branch-name] # 지정된 이름으로 새 분기가 생성되고 해당 분기로 전환 git checkout -b [branch-name]  # 현재 작업하고 있는 로컬 브랜치를 원격 저장소에 푸시하는 명령어 git push --set-upstream origin [branch name]   ","categories": ["BoostCamp_AI"],
        "tags": ["GitHub"],
        "url": "/boostcamp_ai/GitHub_Command_Summary/",
        "teaser": null
      },{
        "title": "Boostcamp Week 06 ~ 07 [TIL]",
        "excerpt":"Week 06 ~ 07     Book Rating Prediction [AI_stages]   사용자의 책 평점 데이터를 바탕으로 사용자가 어떤 책을 더 선호할지 예측하는 태스크입니다.   #부스트캠프5기 #추천시스템      - 소개     일반적으로 책 한 권은 원고지 기준 800~1000매 정도 되는 분량을 가지고 있습니다.     뉴스기사나 짧은 러닝 타임의 동영상처럼 간결하게 콘텐츠를 즐길 수 있는 ‘숏폼 콘텐츠’는 소비자들이 부담 없이 쉽게 선택할 수 있지만, 책 한권을 모두 읽기 위해서는 보다 긴 물리적인 시간이 필요합니다. 또한 소비자 입장에서는 제목, 저자, 표지, 카테고리 등 한정된 정보로 각자가 콘텐츠를 유추하고 구매 유무를 결정해야 하기 때문에 상대적으로 선택에 더욱 신중을 가하게 됩니다.     해당 경진대회는 이러한 소비자들의 책 구매 결정에 대한 도움을 주기 위한 개인화된 상품 추천 대회입니다.    2주간 대회를 진행하였습니다. TIL 형식보다는 대회를 기록한 전체 report를 작성하는 방식을 새롭게 적용 해볼까라는 생각에 2주간의 과정을 담은 Wrap Report로 대체합니다      링크 - [Wrap_up] 도메인 기초 대회]   ","categories": ["TIL"],
        "tags": ["BoostCamp_AI"],
        "url": "/til/Week_06_07_TIL/",
        "teaser": null
      },{
        "title": "[linux] tmux - terminal multiplexer",
        "excerpt":"- tmux [terminal multiplexer]      tmux는 터미널 다중 작업을 지원하는 유틸리티          여러 개의 윈도우(window)를 생성하여, 하나의 세션(session)에서 각각의 작업 수행 가능     각 윈도우(window)는 여러 개의 팬(pane)으로 분할하여 하나의 윈도우에서 여러 작업 병행 가능     tmux는 백그라운드(background)에서 실행되기 때문에, 세션이 종료되더라도 작업 내용 유지     tmux는 복수의 터미널을 연결할 수 있다. 이를 통해, 여러 개의 터미널에서 동시에 하나의 세션 사용 가능       SSH 접속을 이용하여 원격 서버에서 작업을 수행하는 경우에, tmux를 사용하면 세션을 분리하고 백그라운드에서 작업을 유지할 수 있으므로, 안정적인 작업환경을 제공한다    tmux 설치                  Ubuntu나 Debian에서 apt-get을 이용하여 tmux를 설치           sudo apt-get update   sudo apt-get install tmux                        macOS에서 Homebrew를 이용하여 tmux를 설치         brew install tmux                   tmux 명령어                  명령어       설명                       tmux new -s [session-name]       새로운 세션 생성                 tmux ls       생성된 세션 목록 조회                 tmux attach-session -t [session-name]       기존 세션에 연결                 tmux switch -t [session-name]       기존 세션으로 이동                 tmux detach       세션에서 분리                 tmux kill-session -t [session-name]       세션 종료                 tmux new-window       새로운 윈도우 생성                 tmux select-window -t [window-number]       특정 윈도우 선택                 tmux split-window       현재 윈도우 분할                 tmux select-pane -[L,R,U,D]       분할된 윈도우 팬 선택                 tmux resize-pane -[L,R,U,D] [size]       선택한 팬 크기 조절                 tmux list-keys       현재 키 바인딩 목록 조회                 tmux list-commands       사용 가능한 명령어 목록 조회                 tmux source-file [config-file]       tmux 설정 파일 적용           tmux 단축키                  단축키       설명                       Ctrl+b c       새로운 윈도우 생성                 Ctrl+b w       윈도우 목록 조회                 Ctrl+b n       다음 윈도우로 이동                 Ctrl+b p       이전 윈도우로 이동                 Ctrl+b d       현재 세션에서 분리(detach)                 Ctrl+b [       스크롤 모드 진입                 Ctrl+b %       현재 팬을 수직으로 분할                 Ctrl+b \"       현재 팬을 수평으로 분할                 Ctrl+b z       현재 팬을 전체 화면으로 확장(zoom)                 Ctrl+b ,       현재 윈도우의 이름 변경                 Ctrl+b &amp;       현재 윈도우 닫기                 Ctrl+b o       다음 팬으로 이동                 Ctrl+b ;       마지막으로 사용한 팬으로 이동                 Ctrl+b x       현재 팬 닫기                 Ctrl+b !       현재 팬을 새로운 윈도우로 분리                 Ctrl+b :       tmux 명령 입력 모드 진입          ","categories": ["DevTools"],
        "tags": ["DevTools"],
        "url": "/devtools/tmux/",
        "teaser": null
      },{
        "title": "[Jupyter Notebook] Jupyter Notebook - Shortcuts",
        "excerpt":"Jupyter Notebook 단축키                  단축키       설명                       Shift + Enter       현재 셀 실행 및 다음 셀 선택                 Ctrl + Enter       현재 셀 실행                 Alt + Enter       현재 셀 실행 및 아래에 새로운 셀 삽입                 Esc       셀 편집 모드에서 노트북 모드로 변경                 Enter       노트북 모드에서 셀 편집 모드로 변경                 Ctrl + S       노트북 저장                 A       새로운 셀 위에 셀 추가                 B       새로운 셀 아래에 셀 추가                 D + D       셀 삭제                 X       셀 잘라내기                 C       셀 복사하기                 V       셀 붙여넣기                 Z       실행 취소                 Shift + Z       다시 실행                 Y       셀 타입을 코드 타입으로 변경                 M       셀 타입을 마크다운 타입으로 변경                 Ctrl + Shift + -       현재 셀을 분할                 Shift + M       선택된 셀 병합                 Shift + Tab       함수 설명 보기                 Ctrl + Shift + P       명령어 팔레트 열기          ","categories": ["DevTools"],
        "tags": ["DevTools"],
        "url": "/devtools/Jupyter_Notebook_Shortcuts/",
        "teaser": null
      },{
        "title": "[Python Library] pickle - 파이썬 객체 직렬화, 역직렬화 라이브러리",
        "excerpt":"- pickle [.pkl]   파이썬 객체 직렬화, 역직렬화 라이브러리                  특징         pickle 라이브러리는 파이썬 객체를 파일에 저장하여 다른 시스템에서 사용할 수 있게 한다  pickle 라이브러리를 이용하여 파일에 저장할 수 있는 객체는 파이썬의 모든 객체  리스트, 딕셔너리, 클래스 인스턴스, 함수, 모듈 등을 모두 저장할 수 있다                  주의점         pickle 라이브러리로 저장된 파일은 파이썬 외부에서는 읽을 수 없고, 저장된 객체의 버전이 바뀌면 역직렬화가 실패할 수 있다  따라서 pickle 라이브러리를 이용하여 파일에 저장할 때는 파일의 안정성과 객체의 버전 관리를 고려하여 사용해야 한다            - Save   import pickle  # 직렬화할 파이썬 객체 생성 person = {\"name\": \"Alice\", \"age\": 25}  # 직렬화한 객체를 저장할 파일 생성 with open(\"person.pkl\", \"wb\") as f:     pickle.dump(person, f)   - Load   import pickle  # 역직렬화할 파일을 연다. with open(\"person.pkl\", \"rb\") as f:     person = pickle.load(f)  # 역직렬화된 파이썬 객체 출력 print(person)   - Example      머신러닝 학습 모델도 저장 가능    import pickle from sklearn.linear_model import LogisticRegression  # 모델 학습 X = [[1, 2], [3, 4], [5, 6]] y = [0, 0, 1] model = LogisticRegression().fit(X, y)  # 모델 저장 with open('logistic_regression.pkl', 'wb') as f:     pickle.dump(model, f)  # 모델 불러오기 with open('logistic_regression.pkl', 'rb') as f:     model = pickle.load(f)  # 새로운 데이터로 예측 X_new = [[7, 8], [9, 10]] y_new = model.predict(X_new) print(y_new)  ","categories": ["DevTools"],
        "tags": ["DevTools","Python Library"],
        "url": "/devtools/Pickle/",
        "teaser": null
      },{
        "title": "[Wrap_up] 도메인 기초 대회",
        "excerpt":"- [Wrap_up] 도메인 기초 대회      - 개발 환경  OS : ubuntu 18.04.5 LTS  GPU : Tesla V100-SXM2-32GB    cuda : 11.0.0     - version                          python: 3.8.5                                           torch: 1.7.1 numpy: 1.20.3 pandas: 2.0.0 scikit-learn: 1.2.2 matplotlib: 3.7.1         CatBoost: 1.1.1                        - base  base model : ‘FM’, ‘FFM’, ‘NCF’, ‘WDN’, ‘DCN’, ‘CNN_FM’, ‘DeepCoNN’  base loss_fn : ‘MSE’, ‘RMSE’  base optimizer : ‘SGD’, ‘ADAM’     - 추가로 적용 해본것  model : ‘CatBoost’  loss_fn : ‘Huber Loss[pytorch : SmoothL1Loss()]’  optimizer : ‘Adagrad’    01_ 데이터 schema           [train, test]_rating.csv : train_rating.csv                                              user_id           isbn           rating                                           0           8           0002136721           2                             1           12           0060154323           9                             2           234           0374255432           8                             3           345           0399573326           4                             4           4567           0425125963           6                                users.csv                                              user_id           location           age                                           0           8           timmins, ontario, canada           nan                             1           12           ottawa, ontario, canada           49                             2           234           n/a, n/a, n/a           nan                             3           345           toronto, ontario, canada           30                             4           4567           victoria, british, canada           36                                books.csv                                              isbn           book_title           book_author           year_of_publication           publisher           img_url           language           category           summary           img_path                                           0           0002…           book_a           auth_a           2000           pub_a           …           en           [‘Actresses’]           …           …                             1           0060…           book_b           auth_b           1991           pub_b           …           en           [‘Fiction’]           …           …                             2           0374…           book_c           auth_c           1999           pub_c           …           en           [‘Medical’]           …           …                             3           0399…           book_d           auth_d           1991           pub_d           …           en           [‘Fiction’]           …           …                             4           0425…           book_e           auth_e           2000           pub_e           …           en           [‘History’]           …           …                                  *_rating.csv를 기반으로 user_id -&gt; user.csv, isbn -&gt; books.csv를 기반으로 train/test데이터를 구성한다                 Example - train data [context_data]       - drop columns : img_url, impg_path , year_of_publication, summary       - ‘location’ : location.split() -&gt; location_city, location_state, location_country                                              user_id           isbn           age           location_city           location_state           location_country           category           publisher           language           book_author           rating                                           0           8           0002…           nan           timmins           ontario           canada           [‘Actresses’]           pub_a           en           auth_a           2                             1           12           0060…           49           ottawa           ontario           canada           [‘Fiction’]           pub_b           en           auth_b           9                             2           234           0374…           nan           n/a           n/a           n/a           [‘Medical’]           pub_c           en           auth_c           8                             3           345           0399…           30           toronto           ontario           canada           [‘Fiction’]           pub_d           en           auth_d           4                             4           4567           0425…           36           victoria           british           canada           [‘History’]           pub_e           en           auth_e           6                               02_ EDA   02_01 결측치 확인                  users.csv       train_ratings.csv       books.csv       test_ratings.csv                       [ data_size: 68092 ]       [ data_size: 306795 ]       [ data_size: 149570 ]       [ data_size: 76699 ]                 user_id         0 location        0 age         27833 dtype: int64       user_id    0 isbn       0 rating     0 dtype: int64       isbn                       0 book_title                 0 book_author                1 year_of_publication        0 publisher                  0 img_url                    0 language               67227 category               68851 summary                67227 img_path                   0 dtype: int64       user_id    0 isbn       0 rating     0 dtype: int64              users data는 68092, books data는 149570개 rating data는 306795개 존재         users, books data 대비 rating data 부족     딥러닝 기반 모델에 학습데이터로 사용했을 경우 성능 개선이 어려울 것이라 예상        02_02 users.csv           age 전처리       def age_map(x: int) -&gt; int:   x = int(x)   if x &lt; 20:     return 1   elif x &gt;= 20 and x &lt; 30:     return 2   ...   else:     return 6    ... train_df['age'] = train_df['age'].fillna(int(train_df['age'].mean())) train_df['age'] = train_df['age'].apply(age_map) test_df['age'] = test_df['age'].fillna(int(test_df['age'].mean())) test_df['age'] = test_df['age'].apply(age_map) ...                  train_df와 test_df의 평균 age로 각각 결측치를 처리한 다음 age_map함수를 통해 age를 범주화 시킨다                 location 전처리              location을 city, state, country로 분리         users['location_city'] = users['location'].apply(lambda x: x.split(',')[0]) users['location_state'] = users['location'].apply(lambda x: x.split(',')[1]) users['location_country'] = users['location'].apply(lambda x: x.split(',')[2])               분리시 결측치 생성됨   `toronto,n/a,canada -&gt; location_city: toronto, location_state: n/a, location_country: canada         결측치가 없는 국가들을 데이터프레임으로 추출하여 결측치와 비교 후 데이터 입력                             채우는 순서는 location_city, location_state, location_country순으로 우선하여 채움  [상향식으로 채우는 것이 정확도가 더 높다. - city가 주어질 경우 city에서 country를 맞추는 것이 country에서 city를 맞추는 것보다 더 정확하다]                              1번 규칙을 지키며 우선 적으로 결측치가 1개인 행들을 채우고 결측치가 2개인 행들을 채운다                               02_03 train_rating.csv / test_rating.csv,         heavy users를 보았을 떄, train과 test data 구성이 유사한 것으로 파악    02_04 books.csv      books_preprocessing 함수 추가              language [book_language_change]              isbn으로 국가를 파악         Reference : List of ISBN registration groups         전처리를 위해 isbn 앞자리를 기준으로 같은 앞자리를 가진 book들의 language의 값의 개수를 측정하여 가장 많은 값을 가진 언어를 대표로 딕셔너리를 생성하여 결측치의 language를 채움         books_isbn_language = {'00': 'en', '03': 'en', '04': 'en', '06': 'en', '07': 'en', '08': 'en', '15': 'en', ...                  publisher [publisher_change]              isbn으로 출판사 파악         4번째 자리까지 동일시 같은 출판사로 통일 (대략적으로 4자리까지 동일한 경우 같은 출판사로 파악)         같은 출판사를 약간씩 다르게 표현한 경우가 많아 publisher를 새롭게 전처리         book_pub = {'0002': 'HarperFlamingo Canada', '0060': 'HarperPerennial', '0374': 'Farrar Straus Giroux', '0399': 'Putnam Pub Group', '0425': 'Berkley Publishing Group', '0671': 'Audioworks', ...                 category [category_change, category_fill]                                  category_change             상위 50개의 카테고리를 추출하여 역순으로 비교하여 상위 카테고리 리스트에 값이 있을 경우 해당 카테고리로 값을 변경하고, 같은 카테고리를 가진 항목이 50이하인 책들의 카테고리를 others로 변경             for category in categories:     books.loc[books['category'].str.contains(category, case=False), 'category'] = category        others_list = category_df[category_df['count']&lt;50]['category'].values books.loc[books[books['category'].isin(others_list)].index, 'category']='others'                                        category_fill             book_author를 기준으로 해당 author가 가장 많이 작성한 장르의 책을 대표 category로 dict를 생성             name_dict = {'Stephen King': 'fiction', 'Agatha Christie': 'fiction', 'William Shakespeare': 'drama', 'Barbara Cartland': 'fiction',...             해당 dict를 기반으로 결측치를 채우고 채워지지 않는 결측치들을 other로 처리                            03_ BaseModel predict 분포      전처리, parameter 변경 없는 기본 모델  predict 데이터 분포와 public test RMSE 결과       - 모델별 특징      FM - context_data를 메인으로 학습            FM은 모든 정보를 one-hot encoding 한 뒤 Latent Factor Model을 이용해 변수를 축약                 FFM - context_data를 메인으로 학습              feature마다 자신이 속한 필드 이외에 여러 개의 잠재 벡터를 가짐  Latent Factor 구성 시 대응되는 field를 미리 고려하여, 대응되는 field와만 상호작용을 계산  FM과 다르게 필드별로 여러 개의 잠재 벡터를 가짐                 NCF, WDN, DCN - dl_data를 메인으로 학습 _ traindata : user_id, isbn으로 구성                                               모델             특징                                                     NCF             MLP를 이용해 선형조합보다 더 다양한 정보를 학습할 수 있게 설계                                   WDN             DNN과 동시에 WNN을 학습하여 두 네트워크 구조의 장점을 가지는 방법 sparse, dense 두가지 대표 특성을 학습                                   DCN             변수의 상호작용을 자동적, 효율적으로 가능하게 하는 구조 제안 총 깊이와 교차 변수들의 차수가 커질수록 명시적 변수들의 상호관계를 효율적으로 학습                                           CNN_FM - image_data를 메인으로 학습              상품에 대한 이미지 정보를 활용한 추천 모델  상품 이미지와 같은 비정형 데이터 특징을 추출하기 위해 Convolutional Neural Network 사용                 DeapCoNN              사전 학습된 BERT를 사용하여 리뷰 데이터와 같은 비정형 데이터 특징을 추출하기 위해서 사전 학습된 언어 모델을 사용합니다  하지만 총 149570의 book data중 summary 결측치가 67227개로 상당수 차지하고 있어, 모델을 적용하기에 적합하지 않다고 판단하였고, 실제로도 성능적인 아쉬움까지 이어졌습니다.            - 모델 결정 [FM, FFM]      - 룰베이스 RMSE                                         ratings                                 * user_id         0.642556                       * isbn         0.709715                       age         0.083359                       location_city         0.373939                       location_state         0.179588                       location_country         0.077136                       category         0.056237                       publisher         0.139969                       language         0.042713                       book_author         0.508389                   ratings를 기준으로 상관 관계를 파악 했을 때 상위 2가지 feature인 user_id, isbn을 선택하여 룰베이스를 적용하였다  user_id별로 해당유저가 부여하는 평점 평균 딕셔너리, isbn에 따른 평점 평균을 딕셔너리로 저장하였고, test 데이터셋에서 없는 user_id나 isbn을 호출하였을 때에는 전체 평균값을 채우도록하여 결측치를 처리하였다  ratings = (user_id + isbn) / 2로 예측을 채워서 Rule-Based 결과를 제출하였을 때에 2.2581으로 기본 베이스 모델보다 성능이 높게 나왔다  그래서 전체 context 데이터를 학습하는 FM / FFM 모델이 피쳐 튜닝을 잘 진행하면 성능이 향상될 여지가 더 많다고 판단하고  두 모델을 집중적으로 진행하였다    04_ 프로젝트 FlowChart      05_ 최종 Dataset   05_01 categorical [train/test] dataset      피처당 같은 value들을 index화 해서 데이터를 범주화한 데이터 셋                   index       user_id       isbn       rating       age       location_city       location_state       location_country       category       publisher       language       book_author                       0       0       0       4       3       0       0       0       0       0       0       0                 1       1       0       7       3       1       0       0       0       0       0       0                 2       2       0       8       3       2       0       0       0       0       0       0                 3       3       0       8       3       3       0       0       0       0       0       0                 4       4       0       9       3       4       0       0       0       0       0       0                 …       …       …       …       …       …       …       …       …       …       …       …                 306790       6313       129772       7       2       1606       5       1       0       5       0       1272                 306791       1879       129773       6       3       19       10       1       6       29       0       69                 306792       1879       129774       7       3       19       10       1       0       188       0       54713                 306793       1879       129775       7       3       19       10       1       2       118       0       54714                 306794       1879       129776       10       3       19       10       1       0       41       0       54715           05_02 numerical [train / test] dataset      피쳐당 같은 value들의 rating을 평균을 내서 수치화한 데이터 셋     - 전처리 규칙    user_id : 0이 4개 있다고 가정 각각의 rating은 user_id {0 : [7,8,9,10], ...} user_id 0의 평점 평균은 8.5 user_id { 0 : 8.5, ...}로 변경  이러한 규칙으로 나머지 feature들도 동일하게 수치화하여 데이터 전처리                     index       user_id       isbn       rating       age       location_city       location_state       location_country       category       publisher       language       book_author                       0       4.42857       6.857143       4       6.883493       6.266667       6.980202       6.934104       7.020942       7.091387       7.091387       6.857143                 1       7.285714       6.857143       7       6.883493       6.823871       6.980202       6.934104       7.020942       7.091387       7.091387       6.857143                 2       8.000000       6.857143       8       6.883493       6.782609       6.980202       6.934104       7.020942       7.091387       7.091387       6.857143                 3       8.000000       6.857143       8       6.883493       8.000000       6.980202       6.934104       7.020942       7.091387       7.091387       6.857143                 4       8.400000       6.857143       9       6.883493       7.293103       6.980202       6.934104       7.020942       7.091387       7.091387       6.857143                 …       …       …       …       …       …       …       …       …       …       …       …                 306790       8.000000       7.000000       7       7.244307       7.733333       7.104575       7.117356       6.974887       7.105181       7.091387       6.076923                 306791       5.833333       6.000000       6       6.883493       6.744807       7.115454       7.117356       7.387309       6.931951       7.091387       5.827586                 306792       5.833333       7.000000       7       6.883493       6.744807       7.115454       7.117356       6.974887       6.569892       7.091387       7.000000                 306793       5.833333       7.000000       7       6.883493       6.744807       7.115454       7.117356       7.055849       6.760032       7.091387       7.000000                 306794       5.833333       10.00000       10       6.883493       6.744807       7.115454       7.117356       6.974887       7.457627       7.091387       10.00000           06_ FM / FFM / CatBoost   - 용어 정리  min valid loss[RMSE] - 학습중 최소 손실  public score[RMSE] - ai stage 제출 점수   06_01 FFM      같은 데이터와 하이퍼파라미터로 학습 하였을 때 FFM 모델이 FM모델보다 public score가 높게 측정 되었습니다  FM은 모든 latent vetor간 interaction을 하나의 vector로 표현하는 한계를 FFM은 필드마다 latent vector를 만들어 개선했기 때문으로 파악하였기 때문에 FFM 모델을 중심으로 시각화를 진행하겠습니다    - FFM 데이터 전처리 X       min valid loss[RMSE] : 2.516 / public score[RMSE] : 2.4913         - FFM 데이터 전처리 O      min valid loss[RMSE] : 2.473 / public score[RMSE] : 2.4567         - FFM 데이터 전처리 O [Categorical -&gt; Numerical]      min valid loss[RMSE] : 1.428 / public score[RMSE] : 2.3943         - test dataset rating 분포     학습을 진행하면서 categorical dataset을 통해 FFM을 학습하면서 임의로 매겨진 index 규칙에 따라서 전처리된 학습 데이터셋을 입력하는 것에 대해서 의문이 생겼고, 각각의 value에 따라서 평균값을 이용한 Rule-Based를 적용했을 때 성능이 좋았던 것을 기억하여, categorical dataset을 numerical dataset으로 변경하자는 아이디어를 적용했습니다  데이터셋 형식의 변화로 2.4567 -&gt; 2.3943의 성능향상이 일어났고 분포 또한 rating을 특징을 잘 파악 할수 있는 분포로 변경되었습니다         06_02 CatBoost      범주형(categorical) 데이터를 처리하기 위한 부스팅 알고리즘     CatBoost는 범주형 데이터를 자동으로 처리하여 모델 성능을 향상시키는데 기여, 이때 중요한 파라미터 중 하나가 cat_features  cat_features는 CatBoost 모델에서 범주형 데이터의 열 인덱스를 지정하는 파라미터이며, cat_features에는 범주형 열의 인덱스나 열 이름을 포함하는 리스트를 전달할 수 있다.  CatBoost는 cat_features에 지정된 열을 범주형 데이터로 간주하고, 자동으로 원핫인코딩을 수행하거나, 이를 위한 특별한 분기 방법을 사용     그래서 CatBoost는 cat_features를 사용하여 범주형 데이터에 대한 특별한 처리를 수행하므로, 범주형 데이터를 미리 원핫인코딩하지 않아도 된다.  따라서, cat_features를 지정함으로써, 머신러닝 모델 구축에 필요한 전처리 과정을 줄일 수 있다.       - 하이퍼파라미터                          cat_features 설정         public score[RMSE]                                 X         2.3975                       O         2.1539                  - Catboost [데이터 전처리 : O, cat_features : O]      min valid loss[RMSE] : 2.144 / public score[RMSE] : 2.1539         - 프로젝트 수행결과        상위 3명의 모델을 앙상블을 하여 제출하였습니다  CatBoost : CatBoost : CNN_FFM = 1 : 1 : 1 -&gt; 2.1359 (public score[RMSE])       EDA 과정에서 예상 했던 것 처럼, users, books data 대비 rating data 부족한 문제 때문에 딥러닝의 성능이 머신러닝에 비해서 떨어진것 같다고 파악하였습니다     데이터가 좀 더 다양해지고 늘어난다면 딥러닝도 좋은 성능을 보일 것으로 예상합니다    회고           학습 목표              이번에는 첫 추천 시스템 프로젝트이므로 데이터의 전처리와 예측 모델 알고리즘을 파악하는 것을 목표로 하였다.                 시도한 개선 방식              결측치를 채우는 데이터 전처리를 하였을 때는 FM model은 2.463 -&gt; 2.418, FFM model은 2.516 -&gt; 2.473의 min valid loss 변화가 있었다.         Categorical 데이터로 이루어진 데이터를 각각의 피처에 따라 레이팅의 평균으로 매칭하여 전처리한 결과로 numerical 데이터로 전처리 했을 때에는 valid loss가 FM - 1.446, FFM - 1.438으로 큰 차이가 있었지만 실제 테스트 결과로는 RMSE가 2.3대로 Categorical 데이터로 처리했을 때의 RMSE인 2.4보다는 향상되었다.         Catboost Regression의 파라미터 cat_feature을 활용하여 Categorical데이터 표적 학습을 한 결과 RMSE가 2.15를 달성하였다. 머신러닝은 딥러닝에 비해 범주형 데이터를 더 잘 처리하였다.                 다음 프로젝트에서 시도해볼 것              context data를 처리하는 부분에서는 FFM도 좋은 성능을 보일 것으로 예상된다.  book recommend project에서는 데이터의 양이 딥러닝 모델을 적용하기에 충분하지 않았었던 것으로 파악을 하였다.         cold start문제를 해결하기 위해 이미 Categorical 데이터로 주어진 부분은 머신러닝 기법인 catboost를 활용하고, 새로 주어진 데이터는 numerical 데이터로 학습한 딥러닝 모델을 활용하여 하이브리드 모델을 구성하는 것을 목표로 하였다.         WandB를 통한 결과 기록                 학습 과정에서의 교훈              Git을 통해서 버전 관리를 초기에 빠르게 시작 했어야 했다. 중간에 일정이상 진행된 상태로 팀원들과 코드를 합치려는 과정에서 어려움을 겪었다.         성능만을 높이기 위한 앙상블보다는 데이터를 파악하고 서로 다른 모델끼리 부족한 점을 보완할 수 있도록 앙상블을 진행하여야 된다.         데이터 전처리와 모델에 집중하는 것 외에도 ColdStart문제를 해결하는 것도 성능 향상에 영향을 준다           ","categories": ["BoostCamp_AI"],
        "tags": ["Project"],
        "url": "/boostcamp_ai/Domain_Foundation_Competition/",
        "teaser": null
      },{
        "title": "Boostcamp Week 08 [TIL]",
        "excerpt":"04.24 (월)     소프트웨어 엔지니어링   높은 응집도(모듈 내 교류)와 느슨한 결합도(모듈 끼리는 덜 교류)를 가진 소프트웨어를 지향하자   파이썬 버전 관리   버저닝 방법에 대해 학습을 하였습니다                  CalVer (Calendar Versioning)       SemVer (Semantic Versioning)       HashVer (Hash Versioning)                       2023.01.0 [year.month. patch version] ex_ Ubuntu 23.01       3.11.3 [major version.minor version.patches] ex_ python 3.11.3       SHA-1, SHA-256 해시 알고리즘  활용 ex_ Git commit 9e8b9fe           특히 SemVer이 가장 기억에 남았습니다.  각각의 의미가 이전 버전과 호환되지 않으면 major version의 증가  이전 버전과 호환되며 새로운 기능이 추가되면 minor version의 증가  이전 버전의 버그 수정이 진행되면 patches 번호가 증가하는 형식이라 라이브러리를 업데이트를 진행할 때 의미를 파악하고 버전이 충돌을 일으키지 않도록 관리하는데 도움이 될 것 같았습니다.       04.25 (화)     디버깅 Case Study   디버깅의 process에 대해서 배울 수 있었다  poetry로 버전관리를 하면서 catboost를 설치하려고 했는데 오류가 계속 발생하였다  디버깅 process에 따라서 해결책 검색을 해보고 chatgpt를 활용 해보았지만 최근 버전에서 일어난 버그라서 해결 방안이 없었다  마지막 방법으로 오픈소스 코드 확인을 위해서 poetry github을 확인하였는데 issue에서 같은 문제가 발생했던 사람을 찾을 수 있었고, 이미 closed가 된 이슈여서 해결 방안을 적용하여 문제를 해결하였다      Issue with catboost and Poetry version 1.4.1 #7692  link - https://github.com/python-poetry/poetry/issues/7692        04.26 (수)     MLOps 개론, Model Serving   전반적인 모델 개발 프로세스를 파악할 수 있었다                  구분       온라인 서빙 (Online Serving)       배치 서빙 (Batch Serving)                       대상 시스템       인터넷, 모바일 디바이스, IoT 장치 등       대용량 데이터 처리 시스템 (데이터 웨어하우스, Hadoop 등)                 처리 방식       실시간 처리 (Real-time Processing)       일괄 처리 (Batch Processing)                 데이터 처리량       작은 양의 데이터를 빠르게 처리       대규모 데이터를 일괄적으로 처리                 데이터 처리 시간       처리 속도가 빠르며 즉각적인 결과 제공 가능       일괄 처리 시간이 오래 걸리며 결과 제공이 느릴 수 있음                 대응 기술       온라인 머신 러닝, 실시간 스트리밍, 웹소켓 등       배치 처리, 병렬 처리, 맵리듀스 등                 사용 사례       검색 엔진, 추천 시스템, 실시간 예측 등       대용량 로그 처리, 배치 데이터 분석 등               04.27 ~ 04.28 (목 ~ 금)     Week 06 ~ 07 [2023.04.10 ~ 2023.04.21] 동안 진행했던 ai staeges Book Rating Prediction 기초 대회에서 개발했던 머신러닝 예측 모델을 웹 서비스 서빙에 알맞은 모델로 새롭게 학습을 진행 한 후 모델 서빙 프로젝트를 진행하였습니다.   Links      GitHub - Books_Recommendation_Platform  Website - books-recommendation-platform by Streamlit  Blog - [Project] Books_Recommendation_Platform   ","categories": ["TIL"],
        "tags": ["BoostCamp_AI"],
        "url": "/til/Week_08_TIL/",
        "teaser": null
      },{
        "title": "[Project] Books_Recommendation_Platform",
        "excerpt":"   Week 06 ~ 07 [2023.04.10 ~ 2023.04.21] 동안 진행했던 네이버 커넥트재단 Book Rating Prediction 기초 대회에서 개발했던 머신러닝 예측 모델을 웹 서비스 서빙에 알맞은 모델로 새롭게 학습을 진행 한 후 모델 서빙 프로젝트를 진행하였습니다.    Books_Recommendation_Platform     Naver BoostCamp AI Tech 5th Recsys Product Serving by Streamlit      Books_Recommendation_Platform은 사용자 입력을 기반으로 책 추천을 제공하는 플랫폼입니다.  머신 러닝 알고리즘을 사용하여 사용자의 입력 기반으로 어떤 책을 선호할지 예측합니다.     GitHub - Books_Recommendation_Platform  Website - books-recommendation-platform        - Files      Books_Recommendation_Platform ├── Book_Rec │   ├── app.py │   ├── data │   │   ├── idx2info.pkl │   │   └── info2idx_convert_data.pkl │   ├── model │   │   ├── README.md │   │   ├── catboost_model_IALLC.pkl │   │   └── catboost_model_IALL_.pkl │   └── src │       ├── __init__.py │       ├── data_loader.py │       ├── info2idx.py │       ├── model_loader.py │       └── predict.py ├── README.md ├── poetry.lock └── pyproject.toml      이 프로젝트에는 다음과 같은 파일이 포함되어 있습니다:    - app.py      Streamlit 어플리케이션을 실행하는 주 파일입니다. 사용자 입력을 처리하고 결과로 나오는 책 추천을 표시합니다.     상위 book list를 하위 묘듈을 통해 반환을 받은 후 info2idx.py의 info_data_load()를 통해 books data를 불러온 후 request를 통해 책의 이미지를 불러오고 로컬 데이터셋에서 이름, 작가, 출판 년도를 합쳐 Streamlit 화면에 출력합니다    - data 폴더   머신 러닝 모델에서 사용하는 데이터가 포함된 폴더입니다. idx2info.pkl과 info2idx_convert_data.pkl 두 개의 pickle 파일이 있습니다.   - model 폴더   책 추천에 사용되는 머신 러닝 모델이 포함된 폴더입니다. catboost_model_IALLC.pkl과 catboost_model_IALL_.pkl 두 개의 pickle 파일이 있습니다.   - src 폴더   머신 러닝 모델이 예측을 수행하는 데 사용하는 Python 모듈이 포함된 폴더입니다. data_loader.py, info2idx.py, model_loader.py, predict.py이 있습니다.           data_loader.py              idx2info.pkl의 isbn의 값을 기반으로 add.py에서 받은 데이터를 합쳐 예측을 위한 인풋 데이터셋을 구성합니다                 info2idx.py              에측을 위해서 idx2info.pkl, info2idx_convert_data.pkl의 파일을 불러와서 add.py로 넘겨주는 파일입니다                 model_loader.py              catboost_model_IALLC.pkl, catboost_model_IALL_.pkl category 사용 여부에 따라 예측 모델을 predict.py로 넘겨주는 파일입니다                 predict.py              인풋 데이터셋을 받아 예측 모델을 통해 rating을 예측하고 app.py로 결과를 반환합니다            - README.md   현재 읽고 있는 파일입니다. 이 프로젝트의 개요와 사용 방법에 대한 설명을 제공합니다.   - poetry.lock 및 pyproject.toml   이 파일들은 Poetry를 사용하여 프로젝트 종속성을 관리하는 데 사용됩니다.       - Flow Chart          - Usage   책 추천 플랫폼을 사용하려면 다음 단계를 따르세요:      이 저장소를 로컬 컴퓨터에 복제합니다.   Poetry를 사용하여 필요한 종속성을 설치합니다. poetry install   app.py를 실행합니다. streamlit run app.py   웹 브라우저를 열고 http://localhost:[할당된 포드번호]로 이동합니다.   데이터를 입력하고 “Recommend Books”를 클릭합니다.   페이지에 표시되는 추천 책을 확인합니다.       - License      Python: PSF (Python Software Foundation) 라이선스   Streamlit: Apache-2.0 라이선스   Pandas: BSD-3-Clause 라이선스   NumPy: BSD-3-Clause 라이선스        회고    개선 해야 될점      대부분의 책 데이터가 영어 도서로 이루어져 있고, 2006년에 출판된 도서가 데이터 셋에서 가장 최신 도서이기 때문에 추천 되는 결과도 노후되어 있다   시리즈가 연속적으로 추천 되어 추천의 다양성이 떨어지는 경우가 있다   접속한 유저 데이터를 기록하지 않아 개인화와 로그 수집 및 추가적인 모델 학습이 어렵다  ","categories": ["BoostCamp_AI"],
        "tags": ["Project"],
        "url": "/boostcamp_ai/Books_Recommendation_Platform/",
        "teaser": null
      },{
        "title": "[Project] DKT 대회 - EDA",
        "excerpt":"   Week 09 ~ 12 [2023.05.02 ~ 2023.05.25] 동안 진행했던 네이버 커넥트재단 Deep Knowledge Tracing 대회를 진행하며 했었던 데이터셋 전처리 과정을 기록합니다.    Deep Knowledge Tracing EDA     Dataset 전처리 + ML학습을 위한 전처리      sequence데이터로 학습하기에 ML 알고리즘으로는 어려움이 있다  test데이터가 각각 특정한 유저가 푼 문제와 마지막 풀어야 할 문제로 구성되어 하나로 묶여있는 sequence데이터 형식이다  그대로 사용하면 예측해야 유저가 train에 아예 포함되지 않는 cold start 문제가 발생하여 데이터셋을 별도로 구성해야 한다     - 목표  [협업을 위한 공통 데이터셋 : 피처를 최대한 다양하게 만들어 필요한 부분을 선택하여 사용하도록 데이터셋 전처리]    01_ 데이터 schema           train_data.csv [2266586 rows × 6 columns]                                              userID           assessmentItemID           testId           answerCode           Timestamp           KnowledgeTag                                           0           0           A****1           A05***1           1           ****-**-** **:**:**           2626                             1           0           A****2           A05***1           1           ****-**-** **:**:**           2626                             2           0           A****3           A05***1           1           ****-**-** **:**:**           2625                             …           …           …           …           …           …           …                             2266584           7441           A****1           A04***0           0           ****-**-** **:**:**           2625                             2266585           7441           A****1           A04***0           1           ****-**-** **:**:**           2623                                test_data.csv [260114 rows × 6 columns]                                              userID           assessmentItemID           testId           answerCode           Timestamp           KnowledgeTag                                           0           3           A****1           A05***3           1           ****-**-** **:**:**           2626                             1           3           A****2           A05***3           1           ****-**-** **:**:**           2626                             2           3           A****3           A05***3           -1           ****-**-** **:**:**           2625                                         …           …           …           …           …           …                             260112           7439           A****6           A04***0           1           ****-**-** **:**:**           8244                             260113           7439           A****7           A04***0           -1           ****-**-** **:**:**           8832                           02_ EDA   02_01 Create Dataset [Create Dataset [Dataset Merge / Submission Dataset]             train_data와 test_data 결합       train_test_dataset = pd.concat([train_data, test_data], ignore_index=True) train_test_dataset = train_test_dataset.reset_index(drop=True) # index 초기화                추가로 시도하였지만 제거했던 것       # 같은 유저가 같은 시험지, 문제를 풀었을 경우 최신 데이터만 남기고 이전 데이터를 제거하는 코드 train_test_dataset.drop_duplicates(subset=['userID', 'assessmentItemID', 'testId'], keep='last', inplace=True)                  ML은 sequence데이터처리의 성능이 좋지 않을 것이라 예상을 하여 같은 문제를 푼적이 있다면 이전 데이터를 제거하는 방식으로 전처리를 하여 실험을 하였지만, 2526700 -&gt; 2476706로  1.98%의 데이터가 감소하였고, categorical 데이터로 처리 하였을 때 다양성이 줄어들어 성능이 떨어졌다                 result [train_test_dataset.info(show_counts=True)]       &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 2526700 entries, 0 to 2526699 Data columns (total 6 columns):  #   Column            Non-Null Count    Dtype  ---  ------            --------------    -----   0   userID            2526700 non-null  int64   1   assessmentItemID  2526700 non-null  object  2   testId            2526700 non-null  object  3   answerCode        2526700 non-null  int64   4   Timestamp         2526700 non-null  object  5   KnowledgeTag      2526700 non-null  int64  dtypes: int64(3), object(3) memory usage: 115.7+ MB                  결측치가 없는 것을 확인하고 결측치 처리과정 생략            02_02 시험지 번호, 문제 번호, 대분류 카테고리 생성        assessmentItemID ex_A080080003의 index=2 문자는 1-9 범위의 대분류이다            시험지 번호, 문제 번호 [사용 안함]       train_test_dataset['test_num']=train_test_dataset['assessmentItemID'].str[4:7].astype(int) train_test_dataset['question_num']=train_test_dataset['assessmentItemID'].str[7:].astype(int)                  시험지 번호와 문제 번호가 의존성이 있어 함께 사용되어야 문제를 구분할 수 있기 때문에 오용을 방지하기 위해 개인적으로  배제하고 실험                 대분류 카테고리       train_test_dataset['LargeCategory'] = train_test_dataset['assessmentItemID'].str[2] train_test_dataset['LargeCategory'] = train_test_dataset['LargeCategory'].astype(int) # 정수화                  1 ~ 9 범위로 데이터의 개수에 비해 분류 가짓수가 작아 전처리 내에서는 사용하지 않고 모델링 실험에만 사용            02_03 TimeStamp             base code       diff = df.loc[:, ['userID', 'Timestamp']].groupby('userID').diff().fillna(pd.Timedelta(seconds=0)) diff = diff.fillna(pd.Timedelta(seconds=0)) diff = diff['Timestamp'].apply(lambda x: x.total_seconds()) df['elapsed'] = diff                  - 코드 문제점  1. 결측치가 생기게 되는 첫 번째 값은 0으로 다 채워서 첫번째 문제는 푼 시간이 0이 나오는 문제 발생    2. groupby를 userID로만 하면서 다음 시험지까지 시간차가 있는 경우에도 그대로 diff() 함수가 적용되어 오차 발생  3. 문제 푸는 시간은 다음 문제로 넘어가는 시간을 기준으로 해야하는 데 diff()만 사용하면 해당 줄 기준으로 앞줄 시간을 빼서 한 칸씩 밀린다                 update code       train_test_dataset['Timestamp'] = pd.to_datetime(train_test_dataset['Timestamp']) time_diff = train_test_dataset.loc[:, ['userID','testId', 'Timestamp']].groupby(['userID', 'testId']).diff(periods=-1).abs() time_diff = time_diff['Timestamp'].apply(lambda x: x.total_seconds()) train_test_dataset['TimeElapsed'] = time_diff                  - 해결 과정    1. nan결측치로 방치 -&gt; 추가로 전처리로 연결    2. groupby를 userID, testId를 기반으로 적용하여 유저아이디를 기반으로 시험지 아이디로 추가적으로 분리    3. .diff(periods=-1).abs()로 수정하여 앞에 값에 뒤에 값을 빼고 절대값으로 변경한다                 train_test_dataset [2526700 rows × 8 columns]                                              userID           assessmentItemID           testId           answerCode           Timestamp           KnowledgeTag           LargeCategory           TimeElapsed                                           0           0           A****1           A05***1           1           ****-**-** **:**:**           7224           6           3                             1           0           A****2           A05***1           1           ****-**-** **:**:**           7225           6           8                             2           0           A****3           A05***1           1           ****-**-** **:**:**           7225           6           7                             3           0           A****4           A05***1           1           ****-**-** **:**:**           7225           6           7                             …           …           …           …           …           …           …           …           …                           02_04 TimeElapsed Outlier 처리                    주요 사항  outlier를 처리하기 위해 앞뒤 값을 잘라내는 경우가 있다  최소값은 0부터 시작하지만 문제를 찍는 다면 짧을 시간이 나올 수 있어, 유효한 값이다  하지만 최대값의 경우에는 문제를 푸는 것에 비해서 정상범위를 벗어난 값들이 있어 상위 값들은 모델의 학습에 방해가 될 만한 값들이였다                  해결 방법  사분위수 기준으로 outlier 값들을 nan으로 처리하여 결측치로 변환하였다                    train_test_dataset[‘TimeElapsed’].describe()       count    2119511.0000 mean       17075.7388 std       387221.2765 min            0.0000 25%           10.0000 50%           27.0000 75%           66.0000 max     28695878.0000 Name: TimeElapsed, dtype: float64                  TimeElapsed 값은 0부터 28,695,878까지 다양하게 분포되어 있음을 알 수 있다.  평균 경과 시간은 약 17,075.74이고, 표준 편차는 387,221.28로 상당히 큰 값이다.  백분위수를 통해 대부분의 수치들은 상대적으로 짧은 경과 시간을 갖는 반면, 최댓값을 기록하는 일부 관측값은 훨씬 더 긴 경과 시간을 갖고 있음을 알 수 있다.                 전처리 전 scatter 그래프                   사분위수 기준 Outlier 제거 함수 코드       # 이상치 처리 함수 def _handle_outliers(df: pd.DataFrame, column: str)-&gt;pd.DataFrame:     # 사분위수 계산     q1 = df[column].quantile(0.25)     q2 = df[column].quantile(0.5)  # 추가: 중위값 - 사용은 안함     q3 = df[column].quantile(0.75)     # IQR 계산     iqr = q3 - q1     # 상위 이상치 처리     upper_bound = q3 + 1.5 * iqr     df[column] = df[column].apply(lambda x: np.nan if x &gt; upper_bound else x)     print(f\"q1: {q1}, q2: {q2}, q3: {q3}, iqr: {iqr}, upper_bound: {upper_bound}\")     return df    train_test_dataset = _handle_outliers(train_test_dataset, 'TimeElapsed')                  - output -  q1: 10.0, q2: 27.0, q3: 66.0, iqr: 56.0, upper_bound: 150.0                 신뢰구간 95% 함수 코드       from scipy.stats import t def _calculate_confidence_interval(df: pd.DataFrame, target_column:str, confidence_level=0.95)-&gt; tuple:     # 평균과 표준편차 계산     mean = df[target_column].mean()     std = df[target_column].std()        # 데이터 개수와 신뢰수준 설정     n = len(df)        # 신뢰구간 계산     t_critical = t.ppf((1 + confidence_level) / 2, df=n-1)     margin_of_error = t_critical * std / np.sqrt(n)     confidence_interval = (mean - margin_of_error, mean + margin_of_error)        print(\"Mean:\", mean)     print(\"Standard Deviation:\", std)     print(\"Confidence Interval ({}%):\".format(confidence_level * 100), confidence_interval)     return confidence_interval      confidence_interval = _calculate_confidence_interval(train_test_dataset, 'TimeElapsed', 0.95)                  - output -  Mean: 34.22725368509248  Standard Deviation: 33.77893776146775  Confidence Interval (95.0%): (34.185603450952044, 34.26890391923291)                 outlier 제거 후 scatter 그래프 [유저아이디 / 시간]                   outlier 제거 후 hist 그래프 [시간 / 빈도]              02_05 TimeElapsed 결측치 채우기        userID,\tassessmentItemID,\ttestId, KnowledgeTag, LargeCategory를 기준으로 각 피처당 TimeElapsed 평균 값으로 새 피처 생성            nan을 채우기 위한 전과정       def _fill_nan_process(dataset: pd.DataFrame, columns_list: list) -&gt; pd.DataFrame:     for name in columns_list:         create_column_name = name + '_time_average'         time_average = dataset.groupby(name)['TimeElapsed'].mean()         dataset[create_column_name] = train_test_dataset[name].map(time_average)     return dataset    columns = ['userID', 'assessmentItemID', 'testId', 'KnowledgeTag', 'LargeCategory'] train_test_dataset = _fill_nan_process(train_test_dataset, columns)                  columns당 같은 값을 가진 group의 시간 평균을 추가 해주는 함수                                                   …           TimeElapsed           userID_time_average           assessmentItemID_time_average           testId_time_average           KnowledgeTag_time_average           LargeCategory_time_average                                           0           …           3           30.5714           10.587           17.9805           12.4831           37.1284                             1           …           8           30.5714           19.8776           17.9805           22.2376           37.1284                             2           …           7           30.5714           17.6694           17.9805           22.2376           37.1284                             3           …           7           30.5714           14.065           17.9805           22.2376           37.1284                             4           …           …           …           …           …           …           …                                결측치 확인       ... TimeElapsed                      626612 userID_time_average                   0 assessmentItemID_time_average      7050 testId_time_average                   0 KnowledgeTag_time_average             0 LargeCategory_time_average            0 dtype: int64                  assessmentItemID에서 마지막 문제인 경우에는 풀이 시간을 알수 없어 assessmentItemID_time_average에서 NaN값이 생성됨                 결측치 처리 함수 코드 [선형회귀 모델 사용]              먼저 assessmentItemID_time_average의 결측치를 치운 후 ItemID_time_average의 값을 함께 포함한 평균으로 TimeElapsed를 채운다  결측치가 없는 행과 변수들을 활용하여 회귀 모델 피팅  NaN값이 존재하는 행을 제거한 다음 학습을 진행한 다음 NaN값이 있는 행들을 model.predict를 통해 예측하여 결측치를 채운다            from sklearn.linear_model import LinearRegression def _predict_missing_with_regression(df: pd.DataFrame, target_column: str) -&gt; pd.DataFrame:     \"\"\"     결측치가 있는 변수를 선형 회귀 모델을 사용하여 예측     Args:         df (pd.DataFrame): 결측치를 예측할 데이터프레임         target_column (str): 예측 대상 변수 이름     Returns:         pd.DataFrame: 결측치가 채워진 변수 열     \"\"\"     # 결측치가 없는 행과 변수들을 활용하여 회귀 모델 피팅     train_data = df.dropna()     X = train_data.drop(columns=[target_column])     y = train_data[target_column]     model = LinearRegression()     model.fit(X, y)            # 결측치를 예측하기 위해 회귀 모델을 활용     missing_data = df[df[target_column].isnull()].drop(columns=[target_column])     predicted_values = model.predict(missing_data)            # 예측된 값으로 결측치 채우기     df.loc[df[target_column].isnull(), target_column] = predicted_values            # 0 미만의 값들을 0으로 처리     df.loc[df[target_column] &lt; 0, target_column] = 0     return df[target_column]      # 결측치 예측 및 채우기 df = train_test_dataset[['userID_time_average', 'assessmentItemID_time_average',                          'testId_time_average', 'KnowledgeTag_time_average',                          'LargeCategory_time_average']] train_test_dataset['assessmentItemID_time_average'] = _predict_missing_with_regression(df, 'assessmentItemID_time_average')    df = train_test_dataset[['TimeElapsed', 'userID_time_average',                          'assessmentItemID_time_average', 'testId_time_average',                          'KnowledgeTag_time_average', 'LargeCategory_time_average']] train_test_dataset['TimeElapsed'] = _predict_missing_with_regression(df, 'TimeElapsed').astype(int)                  먼저 assessmentItemID_time_average의 결측치를 채우고 TimeElapsed를 채운다                 결측치 처리 완료       # TimeElapsed 정수화 train_test_dataset['TimeElapsed'] = train_test_dataset['TimeElapsed'].astype(int) # 계산이 끝난 columns 제거 train_test_dataset.drop(['userID_time_average', 'assessmentItemID_time_average', 'testId_time_average', 'KnowledgeTag_time_average', 'LargeCategory_time_average'], axis=1, inplace=True) train_test_dataset                TimeElapsed_category 생성       # 10을 기준으로 카테로리 생성 [0 ~ 15] 범위 train_test_dataset['TimeElapsed_category'] = train_test_dataset['TimeElapsed'] // 10 sorted(train_test_dataset['TimeElapsed_category'].unique())                  - output - [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]                 결측치 처리 완료 후 hist 그래프 [시간 / 빈도]                     - output - Mean: 34.22725368509248 -&gt; 33.83793445996755  Standard Deviation: 33.77893776146775 -&gt; 30.874327640807945  Confidence Interval (95.0%): (34.185603450952044, 34.26890391923291) -&gt; (33.79986567923061, 33.87600324070449)                 Timestamp 결측치 처리 완료                                              userID           answerCode           Timestamp           KnowledgeTag           LargeCategory           TimeElapsed           TimeElapsed_category                                           count           2.5267e+06           2.5267e+06           2526700           2.5267e+06           2.5267e+06           2.5267e+06           2.5267e+06                             mean           2083.08           0.654151           2020-07-18 15:59:12.793359104           4909.09           4.70513           33.8379           2.96018                             min           0           -1           2019-12-31 15:08:01           23           1           0           0                             25%           800           0           2020-05-19 05:24:21.750000128           1934           3           11           1                             50%           1748           1           2020-07-30 07:12:41           5258           5           26           2                             75%           3034           1           2020-09-21 11:48:52           7913           7           47           4                             max           7441           1           2020-12-29 16:46:21           11271           9           150           15                             std           1585.56           0.476263           nan           3303           2.43182           30.8743           3.07131                           03_ FeatureEngineering   train_test_dataset['answerCode'] = np.where(train_test_dataset['answerCode'] == -1, np.nan, train_test_dataset['answerCode'])      먼저 answerCode의 -1을 nan으로 처리하여 함께 평균에 포함되어 연산되지 않도록 한다    03_01 Timestamp     def _datetime_process(df: pd.DataFrame):     # 년도 (Year) 추출     df['year'] = df['Timestamp'].dt.year     # 월 (Month) 추출     df['month'] = df['Timestamp'].dt.month     # 일 (Day) 추출     df['day'] = df['Timestamp'].dt.day     # 요일 (Weekday) 추출 - [0~6] 월 ~ 일     df['weekday'] = df['Timestamp'].dt.weekday     # 시간 (hour) 추출     df['hour'] = df['Timestamp'].dt.hour     return df  train_test_dataset = _datetime_process(train_test_dataset)      Timestamp 값들을 세분화한다    04_ 실패 했던 FeatureEngineering      FeatureEngineering에서 예측 모델 학습에서 auc나 accuracy를 1에 가깝게 과적합을 발생시킨 feature들입니다  전 book recommend 대회에서는 예측 모델의 성능 향상을 이루어낸 FeatureEngineering 방식이 였지만 이번에는 [0,1]로 이루어진 분류 문제여서 정답을 기반으로 다른 feature들을 결합하는 순간 새로 생성된 feature들이 정답을 너무 잘 표현한 값들이여서 과적합을 발생시켰습니다    04_01 _columns_to_mean_score 함수 코드        _columns_to_mean_score(df: 데이터프레임, columns_list: 리스트) -&gt; 데이터프레임:         columns_list 규칙 - *_mean   userID_mean - 같은 유저 아이디별 평균 점수   userID_assessmentItemID_mean - 같은 유저의 같은 문제 평균 점수   userID_testId_mean - 같은 유저의 같은 시험지 평균 점수      def _columns_to_mean_score(df: pd.DataFrame, columns_list: list) -&gt; pd.DataFrame:     create_column_name = '_'.join(columns_list) + '_mean'     # DataFrame을 columns_list로 그룹화한 후, answerCode 열의 평균을 계산하여 새로운 DataFrame average_data를 생성     average_data = df.groupby(columns_list)['answerCode'].mean().reset_index()     # average_data DataFrame의 answerCode 열의 이름을 create_column_name으로 변경     average_data.rename(columns={'answerCode': create_column_name}, inplace=True)      # 평균치라서 결측치는 0.5로 채움     average_data = average_data.fillna(0.5)          # df와 average_data를 columns_list를 기준으로 left 조인하여 병합     df = pd.merge(df, average_data, on=columns_list, how='left')     return df      userID, assessmentItemID, testId, KnowledgeTag, LargeCategory, TimeElapsed_category, weekday,\thour 활용하여 feature들을 생성    05_ 전처리 완료      개인 전처리를 완료하고 이후에는 팀원들의 전처리를 추가적으로 합쳐 하나의 데이터셋을 만들고 hugging face datasets를 사용하여 버전관리를 했습니다          - 회고             목표              이전의 대회인 book recommend 대회에서는 각자 eda를 하고 예측 모델 생성을 하였기 때문에 같은 모델과 파라미더를 사용해도 결과가 다르게 나오는 시행착오를 겪어 이번에는 공통된 데이터셋을 구성하는 것을 목표로 하였습니다                 시도한 개선 방식              04_01의 _columns_to_mean_score방식을 구현 하였을 때 전에는 dict로 key:value를 통해서 연산하는 방식을 사용하여 속도가 느렸는데 pandas에 더 익숙해지면서 groupby()등 여러 개념을 활용하여 같은 데이터 처리 시간을 단축시켰습니다  결측치를 처리하는 과정에서 그래프를 보고 임의로 잘라내는 방식을 사용하였는데 이번에는 사분위수를 적용하여 통계학적으로 접근을 하였습니다  추가로 결측치를 처리할 때 평균으로 처리를 하였지만 의존성이 있는 feature들을 생성하여 LinearRegression 모델을 활용하여 채웠습니다                 학습 과정에서의 교훈              이전에 좋은 결과를 얻었다고 같은 전처리가 항상 좋은 결과를 보장하는 것이 아니라는 점을 배울 수 있었고, 데이터셋의 구조를 파악하고 데이터셋에 맞는 전처리가 필요하다는 것을 배울 수 있었습니다. 다양한 데이터를 접하고 직접 전처리를 하는 것이 실력을 향상 시키는 방법이라는 생각이 들었습니다           ","categories": ["BoostCamp_AI"],
        "tags": ["Project"],
        "url": "/boostcamp_ai/DKT_EDA/",
        "teaser": null
      },{
        "title": "[Project] DKT 대회 - Modeling",
        "excerpt":"Deep Knowledge Tracing Modeling        - 개발 환경  OS : ubuntu 18.04.5 LTS  GPU : Tesla V100-SXM2-32GB    cuda : 11.0.0     - 버전 관리  Git, HuggingFaceDatasets     - version                          python: 3.10.11                                           catboost: 1.2 hydra-core: 1.3.2 numpy: 1.24.3 omegaconf: 2.3.0         xgboost: 1.7.5 pandas: 2.0.1 scikit-learn: 1.2.2                     ML_models. |-- log : [hydra log file] |-- src : [source code] |   |-- catboost |   |   |-- catboost_classifier.py |   |   `-- catboost_regressor.py |   |-- data_loader.py |   |-- data_process.py |   |-- submission.py |   |-- train_test_split.py |   `-- xgboost |       `-- xgboost_classifier.py |-- submit : [submission.csv] |-- __init__.py |-- train.py : [main file] `-- config_hydra.yaml : [hydra config file]      01_ train_test_split.py      목표 : test셋의 데이터 크기와 구성에 따른 모델 성능 파악  참조 : Andrew ng - Structuring Machine Learning Projects [Sizeof Dev and Test Sets]  Key point : 과거에 데이터가 작을 경우에는 7:3의 train/test split이 성능이 좋았지만, 데이터셋이 크면 1%의 데이터도 10,000이상이 되기 때문에 test 데이터의 비율을 줄이고 학습 데이터를 더 크게 하는 것이 성능향상에 도움이 되는 경향이 있다            config_hydra.yaml       ... dataset_feature:   train_data_cut: head_tail_one # [all, head, tail], [head_tail_one, head_tail_group_one, head_one, tail_one] - userID별 첫 문제, 마지막 문제를 test에 사용   test_split: 0.1 # 1&gt; value &gt; 0   feature: [             # Base feature [base데이터 기반으로 검증을 하기 때문에 변경 금지]             'userID', 'assessmentItemID', 'testId', 'Timestamp',             'KnowledgeTag','LargeCategory',             'TimeElapsed', 'TimeElapsed_category',             ...,            ]      # 정답 데이터   label: 'answerCode'      # merge 할때 검증 데이터   valid_feature: ['userID', 'assessmentItemID', 'testId', 'KnowledgeTag', 'LargeCategory', 'TimeElapsed', 'answerCode'] ...                  dataset_feature의 train_data_cut 기반으로 학습 데이터셋 생성                 train_test_split             - all        if train_data_cut == 'all':     test_dataset_tail = dataset.groupby('userID').apply(lambda x: x.tail(int(len(x) * test_split))).reset_index(drop=True)     test_dataset_head = dataset.groupby('userID').apply(lambda x: x.head(int(len(x) * test_split))).reset_index(drop=True)     test_dataset = pd.concat([test_dataset_head, test_dataset_tail], ignore_index=True)                       userID 그룹별 head에서 test_split%, tail에서 test_split%의 행들을 테스트 데이터로 사용                - head        elif train_data_cut == 'head':         test_dataset = dataset.groupby('userID').apply(lambda x: x.head(int(len(x) * test_split))).reset_index(drop=True)                       userID 그룹별 head에서 test_split%의 행들을 테스트 데이터로 사용                - tail        elif train_data_cut == 'tail':         test_dataset = dataset.groupby('userID').apply(lambda x: x.tail(int(len(x) * test_split))).reset_index(drop=True)                       userID 그룹별 tail에서  test_split%의 행들을 테스트 데이터로 사용                - head_tail_one         elif train_data_cut == 'head_tail_one':         test_dataset_head = dataset[dataset[\"userID\"] != dataset[\"userID\"].shift(1)]         test_dataset_tail = dataset[dataset[\"userID\"] != dataset[\"userID\"].shift(-1)]         test_dataset = pd.concat([test_dataset_head, test_dataset_tail], ignore_index=True)                        userID 그룹별 head에서 1줄, tail에서 1줄의 행들을 테스트 데이터로 사용                - head_tail_group_one         elif train_data_cut == 'head_tail_group_one':         test_dataset_head = dataset.groupby([\"userID\", \"testId\"]).head(1).reset_index(drop=True)         test_dataset_tail = dataset.groupby([\"userID\", \"testId\"]).tail(1).reset_index(drop=True)         test_dataset = pd.concat([test_dataset_head, test_dataset_tail], ignore_index=True)                        userID 그룹내의 testId별 head에서 1줄, tail에서 1줄의 행들을 테스트 데이터로 사용                - head_one         elif train_data_cut == 'head_one':         test_dataset = dataset[dataset[\"userID\"] != dataset[\"userID\"].shift(1)]                        userID 그룹별 head에서 1줄의 행들을 테스트 데이터로 사용                - tail_one         elif train_data_cut == 'tail_one':         test_dataset = dataset[dataset[\"userID\"] != dataset[\"userID\"].shift(-1)]                        userID 그룹별 tail에서 1줄의 행들을 테스트 데이터로 사용                   02_ Modeling   02_01 CatBoost_Classifier            feature_importances [head_tail_one]                                  피처           중요도                                           assessmentItemID           30.837479802165063                             LargeCategory           20.379422848684584                             userID           17.992163365480707                             TimeElapsed           10.034321065891932                             testId           9.408169387494867                             TimeElapsed_category           8.54008358793767                             KnowledgeTag           1.1509929524935065                             weekday           0.8977411576214861                             hour           0.7596258322302241                                Learning Curve              - all [userID group당 head: 5%, tail: 5%]                  - head [userID group당 head: 10%]                  - tail [userID group당 tail: 10%]                  - head_tail_one [userID group당 head: 1, tail: 1]                  - head_tail_group_one [userID group+assessmentItemID당 head: 1, tail: 1]                  - head_one [userID group당 head: 1]                  - tail_one [userID group당 tail: 1]                          결과 요약             - Result Table [AUC 기준 정렬]                                          train/test split             [AUC]             ACC             Train data: 0             Train data: 1             Train data: 0/1 ratio             Test data: 0             Test data: 1             Test data: 0/1 ratio             Test data percentage                                                     head_one             0.858163             0.818597             1648249             870263             1.89             5337             2105             2.54             0.40%                                   head_tail_one             0.851704             0.779024             1644588             866417             1.90             8958             5926             1.51             0.74%                                   all             0.832974             0.777895             1494743             784845             1.90             158303             87229             1.81             32.45%                                   tail             0.832326             0.771043             1497730             778199             1.93             155379             93904             1.66             38.01%                                   head             0.826128             0.780402             1486194             789710             1.88             166913             82370             2.03             33.16%                                   tail_one             0.816565             0.741064             1649927             868522             1.90             3821             3621             1.05             0.52%                                   head_tail_group_one             0.813343             0.765324             1117402             589584             1.90             533203             281175             1.90             68.92%                                            ACC[accuracy]는 학습 완료 후 test 데이터셋을 통해 측정 하였습니다                - hyperparameter         iterations: 1500 learning_rate: 0.1 eval_metric: AUC random_seed: 42 use_best_model: True bagging_temperature: 1 border_count: 254               대회에서의 public score은 head_tail_one가 가장 높게 나왔습니다. 로컬에서는 head_one가 성능이 가장 높게 나왔지만 public score은 head_tail_one에 비해 낮았습니다.  파악한 이유로는 학습 데이터 자체가 라벨인 0인 데이터가 많았고, 모델에서도 좀 더 빈번하게 학습을 하게 되었던 것으로 보이며, head_one의 0, 1 비율도 2.54로 가장 0이 많은 데이터였습니다.  또한 데이터의 원래 데이터셋이 Sequence한 데이터이기 같은 userID가 푼 문제를 고려해야하는 동시에 각자 푼 문제의 개수가 달라지고 임의의 수치로 집어 넣기 어려운 개인의 숙련도가 있기 때문에 단순하게 각각 유저의 첫번째 문제를 test데이터로 검증하고 조절하는 것이 좀 더 단순 했고 이 때문에 로컬 테스트 환경에서 모델의 해석 및 설명력에서 더 좋았고 AUC/ACC가 더 높을 것이라 추측 됩니다.  그래프를 통해서도 head_one의 Logloss_test가 Logloss_train보다 더 낮게 나오며 일반화가 빠르게 된것을 확인할 수 있었습니다.  모델의 일반화는 head_one가 잘 되었지만 test데이터 자체가 dkt대회의 의도인 마지막으로 푼 문제의 결과를 예측하는 것과는 정반대의 실험이였기 때문에 head_tail_one이 더 좋은 public score를 달성하였습니다.            02_02 CatBoost_Regressor             feature_importances [head_tail_one]                                  피처           중요도                                           assessmentItemID           25.6370474484145                             userID           23.34964132877283                             TimeElapsed           23.117647780172927                             LargeCategory           11.681206689716554                             testId           6.881576643100849                             TimeElapsed_category           6.672992232579046                             hour           1.4462463208473804                             weekday           0.6146904016820545                             KnowledgeTag           0.5989511547139237                                Learning Curve              - all [userID group당 head: 5%, tail: 5%]                  - head [userID group당 head: 10%]                  - tail [userID group당 tail: 10%]                  - head_tail_one [userID group당 head: 1, tail: 1]                  - head_tail_group_one [userID group+assessmentItemID당 head: 1, tail: 1]                  - head_one [userID group당 head: 1]                  - tail_one [userID group당 tail: 1]                          결과 요약             - Result Table [AUC 기준 정렬]                                          train/test split             RMSE             [AUC]             ACC             Train data: 0             Train data: 1             Train data: 0/1 ratio             Test data: 0             Test data: 1             Test data: 0/1 ratio             Test data percentage                                                     head_one             0.361193             0.855990             0.821015             1648249             870263             1.89             5337             2105             2.54             0.40%                                   head_tail_one             0.391693             0.850371             0.779629             1644588             866417             1.90             8958             5926             1.51             0.74%                                   all             0.396145             0.831156             0.775251             1494743             784845             1.90             158303             87229             1.81             32.45%                                   tail             0.400726             0.830980             0.768504             1497730             778199             1.93             155379             93904             1.66             38.01%                                   head             0.393213             0.823580             0.778051             1486194             789710             1.88             166913             82370             2.03             33.16%                                   tail_one             0.421282             0.812667             0.734077             1649927             868522             1.90             3821             3621             1.05             0.52%                                   head_tail_group_one             0.437356             0.740576             0.710391             1117402             589584             1.90             533203             281175             1.90             68.92%                                            CatBoost Regressor에서 AUC를 사용할 수 없기 때문에 RMSE를 기반으로 학습하여  AUC[Area Under Curve], ACC[accuracy]는 학습 완료 후 predict model과 test 데이터셋을 통해 측정 하였습니다                - hyperparameter         iterations: 1500 learning_rate: 0.1 loss_function: RMSE random_seed: 42 use_best_model: true task_type: GPU bagging_temperature: 1 border_count: 254               Catboost Classifier과 유사한 결과가 나왔습니다.  그래프를 통해서도 head_one의 RMSE_test가 RMSE_train보다 더 낮게 나오며 일반화가 빠르게 된것을 확인할 수 있었습니다.  모델의 일반화는 head_one가 잘 되었지만 test데이터 자체가 dkt대회의 의도인 마지막으로 푼 문제의 결과를 예측하는 것과는 정반대의 실험이였기 때문에 head_tail_one이 더 좋은 public score를 달성하였습니다.            02_03 XGBoost_Classifier       XGBoost Classifier 실험 과정에서, Catboost Classifier와 최대한 비슷한 파라미터와 데이터 처리로 실험을 하였지만, 성능이 20% 이상이 차이나서 실험을 중단하였습니다.    03_ Ensemble   03_01 Base Model                             Model name         Public Score [AUC / ACC]                                 LGBM_regressor         [0.7850 / 0.7070]                       Catboost_classifier         [0.7438 / 0.7419]                       Catboost_regressor         [0.7930 / 0.7097]                       lstm [kfold]         [0.7932 / 0.7151]                       saint+ [kfold]         [0.8072 / 0.7339]                   - 모델별 예측 결과 분포            catboost_regressor_clip의 경우 0이하의 값은 0으로, 1이상의 값은 1로 clip       03_02 Ensemble Result                    ensemble       ratio       voting       Public score [AUC / ACC]       Private score [AUC / ACC]                       saint+[kfold] Catboost_regressor Catboost_classifier       7 : 2 : 1       soft       [1st] 0.8143 / 0.7554        0.8373 / 0.7473                 saint+[kfold] Catboost_regressor Catboost_classifier       5 : 3 : 2       soft       [2nd] 0.8137 / 0.7527       [3rd] 0.8382 / 0.7581                 saint+[kfold] Catboost_regressor Catboost_classifier       1 : 1 : 1       hard       [3rd]] 0.8134 / 0.7419        0.8375 / 0.7581                 saint+[kfold] Catboost_regressor Catboost_classifier       1 : 1 : 1       soft       0.8133 / 0.7419       0.8375 / 0.7581                 saint+[kfold] Catboost_regressor Catboost_classifier lstm[kfold]       1 : 1 : 1 : 1       soft        0.8105 / 0.7419       [1st] 0.8390 / 0.7634                 saint+[kfold] Catboost_regressor lstm[kfold]       1 : 1 : 1       soft        0.8084 / 0.7312       [2nd] 0.8388 / 0.7581                 saint+[kfold] Catboost_regressor LGBM_regressor       1 : 1 : 1       soft       0.8067 / 0.8314       0.7312 / 0.7527           03_03 결론        대회에서 등수를 상승을 목적으로 ensemble을 한다면, Ensemble 과정에서 public score에만 집중하여 ensemble을 하였는 데, 결과적으로는 다양한 모델이 ensemble되는 것이 좋은 결과를 보였다.  대회에서는 2개를 선택하여 제출하는 것이 규칙이기 때문에 ensemble과정에서 같은 모델의 비율만 바꿔서 높은 것을 2개 내는 것보다는 서로 다른 모델로 이루어진 ensemble 결과물에서 각자 높은 것을 선택하여 제출하는 방법을 사용하는 전략이 좋은 점수를 받을 수 있을 것으로 예상된다.    - 회고             목표              ML 알고리즘 기반 baseline 코드 작성 및 hydra, wandb 연결  예측 모델 성능 향상                 다음 프로젝트에서 시도해볼 것              recbole 같은 오픈소스 라이브러리 구조를 생각해서 여러 머신러닝 알고리즘을 하나로 묶는 것을 목표로 하였지만, 이 과정에서 hydra같은 실험 관리 라이브러리 사용이 이상적으로 잘 되지 않았다.  여러 머신러닝 알고리즘을 함께 사용하다 보니 hydra yaml파일의 피처들이 많이 늘어나게 되어 오히려 파악이 어려워지고, 그에 따라 하이퍼 파라미터에 따른 로깅 기능도 사용하기 어렵게 되었다.  이점을 보완해서 다음 프로젝트에는 통합으로 관리하기 보다는 알고리즘별로 나누는 것이 필요한것 같고 이 부분을 다시 시도해 볼 것이다.                 학습 과정에서의 교훈              테스트 데이터 셋에서 정답 라벨의 분포를 균형있게 맞추어야 된다는 점을 파악할 수 있었습니다.           ","categories": ["BoostCamp_AI"],
        "tags": ["Project"],
        "url": "/boostcamp_ai/DKT_Modeling/",
        "teaser": null
      },{
        "title": "[dis] dis 모듈을 활용해서 바이트코드 분석하기",
        "excerpt":"dis 모듈을 활용해서 바이트코드 분석하기      파이썬 코드 리뷰 꿀팁, 김동현 - PyCon Korea 2022 [https://youtu.be/dzp0-5lInw0]  [파이썬] dis모듈로 바이트 코드 확인 - dis - [https://aia1235.tistory.com/60]    def func():     a = 1     b = 2     c = a + b     return c     - 바이트코드에 익숙하지 않다면 이해하기 어렵다   print(func.__code__.co_code) print(list(func.__code__.co_code))   - output -   b'\\x97\\x00d\\x01}\\x00d\\x02}\\x01|\\x00|\\x01z\\x00\\x00\\x00}\\x02|\\x02S\\x00' [151, 0, 100, 1, 125, 0, 100, 2, 125, 1, 124, 0, 124, 1, 122, 0, 0, 0, 125, 2, 124, 2, 83, 0]     - 이럴 때 사용할 수 있는 모듈이 dis   import dis import timeit   dis.dis(func)   - output -   1           0 RESUME                   0  2           2 LOAD_CONST               1 (1)             4 STORE_FAST               0 (a)  3           6 LOAD_CONST               2 (2)             8 STORE_FAST               1 (b)  4          10 LOAD_FAST                0 (a)            12 LOAD_FAST                1 (b)            14 BINARY_OP                0 (+)            18 STORE_FAST               2 (c)  5          20 LOAD_FAST                2 (c)            22 RETURN_VALUE     - 리스트 값 확인      [151, 0, 100, 1, 125, 0, 100, 2, 125, 1, 124, 0, 124, 1, 122, 0, 0, 0, 125, 2, 124, 2, 83, 0]    print(f'151: {dis.opname[125]}') print(f'100: {dis.opname[100]}') print(f'125: {dis.opname[125]}')   - output -   151: STORE_FAST 100: LOAD_CONST 125: STORE_FAST     - 더 쉽게 보고 싶다면   dis.show_code(func)   - output -   Name:              func Filename:          /tmp/ipykernel_30725/21688586.py Argument count:    0 Positional-only arguments: 0 Kw-only arguments: 0 Number of locals:  3 Stack size:        2 Flags:             OPTIMIZED, NEWLOCALS Constants:    0: None    1: 1    2: 2 Variable names:    0: a    1: b    2: c             사용 예시      어떤게 더 효율적인지 비교    ee={} ee['3'] = 32 ee   ew={} ew.update({'3': 32}) ew      둘 다 {‘3’: 32} 반환    # 서브스크립션 dis.dis('d[k] = v')   - output -   0           0 RESUME                   0  1           2 LOAD_NAME                0 (v)             4 LOAD_NAME                1 (d)             6 LOAD_NAME                2 (k)             8 STORE_SUBSCR            12 LOAD_CONST               0 (None)            14 RETURN_VALUE      0 RESUME 0: 이는 코드의 첫 번째 라인을 나타냅니다. RESUME은 실행을 재개하는 지점을 나타내는 명령입니다. 여기서는 실행을 계속 진행합니다.  2 LOAD_NAME 0 (v): 변수 v를 로드합니다.  4 LOAD_NAME 1 (d): 변수 d를 로드합니다.  6 LOAD_NAME 2 (k): 변수 k를 로드합니다.  8 STORE_SUBSCR: 변수 d의 k번째 요소에 변수 v를 할당합니다.  12 LOAD_CONST 0 (None): 상수 None을 로드합니다.  14 RETURN_VALUE: None을 반환하고 코드 실행을 종료합니다.    dis.dis('d.update({k: v})')   - output -   0           0 RESUME                   0  1           2 LOAD_NAME                0 (d)             4 LOAD_METHOD              1 (update)            26 LOAD_NAME                2 (k)            28 LOAD_NAME                3 (v)            30 BUILD_MAP                1            32 PRECALL                  1            36 CALL                     1            46 RETURN_VALUE      0 RESUME 0: 이는 코드의 첫 번째 라인을 나타냅니다. RESUME은 실행을 재개하는 지점을 나타내는 명령입니다. 여기서는 실행을 계속 진행합니다.  2 LOAD_NAME 0 (d): 변수 d를 로드합니다.  4 LOAD_METHOD 1 (update): update라는 메서드를 로드합니다. 이는 변수 d의 값을 업데이트하는 메서드일 수 있습니다.  26 LOAD_NAME 2 (k): 변수 k를 로드합니다.  28 LOAD_NAME 3 (v): 변수 v를 로드합니다.  30 BUILD_MAP 1: 로드한 변수 k와 v를 사용하여 사전 형태의 매핑을 만듭니다.  32 PRECALL 1: 메서드 호출을 준비합니다.  36 CALL 1: update 메서드를 호출하고, 변수 d에 매개변수로 생성한 매핑을 전달합니다. 이를 통해 변수 d를 업데이트합니다.  46 RETURN_VALUE: 결과값을 반환하고 코드 실행을 종료합니다.    하지만, 각각의 상황에 맞추어서 사용하는 것이 중요             비교 해보기           Func_else       def Func_else(n):     if n % 2 == 0:         return True     else:         return False dis.dis(Func_else)    ## -------------------- output -------------------- ## 1           0 RESUME                   0    2           2 LOAD_FAST                0 (n)             4 LOAD_CONST               1 (2)             6 BINARY_OP                6 (%)            10 LOAD_CONST               2 (0)            12 COMPARE_OP               2 (==)            18 POP_JUMP_FORWARD_IF_FALSE     2 (to 24)    3          20 LOAD_CONST               3 (True)            22 RETURN_VALUE    5     &gt;&gt;   24 LOAD_CONST               4 (False)            26 RETURN_VALUE ## ------------------------------------------------ ##                Func_elif       def Func_elif(n):     if n % 2 == 0:         return True     elif n % 2 == 1:         return False dis.dis(Func_elif)    ## -------------------- output -------------------- ## 1           0 RESUME                   0    2           2 LOAD_FAST                0 (n)             4 LOAD_CONST               1 (2)             6 BINARY_OP                6 (%)            10 LOAD_CONST               2 (0)            12 COMPARE_OP               2 (==)            18 POP_JUMP_FORWARD_IF_FALSE     2 (to 24)    3          20 LOAD_CONST               3 (True)            22 RETURN_VALUE    4     &gt;&gt;   24 LOAD_FAST                0 (n)            26 LOAD_CONST               1 (2)            28 BINARY_OP                6 (%)            32 LOAD_CONST               4 (1)            34 COMPARE_OP               2 (==)            40 POP_JUMP_FORWARD_IF_FALSE     2 (to 46)    5          42 LOAD_CONST               5 (False)            44 RETURN_VALUE    4     &gt;&gt;   46 LOAD_CONST               0 (None)            48 RETURN_VALUE ## ------------------------------------------------ ##                Func_if       def Func_if(n):     if n % 2 == 0:         return True     return False dis.dis(Func_if)    ## -------------------- output -------------------- ## 1           0 RESUME                   0    2           2 LOAD_FAST                0 (n)             4 LOAD_CONST               1 (2)             6 BINARY_OP                6 (%)            10 LOAD_CONST               2 (0)            12 COMPARE_OP               2 (==)            18 POP_JUMP_FORWARD_IF_FALSE     2 (to 24)    3          20 LOAD_CONST               3 (True)            22 RETURN_VALUE    4     &gt;&gt;   24 LOAD_CONST               4 (False)            26 RETURN_VALUE ## ------------------------------------------------ ##                전체 실행 시간 측정       # 실행 시간 측정 함수 def measure_execution_time(func, n):     start_time = timeit.default_timer()     result = func(n)     end_time = timeit.default_timer()     execution_time = end_time - start_time     return execution_time, result    # 실행 시간 측정 n = 1  # 테스트할 값    # Func_else(n)의 실행 시간 측정 execution_time_else, result_else = measure_execution_time(Func_else, n) print(f\"Func_else 실행 시간: {execution_time_else} 초\") print(f\"Func_else 결과: {result_else}\")    # Func_elif(n)의 실행 시간 측정 execution_time_elif, result_elif = measure_execution_time(Func_elif, n) print(f\"Func_elif 실행 시간: {execution_time_elif} 초\") print(f\"Func_elif 결과: {result_elif}\")       # Func_if(n)의 실행 시간 측정 execution_time_if, result_if = measure_execution_time(Func_if, n) print(f\"Func_if 실행 시간: {execution_time_if} 초\") print(f\"Func_if 결과: {result_if}\")           - result -       Func_else 실행 시간: 1.989305019378662e-06 초 Func_else 결과: False Func_elif 실행 시간: 1.5348196029663086e-06 초 Func_elif 결과: False Func_if 실행 시간: 1.4528632164001465e-06 초 Func_if 결과: False           ","categories": ["Python_Module"],
        "tags": ["Python_Module"],
        "url": "/python_module/Dis/",
        "teaser": null
      },{
        "title": "[Shell Script] retry_script.sh - 코드 자동 재실행 쉘 스크립트",
        "excerpt":"코드 자동 재실행 쉘 스크립트   max_attempts=10000 attempt=1 exit_code=1 ​ while [[ $attempt -le $max_attempts &amp;&amp; $exit_code -ne 0 ]]; do     echo \"Attempt $attempt\"     python recipe_crawl.py # 원하는 파일으로 수정     exit_code=$?     ((attempt++))     sleep 300 # 5분후 재실행 done ​ if [[ $exit_code -ne 0 ]]; then     echo \"Failed after $max_attempts attempts\" fi      max_attempts 변수에 최대 시도 횟수를 설정합니다. 이 변수는 스크립트가 실행될 때 변경할 수 있습니다.   attempt 변수를 1로 초기화합니다. 이 변수는 현재 시도 횟수를 나타냅니다.        exit_code 변수를 1로 초기화합니다. 이 변수는 recipe_crawl.py 스크립트의 종료 코드(exit code)를 저장합니다.              while 루프에서 현재 시도 횟수가 최대 시도 횟수 이하이고 exit_code가 0이 아닐 때까지 반복합니다.각 시도에서 “Attempt [시도 번호]” 메시지를 출력합니다.       python recipe_crawl.py 명령을 실행하여 recipe_crawl.py 스크립트를 실행합니다.       exit_code 변수에 recipe_crawl.py 스크립트의 종료 코드를 저장합니다.       attempt 변수를 증가시킵니다.       sleep300초(5분) 동안 대기합니다.           while 루프가 종료되면, 최대 시도 횟수를 모두 사용했거나 recipe_crawl.py 스크립트의 종료 코드가 0이 아닌 경우 “Failed after [시도 횟수] attempts” 메시지를 출력합니다.   이 스크립트를 실행하면 recipe_crawl.py 스크립트를 최대 시도 횟수까지 반복 실행하며, 실행이 실패한 경우 메시지가 출력됩니다. recipe_crawl.py 스크립트가 실행되는 동안 각 시도마다 5분씩 대기합니다. 필요에 따라 max_attempts 변수의 값을 조정하여 실행할 시도 횟수를 변경할 수 있습니다.      실행 : bash retry_script.sh   ","categories": ["DevTools"],
        "tags": ["DevTools","Shell Script"],
        "url": "/devtools/retry_script/",
        "teaser": null
      },{
        "title": "[Talk] 두런두런 회고",
        "excerpt":"   1. 어쩌다 데이터 사이언티스트      메타 인지의 중요성    첫 강의에서 가장 기억에 남은 것은 메타 인지와 회고의 중요성이었다.  메타인지는 간단하게 요약하면 “너 자신을 알라”였던 것 같다.  자신을 객관화하여 보고 아는 것과 알지 못하는 것을 구분하는 능력이 중요다하는 것을 설명하였는데,  그 당시에는 새로 배우는 것이 많고, 이미 알고 있다고 생각했던 것에서 더 깊은 지식들이 나오면서  뭘 모르는지 모르는 상태여서 매우 공감이 되었다.         공감이 많이 되던 짤    부캠을 보내며 그 당시에는 절망의 계곡에 위치해있었던것 같다.  글을 쓰는 지금도 깨달음의 오르막을 오르기 위해 노력 중이지만, 내가 어떤 것을 모르고 있는 지 정확하게 파악하는 것은 아직 어려운 것 같다.   2. 직군 이야기, 삶의 지도      성장곡선과 고민상담소    “다른 사람보다 많이 부족한 것 같아요”와 같은 고민을 가진 사람들에게 많은 도움이 되었을 것 같은 강의였다.  다양한 성장곡선이 있지만, 결론은 노력을 계속 해나간다면 우상향을 할 것이라는 것이 핵심이였다.  부캠을 진행하면서 내가 성장을 하고 있긴 한건가라는 고민을 계속 하였다.  이번에 회고 글을 작성하면서 발표자료들을 살펴보니 다시 기억이 떠올랐는데, 그 당시에, 하루 단위로 보고 비교한다면 정체되는 날도 있고 떨어지는 날도 있다고 하였었다.  오르는 주식도 떨어지는 날도 있으니 단위를 크게 보면 우상향을 그리고 있을 거라고 하셨는데, 길어도 주 단위로 바라보며 저번주보다 달라진게 없는 것 같은데 하며 조바심을 가졌던 것 같다.  처음 시작했을 때랑 비교한다면 지금은 많은 성장이 있었던것 같아 마음이 조금은 편해지기는 한것 같지만, 아직은 부족한 부분이 많은 것 같다.   3. 고민 상담소, 이력서      🥕마스터님, 이력서를 써야 하는데 이력이 없어요🥕    가장 큰 고민이 아닐까 싶은 내용이었다.  강의를 들어보면서 느낀점은, 이력서에 쓸 내용을 너무 거창하게 생각해서 쓰기 어려운 것이 아닐까라고 느꼈다.  물론 외부 대회를 나가거나 해커톤에 참여하여 좋은 성적을 받은 것을 작성하는 것도 좋지만, 프로젝트를 진행하며 고민한 내용과 해결 과정을 잘 보여주는 것 중요한것 같았다.  특히 단순히 나열하는 것보다 자신의 노력과 문제를 겪었을 때 해결하는 과정을 잘 보여주면서, 보는 사람을 배려하는 이력서를 작성하여야 된다는 것이였는데, 적고나서 보니 여전히 이력서를 쓰는 것은 쉽지 않아 보인다.   4. 커리어 프레임워크, 산업      앞으로의 커리어와 진로탐색    엔지니어 커리어 프레임워크에 대해서 설명을 해주셨는데, 드랍박스 부분이 특히 인상 깊었다.  메타인지와 연관 시켜, 객관적인 지표처럼 활용을 할 수 있어, 내가 부족한 점을 파악하고 어떤 부분을 채워야 될지 알 수 있어서 좋았다.   정리하며   이제 얼마 남지 않은 기간을 남겨두었고, 두런두런 회고를 정리하면서 앞으로 어떻게 성장해야 되는지 다시 한번 되돌아 보는 기회가 된것 같다.  처음 부캠을 들어 올때는 취업 생각만 있었지만, 두런두런을 진행하면서 내가 원하는 길을 가기 위해 필요한 역량들이 어떤것인지 구체화 할 수 있는 기회가 되었고, 대학원에 진학에 대해서도 생각을 해보게 되었다.     ","categories": ["BoostCamp_AI"],
        "tags": ["Talk"],
        "url": "/boostcamp_ai/durundurun/",
        "teaser": null
      },{
        "title": "[TechTalk] 이메일에 쓰던 @가 왜 코드에 있나요?",
        "excerpt":"[TechTalk] 이메일에 쓰던 @가 왜 코드에 있나요?      Boostcamp AI tech 5th에서 테크톡 스피커에 선정 되어 발표를 하게 되었을 때, 사용한 코드 및 설명입니다  코드를 먼저 작성하고 아래 해설을 적는 방식으로 글을 작성하였는데, 해설을 바로 보는 것보다 코드를 보고 유추해보고 해설을 보는 것을 추천드려요     발표 영상 - https://youtu.be/rFgSqHlHZTw    01. 데코레이터를 테크톡 주제로 선정한 이유                https://github.com/twitter/the-algorithm-ml/blob/main/core/train_pipeline.py                  https://github.com/twitter/the-algorithm-ml/blob/main/tools/pq.py        오픈 소스 코드를 참고하기 위해서 코드를 읽을 때, 알 수 없는 @ 문법들이 있어서, 어려움을 겪었습니다.  데코레이터 문법은 각자 맡은 기능에 따라 다르게 작동하기 때문에, 모른다면 코드 해석이 안되는 경우가 생겨서 힘들었었고, 시행착오를 겪으면서 습득한 지식들을 정리해서 공유하면 도움이 될거라 생각해서 주제로 선정하게 되었습니다.    02. @ 데코레이터의 기능      함수나 클래스의 동작을 수정하지 않고도 추가 기능을 부여  유연성은 코드의 재사용성과 유지보수성을 크게 향상    03. 기본 개념: 함수는 객체화되어있다   - 함수는 변수에 할당 가능하다:   def say_hello(): print(\"Hello!\") greeting = say_hello  # 함수 할당 greeting()  # \"Hello!\" 출력  ## ----------------- output ----------------- ## &gt; Hello!   - 함수는 함수내에서 생성 가능하다:   def get_multiplier(factor):     def multiply(num):         return num * factor     return multiply double = get_multiplier(2)  # factor가 2인 multiply 함수 생성 print(double(5))  # 10 출력  ## ----------------- output ----------------- ## &gt; 10   - 함수는 다른 함수의 인자로 전달 가능하다:   def greet(func): func()  # 전달받은 함수 호출 def say_hello(): print(\"Hello!\") greet(say_hello)  # \"Hello!\" 출력  ## ----------------- output ----------------- ## &gt; Hello!      특히 마지막 “함수는 다른 함수의 인자로 전달 가능하다” 부분이 중요한데 데코레이터도 함수를 함수 인자로 활용한다고 할 수 있습니다    04. @ Decorator   def deco(f):     print('데코레이터')     return f   - 04_01 데코레이터 사용   @deco def double(num):     return 2*num    ## ----------------- output ----------------- ## &gt; 데코레이터      @deco가 실행되는 시점에서 deco 데코레이터 함수가 호출되고, deco 함수 내의 print(‘데코레이터’) 문장이 실행됩니다  이후 deco 함수는 인자로 전달받은 f 함수(여기서는 double 함수)를 그대로 반환합니다  따라서 출력 결과로 “데코레이터”가 출력됩니다    print(double(20))  ## ----------------- output ----------------- ## &gt; 40      그 후 double 함수에 인자를 20으로 주고 실행하면 20을 두배 한 값인 40이 출력됩니다    - 04_02 데코레이터 사용 X   def double_(num):     return 2*num double_ = deco(double_)  ## ----------------- output ----------------- ## &gt; 데코레이터      데코레이터를 풀어서 설명한 코드입니다 앞의 @deco를 사용했던 코드와 같은 기능을 합니다  쉽게 말하자면 double_함수를 선언한 후에 double_ = deco(double_)을 했다고 생각하면 됩니다    print(double_(20))  ## ----------------- output ----------------- ## &gt; 40      결과도 데코레이터를 사용한 double 함수와 같은 값을 반환합니다    - 04_03 주의할 점   def deco(f):     print('데코레이터')     return 'deco deco'   @deco def double(num):     return 2*num    ## ----------------- output ----------------- ## &gt; 데코레이터   print(double(20))  ## ----------------- output ----------------- ## --------------------------------------------------------------------------- TypeError                                 Traceback (most recent call last) Cell In[41], line 1 ----&gt; 1 print(double(20))  TypeError: 'str' object is not callable      - 타입 에러가 발생 -  deco함수의 리턴이 “deco deco” 문자열이 되면서 이제는 double 함수가 아닌 문자열 “deco deco”로 변경됨  deco 함수에서 f를 그대로 반환하지 않으면 가로채버리는 결과가 나와버린다    05. @ 데코레이터가 함수처럼 사용 된다면   - 05_01 데코레이터 사용   class lol():     def __init__(self, num):         print(f'init: {num}')         self.num = num          def __call__(self, f):         print('call')         return f  @lol(5) # @lol 데코레이터에 5를 인자로 넣어줌 def damage(n):     return 10 * n  ## ----------------- output ----------------- ## &gt; init: 5 &gt; call   - 05_02 데코레이터 사용 X   class lol():     def __init__(self, num):         print(f'init: {num}')         self.num = num          def __call__(self, f):         print('call')         return f  def damage(n):     return 10 * n damage = lol(5)(damage)  ## ----------------- output ----------------- ## &gt; init: 5 &gt; call      아까 설명한 방식과 똑같이 @뒤에 있는 것을 함수처럼 써서 재할당을 하고 있는 것을 볼 수 있다  그래서 damage는 lol(5)(damage)로 표현이 된다  이런 표현이 자연스럽게 나타날 수 있는 문법은 class이다  클래스가 처음 선언 될때 __init__이 선언되며, 이걸 다시 함수처럼 사용을 할 때에는 __call__ 함수가 실행  그래서 같은 결과로 둘다 init, call 순서로 출력이 된다    - 05_03 데코레이터에 인자 사용 X   class lol():     def __init__(self, num):         print(f'init: {num}')         self.num = num          def __call__(self, f):         print('call')         return f  @lol def damage(n):     return 10 * n   ## ----------------- output ----------------- ## &gt; init: &lt;function damage at 0x1051b84c0&gt;      인자를 사용 안해도 코드가 작동하는 등 다양하게 활용됨    06. @staticmethod   class MathUtils:     def __init__(self, a, b):         self.a = a         self.b = b              @staticmethod     def add(a, b):         return a + b          def multiply(self):         return self.a * self.b   sum_result = MathUtils.add(3, 5) print(sum_result)  ## ----------------- output ----------------- ## &gt; 8      add 함수의 결과가 출력되어 3 + 5의 결과로 8이 출력    product_result = MathUtils.multiply(3, 5) print(product_result)  ## ----------------- output ----------------- ## --------------------------------------------------------------------------- TypeError                                 Traceback (most recent call last) Cell In[9], line 1 ----&gt; 1 product_result = MathUtils.multiply(3, 5)       2 print(product_result)  TypeError: multiply() takes 1 positional argument but 2 were given      1개의 포지션에 2개의 값이 주어졌다며 TypeError가 발생    product_result = MathUtils.multiply(3) print(product_result)  ## ----------------- output ----------------- ## --------------------------------------------------------------------------- AttributeError                            Traceback (most recent call last) Cell In[66], line 1 ----&gt; 1 product_result = MathUtils.multiply(3)       2 print(product_result)  Cell In[55], line 11, in MathUtils.multiply(self)      10 def multiply(self): ---&gt; 11     return self.a * self.b  AttributeError: 'int' object has no attribute 'a'      MathUtils 클래스의 multiply() 메서드를 호출할 때 인자가 부족하여 AttributeError 발생    # 클래스 인스턴스 생성 math_utils = MathUtils(3,5) # 일반 메서드 호출 product_result = math_utils.multiply() print(product_result)  ## ----------------- output ----------------- ## &gt; 15      다음 코드를 실행시키려면 평소에 하던 것처럼 오른쪽과 같이 클래스 인스턴스를 생성하고 호출을 해야 된다  정적 메소드의 경우 주로 유틸리티성 함수를 위한 용도로 사용이 되며, 클래스의 속성에 변화를 일으키지 않는 함수  그러면 왜 사용하는지 의문점이 들 수 있다  이 부분에 대해서는 각자의 취향 차이이며, 클래스와 연관이 깊은 함수라고 의미를 부여하는 방법으로 사용할 수 있다  그래서 이 부분에는 협업시에 규칙을 정할 필요가 있다    07. @classmethod   class BoostCamp:     camper_name: str = \"Booduck\"      def o_change(self, name: str):         self.camper_name = name          @classmethod     def a_change(cls, name: str):         cls.camper_name = name   a, b = BoostCamp(), BoostCamp() print(f\"a: {a.camper_name}, b: {b.camper_name}\")  ## ----------------- output ----------------- ## &gt; a: Booduck, b: Booduck      클래스의 인스턴스인 a와 b를 생성하고 둘 다 camper_name은 초기값인 “Booduck＂인것을 확인 할 수 있다    - 07_01 “o_change” 메서드 사용 [@classmethod 선언 X]   a.o_change(\"DuckDuck\") print(f\"a: {a.camper_name}, b: {b.camper_name}\")  ## ----------------- output ----------------- ## &gt; a: DuckDuck, b: Booduck      a의 camper_name 속성이 “DuckDuck”으로 변경되었지만, b의 camper_name 속성은 변경되지 않았다  이는 o_change 메서드가 a 인스턴스의 속성을 변경한 것이기 때문이다  클래스 변수인 camper_name은 인스턴스 간에 공유되지 않으므로 b의 camper_name은 여전히 “Booduck”으로 유지된다    - 07_02 “a_change” 메서드 사용 [@classmethod 선언]   a.a_change(\"DuckDuck\") print(f\"a: {a.camper_name}, b: {b.camper_name}\")  ## ----------------- output ----------------- ## a: DuckDuck, b: DuckDuck      a와 b의 camper_name 속성 값이 모두 “DuckDuck＂으로 변경 되었다    - 07_03 이유         일반 메소드인 o_change를 선언하면 자기 자신의 인스턴스를 가리키게 된다  @classmethod를 붙이면 첫번째 인자가 자기 자신 인스턴스가 아니라 자기자신의 소속 클래스를 가리키게 되며, 그래서 어떤 인스턴스의 classmethod에서 첫번째 인자에 접근해서 값을 수정하게 된다면 다른 인스턴스에도 영향을 끼치게 된다    08. @property      @property는 간단하게 설명하자면 함수를 변수처럼 사용하게 해준다고 생각하면 된다    class Circle:     def __init__(self, radius):         self._radius = radius      @property     def radius(self):         return self._radius      @radius.setter     def radius(self, value):         if value &lt; 0:             raise ValueError(\"Radius cannot be negative\")         self._radius = value      @property     def area(self):         return 3.14 * self._radius * self._radius               # 인스턴스 생성 my_circle = Circle(5) # 속성 접근 print(f'my_circle.radius: {my_circle.radius}') print(f'my_circle.area: {my_circle.area}') # 속성 변경 my_circle.radius = 7 print(f'my_circle.radius: {my_circle.radius}') print(f'my_circle.area: {my_circle.area}')  ## ----------------- output ----------------- ## &gt; my_circle.radius: 5 &gt; my_circle.area: 78.5 &gt; my_circle.radius: 7 &gt; my_circle.area: 153.86      Circle 클래스에는 radius와 area라는 두 개의 속성이 있다. radius 속성은 @property 데코레이터를 사용하여 읽기 전용으로 정의되었다  이렇게 하면 my_circle.radius와 같이 속성에 접근할 때 메서드처럼 호출되지만 실제로는 해당 속성의 값을 반환한다     radius 속성에는 @radius.setter 데코레이터를 사용하여 설정 메서드를 정의  이렇게 하면 my_circle.radius = 7과 같이 속성에 값을 할당할 때 호출되는 메서드  이 설정 메서드에서는 입력된 값이 음수인지 검사하고, 음수인 경우 ValueError 예외를 발생시키며, 그렇지 않은 경우에만 속성 값을 변경한다    my_circle.area = 10  ## ----------------- output ----------------- ## --------------------------------------------------------------------------- AttributeError                            Traceback (most recent call last) Cell In[35], line 1 ----&gt; 1 my_circle.area = 10  AttributeError: property 'area' of 'Circle' object has no setter      property를 사용할 때 setter를 정의하지 않으면 해당 속성은 읽기 전용이 된다  즉, 속성에 값을 할당하려고 시도하면 오류가 발생하게 된다     그래서 property를 사용할 때는 해당 속성이 읽기 전용인지 또는 값을 변경할 수 있는지에 대해 정확히 구현해야 한다 정리하자면 @property를 사용하면서 setter를 정의하지 않는 경우 해당 속성은 읽기 전용이 되어 값을 변경할 수 없다    my_circle.radius = 8      *.radius는 정상 실행이 된다    09. @contextmanager   from contextlib import contextmanager import os  @contextmanager def file_open(file_path, mode):     # -------------- 사전 처리 -------------- #     dir_name = os.path.dirname(file_path)     if not os.path.isdir(dir_name):         os.makedirs(dir_name, exist_ok=True)     print('1: 파일 열기')     file = open(file_path, mode)     try:         print('2: 파일 객체 반환')     # ------------------------------------- #         yield file     # -------------- 사후 처리 -------------- #     finally:         file.close()         print('4: 파일 닫기')     # ------------------------------------- #   # 컨텍스트 관리자를 사용하여 파일 열기 with file_open('data/example.txt', 'w') as file:     file.write('Hello, Booduck!!')     print('3: 파일 작성')      ## ----------------- output ----------------- ## &gt; 1: 파일 열기 &gt; 2: 파일 객체 반환 &gt; 3: 파일 작성 &gt; 4: 파일 닫기      예를 들어 데이터셋 파일을 저장하는 과정에서 폴더를 지정했는데 없을 경우에는 에러가 발생한다  그럴때에는 폴더를 확인하고 없으면 생성하는 코드가 필요하며, 이러한 것들을 간단하게 해주는 것이 contextmanger이다     쉽게 말하면 사전 처리와 사후 처리를 한번에 해준다고 생각하면 된다     yield를 기준으로 사전처리, 사후처리를 해준다     추가로 Yield는 return과 달리 반환을 하고 나서도 파이썬 상에서 위치를 기억하고 있다고 생각하면 쉽게 이해 할 수 잇다  그래서 사전처리 후에 with로 돌아가서 파일을 작성한후에 다시 되돌아와서 파일을 닫는 과정까지 가능하게 한다     추가로 try, finally를 사용하고 있는데 그냥 에러가 발생해도 finally는 실행하고 에러를 띄우라는 뜻이다     이외에도 다양한 방식으로 사용되며, 여러 가지 케이스들은 코드를 접해가며 경험을 쌓아야된다    10. @wraps   - 10_01 @wraps(func) 사용   from functools import wraps def logger(func):     @wraps(func)  # 원본 함수의 메타데이터 유지     def wrapper(*args, **kwargs):         \"\"\"이제부터 이 함수는 wrapper입니다\"\"\"         print(f\"Calling function: {func.__name__}\")         result = func(*args, **kwargs)         print(f\"Function {func.__name__} returned: {result}\")         return result     return wrapper  @logger def add(a, b):     \"\"\"두 수를 더하는 함수입니다.\"\"\"     return a + b  # 데코레이터를 적용한 함수 호출 result = add(3, 4) print(f\"result: {result}\") # 메타데이터 유지 확인 print(f\"add.__name__: {add.__name__}\") print(f\"add.__doc__: {add.__doc__}\")   ## ----------------- output ----------------- ## &gt; Calling function: add &gt; Function add returned: 7 &gt; result: 7 &gt; add.__name__: add &gt; add.__doc__: 두 수를 더하는 함수입니다.   - 10_02 @wraps(func) 사용 X   from functools import wraps def logger(func):     def wrapper(*args, **kwargs):         \"\"\"이제부터 이 함수는 wrapper입니다\"\"\"         print(f\"Calling function: {func.__name__}\")         result = func(*args, **kwargs)         print(f\"Function {func.__name__} returned: {result}\")         return result     return wrapper  @logger def add(a, b):     \"\"\"두 수를 더하는 함수입니다.\"\"\"     return a + b  # 데코레이터를 적용한 함수 호출 result = add(3, 4) print(f\"result: {result}\") # 메타데이터 유지 확인 print(f\"add.__name__: {add.__name__}\") print(f\"add.__doc__: {add.__doc__}\")   ## ----------------- output ----------------- ## &gt; Calling function: add &gt; Function add returned: 7 &gt; result: 7 &gt; add.__name__: wrapper &gt; add.__doc__: 이제부터 이 함수는 wrapper입니다.   - 10_03 정리      Return 해주는 함수 위에 wraps에 함수를 인자로 받아서 데코레이터로 사용하는 것은 함수의 겉껍데이(이름, doc)만 변경한다     - @wraps가 사용된 경우 -  add.__name__: add  add.__doc__: 두 수를 더하는 함수입니다.     - @wraps가 없는 경우 -  add.__name__: wrapper  add.__doc__: 이제부터 이 함수는 wrapper입니다     @wraps가 없는 경우 함수의 이름이 wrapper로 변경되어 있는 것을 확인할 수 있다  이렇게 되면 나중에 다른 사람이 디버깅하는 과정에서 함수의 이름을 확인할 때 파악이 힘들 수가 있다  그래서 함수 껍데기를 옮겨주겠다는 뜻인 @wrap을 설정해주면 함수의 이름을 확인할 때 편하게 확인할 수 있다    11. @abstractmethod   from abc import ABC, abstractclassmethod  class BoostCamper(ABC):     @abstractclassmethod     def track(self):         pass          @abstractclassmethod     def team_number(self):         pass   - 11_01 Try Case      시도한 케이스들을 보고 @abstractclassmethod에 파악해 보세요    - Try 01:   booduck = BoostCamper()  ## ----------------- output ----------------- ## --------------------------------------------------------------------------- TypeError                                 Traceback (most recent call last) Cell In[175], line 1 ----&gt; 1 booduck = BoostCamper()  TypeError: Can't instantiate abstract class BoostCamper with abstract methods team_number, track      TypeError : “추상 메서드 “team_number, track”을 사용하여 추상 클래스 “boostcamer”를 인스턴스화할 수 없습니다”    - Try 02:   class Booduck(BoostCamper):     def track(self):         print('Recsys')  duck = Booduck()  ## ----------------- output ----------------- ## --------------------------------------------------------------------------- TypeError                                 Traceback (most recent call last) Cell In[170], line 5       2     def track(self):       3         print('Recsys') ----&gt; 5 duck = Booduck()  TypeError: Can't instantiate abstract class Booduck with abstract method team_number      TypeError: “추상 메서드 “team_number”를 사용하여 추상 클래스 “Booduck”을 인스턴스화할 수 없습니다”    - Try 03:   class Booduck(BoostCamper):     def track(self):         print('Recsys')      def team_number(self):         print('10')      정상적으로 선언이 된다    - 11_02 정리      예를 들어, 보통 게임 같은 것을 만들때, 캐릭터가 여러개 있을때 클래스를 여러개 만들게 된다  그때 클래스의 클래스에 구현해야 될 기능들을 모아두고 상속을 받아오면 실수를 방지 할 수 있다     정리하자면 상속을 받을때 구현 해야 하는 것을 구현하지 않으면 에러를 띄워주는 기능을 한다     그래서 깃허브에서 abc.ABC를 선언하고 abstractmethod를 쓴 클래스는 직접 인스턴스로 선언되는게 아니라, 다른 클래스로 상속해주는 용도로 사용하겠다는 것이다        마치며        데코레이터 테크톡을 준비하면서, 다양한 종류의 데코레이터들을 접하고 파이썬 버전이 업그레이드 되면서 새로운 데코레이터들이 등장하는 것을 보았습니다.     하지만 “데코레이터는 다른 함수나 메서드에 추가적인 기능을 주입할 수 있게 해주는 디자인 패턴”이라는 것을 잘 이해하고 있다면, document를 통해 어떻게 작동하는 지 잘 파악할 수 있을 것이라 생각합니다.     지금 당장 다 외우지는 못해도 위의 개념만 이해하고 있다면 경험을 통해 쌓아가면 된다고 생각합니다. 감사합니다.        Reference        PEP 318 – Decorators for Functions and Methods - https://peps.python.org/pep-0318/     PEP 557 – Data Classes - https://peps.python.org/pep-0557/     PEP 698 – Override Decorator for Static Typing - https://peps.python.org/pep-0698/     PEP 3129 – Class Decorators - https://peps.python.org/pep-3129/     코딩빌런  CodingVillain - https://youtube.com/playlist?list=PL6R2CbVlhcYtnLsypcisMzDN344c1Conr     + ChatGPT   ","categories": ["BoostCamp_AI"],
        "tags": ["TechTalk"],
        "url": "/boostcamp_ai/TechTalk/",
        "teaser": null
      },{
        "title": "[MongoDB] MongoDB - 기본 설치",
        "excerpt":"Mongo DB      MongoDB 설치하기:            터미널을 열고 다음 명령을 실행하여 MongoDB를 설치합니다:          sudo apt-get update  sudo apt-get install mongodb                           Python MongoDB 드라이버 설치하기:            MongoDB와 Python을 연동하기 위해 PyMongo를 설치합니다. 터미널에서 다음 명령을 실행합니다:          pip install pymongo                           MongoDB 서버 실행하기:            MongoDB를 실행하기 위해 터미널에서 다음 명령을 실행합니다:          sudo service mongodb start                           Python에서 MongoDB 사용하기:            Python 스크립트에서 MongoDB를 사용하기 위해 다음과 같이 PyMongo를 import합니다:          from pymongo import MongoClient                       MongoDB에 연결하려면 MongoClient를 사용하여 연결을 설정합니다:          client = MongoClient('localhost', 27017)  # MongoDB 호스트 및 포트 번호                       연결이 설정되면 데이터베이스를 선택하고 쿼리를 실행할 수 있습니다. 예를 들어, 데이터베이스 “mydb”에서 컬렉션 “mycollection”에 문서를 삽입하는 방법은 다음과 같습니다:          db = client['mydb']  # 데이터베이스 선택  collection = db['mycollection']  # 컬렉션 선택  data = {'name': 'John', 'age': 30}  result = collection.insert_one(data)                          ","categories": ["DevTools"],
        "tags": ["DevTools","MongoDB"],
        "url": "/devtools/MongoDB_install/",
        "teaser": null
      },{
        "title": "[Python Library] os 모듈",
        "excerpt":"OS           현재 작업 디렉토리 확인하기:       current_dir = os.getcwd() print(\"Current working directory:\", current_dir)                디렉토리 생성하기:       new_dir = \"new_directory\" os.mkdir(new_dir)                디렉토리나 파일이 존재하는지 확인하기:       path = \"path_to_directory_or_file\" exists = os.path.exists(path)     if exists:     print(\"Path exists:\", path) else:     print(\"Path does not exist:\", path)                디렉토리 내의 파일 목록 가져오기:       directory = \"path_to_directory\" files = os.listdir(directory)     print(\"Files in directory:\") for file in files:     print(file)                파일 이동하기:       src_file = \"source_file.txt\" dest_file = \"destination_directory/destination_file.txt\"     os.rename(src_file, dest_file)                파일 삭제하기:       file = \"file_to_delete.txt\"     os.remove(file)          ","categories": ["DevTools"],
        "tags": ["DevTools","Python Library"],
        "url": "/devtools/python_os_library/",
        "teaser": null
      },{
        "title": "[MongoDB] MongoDB - 외부 접속 설정",
        "excerpt":"MongoDB 외부 접속 설정하기   1. *.conf 파일 수정   - /etc/[mongod/mongodb].conf   # bind_ip = 127.0.0.1 # port = 27017  bind_ip=0.0.0.0 # 모든 IP 주소에서의 연결을 허용 port=* #원하는 포트 설정  # transparent_hugepage=always # 리눅스 운영체제의 Transparent Huge Pages 기능을 활성화  # 보안 설정 둘 중 한가지 활성화 #noauth = true # 보안상 위험 존재 auth = true      sudo service mongodb restart - 저장 완료 후 mongodb 재시작                   주의: bindIp를 모든 IP 주소로 설정하면 보안상의 위험을 초래할 수 있으므로 필요한 경우에만 사용                  transparent_hugepage: 메모리 관리 기술로, 프로세스의 가상 주소 공간을 대형 페이지라는 큰 단위로 관리하는 방식         MongoDB의 경우, transparent_hugepage=always 설정이 권장되지 않습니다. MongoDB는 메모리 관리 및 액세스 패턴과 관련하여 작은 페이지를 사용하는 것이 더 효율적이라고 판단되기 때문입니다. 따라서 MongoDB를 사용할 때는 transparent_hugepage 설정을 never로 변경하는 것이 좋습니다            2. mongodb 보안 설정   mongo &gt; use admin &gt; db.createUser(     {       user: \"adminUser\",       pwd: \"adminPassword\",       roles: [ { role: \"root\", db: \"admin\" } ]     } )   3. mongodb admin 접속   - shell [mongo]   mongo --host yourServerIP --port yourServerPort -u remoteUser -p remotePassword --authenticationDatabase admin   - python[pymongo]   import urllib.parse from pymongo import MongoClient  password = urllib.parse.quote_plus('*') client = MongoClient('mongodb://{id}:{pwd}@{ip}:{port}'.format(id='*', pwd=password, ip='*', port=*))   - MongoDB Compass (GUI)      mongodb를 GUI로 편하게 관리 가능  link - https://www.mongodb.com/try/download/compass      ","categories": ["DevTools"],
        "tags": ["DevTools","MongoDB"],
        "url": "/devtools/MongoDB_connect/",
        "teaser": null
      },{
        "title": "[Final Project] MongoDB에 이미지 저장하기 [python]",
        "excerpt":"[Final Project] MongoDB에 이미지 저장하기      최종프로젝트를 진행하며 key: value로 이루어진 MongoDB data에서 바로 저장된 이미지를 가져와서 썸네일로 사용 할 수 없을까 고민을 하였습니다.  딥러닝 학습을 위한 이미지 전처리 과정에서 이미지 데이터는 개별 원소로 이루어진 행렬의 집합이라는 것을 배웠었고 이것을 활용 하여 집합을 저장하고 불러와 사용 할 수 없을 까라는 고민을 해결하는 과정에서 작성한 코드입니다.  대회를 진행하며, 할당된 서버 용량이 부족하여 다른 팀원의 서버에 MongoDB를 구성하고, admin계정으로 다른 팀원 서버의 데이터베이스에 접속을 하여 구성하는 과정으로 진행을 하였기 때문에  이미지 파일을 따로 저장하는 데 어려움이 있어 해당 방법으로 해결을 하였습니다.     DB에 이미지를 저장하는 것은 병목을 발생 시킬 수 있기 때문에 이미지 메타정보만 저장하고 나머지는 물리적인 파일에 저장로 저장하는 것을 권장하는 편이니 실제로 사용하고자 한다면 트래픽이나 크기를 고려하여 결정하는 것을 권장합니다.    - base64   Base64는 바이너리 데이터를 텍스트 문자열 형식으로 인코딩하기 위한 방식 중 하나입니다. 주로 이메일과 같은 텍스트 기반의 전송 매체에서 바이너리 데이터를 전송할 때 사용됩니다. Base64는 64개의 출력 가능한 문자를 사용하여 모든 바이너리 데이터를 표현합니다.   - 바이너리 데이터를 Base64 문자열로 인코딩하기:   import base64  data = b\"hello world\"  # Base64로 인코딩 encoded_data = base64.b64encode(data) print(encoded_data.decode('utf-8'))  # 출력: aGVsbG8gd29ybGQ=   - Base64 문자열을 바이너리 데이터로 디코딩하기:   decoded_data = base64.b64decode(encoded_data) print(decoded_data.decode('utf-8'))  # 출력: hello world   - BytesIO   BytesIO는 Python의 io 모듈에 포함된 클래스로, 바이트 데이터에 대한 인메모리 파일 같은 객체를 제공합니다. 실제 파일을 디스크에 쓰고 읽는 대신, 바이트 데이터를 메모리 상에서 파일과 유사한 객체로 다룰 수 있게 해줍니다.   이를 통해 파일과 유사한 작업들 (예: 읽기, 쓰기, seek)을 메모리에서 직접 수행할 수 있습니다. 주로 바이너리 데이터를 임시로 저장하거나, 바이너리 형식의 파일과 상호작용하는 라이브러리에 바이트 데이터를 전달할 때 사용됩니다.   - BytesIO 객체 생성 및 데이터 쓰기:   from io import BytesIO  # BytesIO 객체 생성 byte_stream = BytesIO()  # 데이터 쓰기 byte_stream.write(b\"Hello, World!\")   - BytesIO 객체의 위치 조정 및 데이터 읽기:   # 읽기 전에 위치를 스트림의 시작으로 이동 byte_stream.seek(0)  # 데이터 읽기 read_data = byte_stream.read() print(read_data)  # 출력: b'Hello, World!'   - BytesIO 객체를 사용하여 이미지 데이터와 같은 바이너리 데이터 다루기:   from PIL import Image  # 이미지 데이터를 BytesIO 객체로 로드하기 image_data = b\"...\"  # 바이너리 이미지 데이터 image_stream = BytesIO(image_data)  # BytesIO 객체에서 이미지 로드 image = Image.open(image_stream) image.show()   - Baseline Code   - load_image   import requests def load_image(url):     response = requests.get(url)     binary_data = response.content     return binary_data      주어진 URL에서 이미지를 얻고 이를 이진 데이터 형식으로 반환하는 코드    - Example   IMAGE_LINK = \"https://logoproject.naver.com/img/img_story_renewal.png\" image_data = download_image(IMAGE_LINK) print(type(image_data)) print(image_data)   &gt; &lt;class 'bytes'&gt; &gt; b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\xdf\\x00\\x00\\x00+\\x08\\x06\\x00\\x00...   - thumbnail_encoder   import base64 def thumbnail_encoder(image_data: bytes):    \t\"\"\"     Encode image data to a base64 encoded string.     Parameters:     - image_data (bytes): Raw image data in bytes format.     Returns:     - str: A base64 encoded string representation of the input image data.     \"\"\"     img_encode = base64.b64encode(image_data).decode('utf-8')     return img_encode      바이너리 형태의 이미지 데이터를 받아서 base64로 인코딩된 문자열로 반환하는 코드  이렇게 하면 바이너리 데이터를 텍스트 형식으로 웹 페이지나 JSON 응답 등에서 사용할 수 있다.    - Example   IMAGE_LINK = \"https://logoproject.naver.com/img/img_story_renewal.png\" image_data = download_image(IMAGE_LINK) image_encode = thumbnail_encoder(image_data) print(type(image_encode)) print(image_encode)   &gt; &lt;class 'str'&gt; &gt; iVBORw0KGgoAAAANSUhEUgAAAN8AAAArCAYAAAD4+l0jAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJ...   - thumbnail_decoder      {key: value}로 저장된 dict형식의 데이터에서 value 값을 받아서 이미지로 반환 해주는 코드    import base64 from io import BytesIO from PIL import Image  def thumbnail_decoder(image_data: str):     \"\"\"     Decodes and converts the image data from base64 to an Image object.     Args:         image_data (str): The base64-encoded image data.     Returns:         PIL.Image.Image: The decoded image as a PIL Image object.     \"\"\"     try:         image_bytes = base64.b64decode(image_data)         try:             image = Image.open(BytesIO(image_bytes)).convert('RGB')         except:             image = Image.open(BytesIO(image_bytes)).convert('RGBA')         return image     except Exception as e:         # Handle any exceptions that might occur during decoding         raise ValueError(f\"Error decoding image data: {e}\")   - Example   image_data = load_image(IMAGE_LINK) image_encode = thumbnail_encoder(image_data) json_dict = {'naver_img': image_encode} thumbnail_decoder(json_dict['naver_img'])         실제로 mongodb에서 가져올 때에는 key값으로 검색을 하여, value를 가져와 value를 thumbnail_decoder에 바로 인자로 넣어주는 방식으로 구현했습니다.    - Result [MongoDB]         Mongodb Database - thumbnail_data    ","categories": ["BoostCamp_AI","Python_Module"],
        "tags": ["Final Project"],
        "url": "/boostcamp_ai/python_module/Mongodb_image_file/",
        "teaser": null
      },{
        "title": "[Final Project] Catboost 모델",
        "excerpt":"[Final Project] CatBoostClassifier 모델      레시피 추천 프로젝트를 진행하면서, 사용자가 클릭한 레시피와 같은 정보는 기업 내부 정보이기 때문에 얻을 수 없었기 때문에 유저나 남긴 평점을 기반으로 데이터셋을 구성하였습니다.  대부분 사용자-아이템 간의 상호 작용을 기반으로 하여 모델을 개발해서 인풋값에 사용자 id가 포함이 되어야 하는 경우가 생겼고, 새로운 유저가 추가되면 새롭게 모델을 학습을 하거나 연산을 해야 되는 문제들이 있었습니다.  그래서 이러한 의존성을 없애고자 연구를 진행 하였습니다.    01. datasets                  - User Ratings Dataset [user_df]       - Recipe Category Dataset [cate_df]                                               - uid: 유저 식별 번호 - 14983 users  - rid: 레시피 식별 번호  - 30241 recipes - time: 평점을 남긴 시간   - star: 평점 - 1 ~ 5 scores       - recipeid: 레시피 식별 번호 - 120829 recipes - cat1: 방법별 [볶음, 끓이기, 부침, 조림, …] - 14 types - cat2: 상황별 [일상, 접대, 술안주, 다이어트, …] - 14 types - cat3: 재료별 [소고기, 돼지고기, 밀가루, 버섯류, …] - 16 types - cat4: 종류별 [밑반찬, 찌개, 디저트, 양식, …] - 17 types - thumb_link: 썸네일 주소              공통 데이터셋 중에서 2개의 데이터셋을 사용하여 학습 데이터를 구성하였습니다.    02. 데이터 전처리   2.1 [user_df / cate_df] load &amp; Pre-processing           user_df       # 유저 레시피 평점 상호작용 데이터 로드 및 전처리 user_df = pd.read_csv(os.path.join(DATA_DIR_PATH, USER_DATA_CSV_NAME)) user_df.rename(columns={'rid': 'recipeid'}, inplace=True) user_df.drop(['star'], axis=1, inplace=True)                  cate_df와 병합을 위해 columns의 이름을 맞춰주고, 이번에는 평점 데이터를 삭제합니다.  목표가 유저가 관심을 가지고 접근한 레시피를 기반으로 추천을 하는 것이기 때문에 평점을 매겼다는 것은 “해당 레시피에 관심을 가지고 접근을 하였다”의 개념으로 처리하기 위해 코드를 작성하였습니다.                 cate_df       cate_df = pd.read_csv(os.path.join(DATA_DIR_PATH, CATEGORY_DATA_CSV_NAME)) cate_df = cate_df.drop(['thumb_link'], axis=1)    offset = 0 for col in cate_df.columns[1:]:     unique_vals = sorted(cate_df[col].unique())     mapping_dict = {val: i for i, val in enumerate(unique_vals, start=offset)}     cate_df[col] = cate_df[col].map(mapping_dict)     offset += len(unique_vals)                  카테고리를 원핫 인코딩을 했을 떄 접근 하기 쉽게 0 ~ 60 범위로 변경하였습니다         print(sorted(cate_df['cat1'].unique())) print(sorted(cate_df['cat2'].unique())) print(sorted(cate_df['cat3'].unique())) print(sorted(cate_df['cat4'].unique()))               &gt; [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13] &gt; [14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27] &gt; [28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43] &gt; [44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60]                  2.2 one-hot 인코딩           get_dummies()              pandas 라이브러리의 함수로, 주로 범주형 변수를 원-핫 인코딩 방식으로 변환할 때 사용            df_encoded = pd.get_dummies(cate_df, columns=[\"cat1\", \"cat2\", \"cat3\", \"cat4\"]) df_encoded = df_encoded.astype(int)                  원-핫 인코딩을 하는 과정에서 False, True로 표현이 되는 데, dataframe 타입을 나중에 합 연산을 하는 과정이 있기 때문에  bool -&gt; int로 변경하여 0, 1로 표현         - df_encoded                          user_df와 df_encoded 결합        new_columns = [\"recipeid\"] + [str(i) for i in range(len(df_encoded.columns) - 1)]  df_encoded.columns = new_columns \t  user_data_df = pd.merge(user_df, df_encoded, on=\"recipeid\").sort_values(by=[\"uid\", \"time\"]).reset_index(drop=True)  user_data_df = user_data_df.drop(['time'], axis=1)                 열 이름을 변경한 후에, “recipeid”를 기준으로 “user_df”와 “df_encoded”를 합치면서, uid 순으로 정렬한 후에 같은 uid 내에서 time에 따라 정렬을 해줍니다.  행들의 순서를 유지하면서 전처리를 할 예정이기 때문에 time을 제거하였습니다.         - user_data_df                     2.3 Train Dataset 생성           카테고리 그룹별 누적합 계산       df_grouped = user_data_df.groupby(\"uid\").apply(lambda x: x.iloc[:, 2:].cumsum()).reset_index(drop=True) df_grouped = pd.concat([user_data_df[[\"recipeid\"]], df_grouped], axis=1)                  user_data_df의 “uid”를 기준으로 그룹을 만들어 각 그룹 내에서 이전 행들의 값들을 현재 행에 더하여 누적 값을 얻습니다.  추가로 그룹화한 후 인덱스가 MultiIndex 형태로 설정되기 때문에 인덱스 초기화를 하여 원래데로 되돌린 후 user_data_df의 “recipeid” 열을 합쳐 “df_grouped”을 생성합니다.                     - 코드 최적화       cols_to_cumsum = user_data_df.columns[2:] df_grouped = user_data_df.copy() df_grouped[cols_to_cumsum] = user_data_df.groupby(\"uid\")[cols_to_cumsum].transform(lambda x: x.cumsum()) df_grouped = pd.concat([user_data_df[[\"recipeid\"]], df_grouped[cols_to_cumsum]], axis=1)                  보통 lamda가 결합하여 사용 될 경우에는 순차적으로 함수를 적용을 해서 큰 데이터를 처리할 때 시간이 오래 걸렸습니다.  그래서 더 좋은 방법이 있을까 찾던 중에 transform을 사용하면 좀 더 빠르게 처리가 된다는 것을 보았고 적용해 보았습니다.  시간은 apply는 1.6s, transform은 1.1s로 transform이 31.25% 빠른 속도를 보였습니다.            - apply와 transform의 속도 차이 이유                       결과의 형태:                        apply: 그룹별로 함수를 적용하고, 그 결과를 합쳐서 새로운 데이터 구조를 반환한다. 반환되는 구조는 적용된 함수와 그룹의 형태에 따라 달라진다.             transform: 그룹별로 함수를 적용하지만, 원래의 데이터와 동일한 shape의 결과를 반환한다. 이는 각 그룹에 대해 동일한 크기의 결과를 반환해야 함을 의미한다.                             속도:                        일반적으로 transform은 apply보다 더 빠르게 동작하는 경우가 많다. 이는 transform이 데이터의 원래 구조를 유지하기 위해 최적화되어 있기 때문이다. 또한 transform은 보통 단순한 요소별 연산에 더 적합하기 때문에 이러한 연산에서는 빠른 성능을 나타낼 수 있다.             반면, apply는 보다 일반적인 목적으로 사용되며, 그룹별로 다양한 연산을 수행할 수 있다. 따라서, apply는 transform보다 유연하지만, 일부 경우에는 더 느릴 수 있다.                             사용 사례:                        transform: 각 그룹에 동일한 연산을 적용하고 원래의 DataFrame 크기와 동일한 결과를 원할 때 주로 사용된다. 예를 들어, 각 그룹별 평균을 계산하여 원래의 데이터셋에 추가하고 싶을 때 사용할 수 있다.             apply: 그룹에 대해 더 복잡한 연산을 수행하거나, 그룹별로 다른 형태의 결과를 반환하고 싶을 때 사용된다.                                   결론적으로, transform은 원래의 데이터 구조를 유지하면서 그룹별로 간단한 연산을 수행하는 데 최적화되어 있기 때문에, 그에 해당하는 작업에서는 일반적으로 apply보다 더 빠른 성능을 보인다.                 각 행 별로 스케일링       max_values = df_grouped.iloc[:, 1:].max(axis=1) df_grouped.iloc[:, 1:] = df_grouped.iloc[:, 1:].div(max_values, axis=0).round(2)                  첫번째(recipeid)열을 제외한 모든 값들을 독립적으로 해당 행의 최대값으로 나누어 스케일링을 해주었습니다.  해당 코드를 통해 각각의 행들의 최대값을 1로 만들고, 다른 모든 값을 0과 1 사이의 값으로 변환하였습니다.                   출현 빈도가 작은 레시피 제거       # 레시피 상호작용이 10회 이하인 유저 제외 df_grouped = df_grouped.groupby('recipeid').filter(lambda x: len(x) &gt; 10) # 인덱스 초기화하면서 기존 인덱스 삭제 df_grouped = df_grouped.reset_index(drop=True)                  가장 고민이 많이 되었던 부분입니다.                  각 레시피의 출현 빈도를 나타낸 그래프입니다. 대다수의 레시피가 1번 출현한 적이 많았습니다. 처음에는 사분위수로 데이터를 자르려 했지만, Q1: 1.0, Q2: 1.0, Q3: 3.0으로 불균형이 심해서 임의로 출현 빈도가 10 이하의 레시피를 제거하였습니다.  자른 후에 총 레시피 종류는 2361개가 남았습니다.  중간에 했던 고민으로는 SMOTE로 데이터의 불균형을 해소 할 수 있을 까라고 고민을 하였지만, 소수 클래스에 적합한 개념이였고, 다중 클래스에 적용하기에는 적합하지 않은 것으로 판단되었습니다.         지금 생각하고 있는 개선법은 자르는 빈도를 다르게하여 학습시켜 모델의 테스트 성능을 비교하는 방법을 생각하고 있습니다.  그 과정에서도 클래스의 개수가 늘어나면서 추천의 가짓수는 커지지만 정확도는 떨어질 것으로 예상하고 있습니다.            03. CatBoostClassifier 학습      Github Link - https://github.com/boostcampaitech5/level3_recsys_finalproject-recsys-10/blob/main/Model/catboost_recipe_classifier/train.py    - loss_function      CatBoost 내의 MultiClass loss funtion으로 학습을 하였습니다.  CrossEntropyLoss랑 동일 하다고 하며 다중 클래스 분류 작업에 주로 사용되는 손실 함수입니다.                                    내용                                 장점         강건성: 로짓과 참 클래스 간의 차이를 극대화하면서 다른 클래스에 대한 예측 확률을 감소시키려고 합니다. 이 특성은 모델이 높은 확신을 가지고 예측할 때 더 나은 성능을 보여줍니다. 자동 정규화: Softmax 함수가 로짓을 확률 분포로 자동으로 변환하므로 로짓의 크기가 클 수록 모델의 확신도가 높아집니다. 수치 안정성: CrossEntropyLoss는 내부적으로 log와 softmax를 결합하므로 수치적 안정성이 있습니다. 일반적으로 잘 작동: 다중 클래스 분류 문제에서 잘 작동하는 것으로 알려져 있습니다. 많은 실제 문제와 벤치마크에서 효과적입니다.                       단점         한 클래스의 오버샘플링: 한 클래스의 샘플이 다른 클래스보다 훨씬 많을 경우, 모델은 많은 클래스를 선호할 수 있습니다. 이를 극복하기 위해 클래스 가중치나 샘플 가중치를 사용할 수 있습니다. 다중 레이블 분류 문제에 적합하지 않음: CrossEntropyLoss는 각 입력 샘플에 대해 하나의 클래스 레이블만을 예상합니다. 다중 레이블 분류를 위해서는 다른 손실 함수가 필요합니다.   수치적 불안정: 매우 큰 로짓 값은 softmax 함수에서 수치적 불안정을 초래할 수 있습니다. 하지만 대부분의 딥 러닝 프레임워크는 이 문제를 해결하기 위해 안정된 버전의 softmax와 로그를 사용합니다. 오버피팅: CrossEntropyLoss는 모델이 특정 클래스에 대해 매우 확신을 가질 때 더 적은 손실을 반환합니다. 이는 때로는 오버피팅을 초래할 수 있습니다.                               알면 좋은 내용                  CrossEntropyLoss가 다중 레이블 분류에 적합하지 않는 것을 알게 되었고, 그 당시에 다중 클래스와 개념이 헷갈려서 추가로 정리한 부분입니다.  - 다중 클래스와 다중 레이블 차이                      다중 클래스 (Multi-Class) 분류:                            각 입력 샘플은 정확히 하나의 클래스에만 속합니다.               예를 들어, 동물의 이미지를 ‘고양이’, ‘개’, ‘말’ 중 하나로 분류하는 문제는 다중 클래스 분류 문제입니다. 각 이미지는 위 세 가지 클래스 중 하나에만 속할 수 있습니다.                                   다중 레이블 (Multi-Label) 분류:                            각 입력 샘플은 여러 개의 레이블을 동시에 가질 수 있습니다.               예를 들어, 영화를 ‘액션’, ‘로맨스’, ‘판타지’, ‘코미디’ 등의 여러 장르로 분류하는 문제는 다중 레이블 분류 문제입니다. 한 영화는 동시에 ‘액션’과 ‘판타지’ 장르에 속할 수 있습니다.                                                         코드 구현         import torch import torch.nn as nn import numpy as np y_logits = np.array([[2.2, 0.8, 0.3], [-0.8, 1.5, 0.2], [0.5, 0.6, 2.1]]) y_true = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]]) tensor_y_true = torch.tensor([0, 1, 2], dtype=torch.long)                                   numpy 구현             def cross_entropy_loss_with_softmax(y_logits, y_true):     # Apply softmax     y_pred = np.exp(y_logits) / np.sum(np.exp(y_logits), axis=1, keepdims=True)     y_pred = np.clip(y_pred, 1e-15, 1-1e-15)              # Compute the loss     loss = -np.sum(y_true * np.log(y_pred)) / y_true.shape[0]     return loss                                        PyTorch 구현             tensor_y_true = torch.tensor([0, 1, 2], dtype=torch.long) tensor_y_logits = torch.Tensor(y_logits)      criterion = nn.CrossEntropyLoss(reduction='mean') loss = criterion(tensor_y_logits, tensor_y_true)                                        output             print(\"Numpy Loss with Softmax:\", cross_entropy_loss_with_softmax(y_logits, y_true)) print(\"PyTorch Loss:\", loss.item())      ## ----- output ----- ## Numpy Loss with Softmax: 0.3349219247071371 PyTorch Loss: 0.33492186665534973                                      - 아쉬운 점      catboost는 다른 부스팅 알고리즘에 비해서 하이퍼파라미터의 영향이 상대적으로 작은 경향이 있습니다.  CatBoost는 대칭 트리 구성 방식을 사용하여 모든 특성을 고르게 사용하도록 합니다. 이로 인해 개별 특성에 대한 처리나 선택에 대한 추가적인 하이퍼파라미터 튜닝이 필요하지 않다고 합니다. CatBoost의 개발자들은 사용자에게 기본값을 제공할 때 여러 실험을 통해 최적화된 값을 제공하여 대부분의 하이퍼파라미터를 기본값으로 두어도 상대적으로 괜찮은 성능을 얻을 수 있다고 합니다.     하지만 하이퍼파라미터의 영향이 아예 없는 것은 아니기 때문에 GridSearchCV나 optuna와 같은 하이퍼파라미터 튜닝을 적용하지 않은게 아쉬웠습니다. 당시에는 프로젝트의 완성에 좀 더 중점을 두었고, 모델 훈련 시간도 빠르지 않은 편이라서 중요도를 낮게 측정하였습니다.    04. 추가 실험      프로젝트가 끝나고 코드를 다시 살펴 보는 도중에 데이터 전처리 부분에서 개선 할수 있을 것 같은 부분이 있어 추가로 학습시킨 부분입니다.            추가 실험              2.3_2번 단계에서 카테고리 별로 독립적으로 스케일링을 했었습니다.            max_values_1 = df_grouped.iloc[:, 1:15].max(axis=1) max_values_2 = df_grouped.iloc[:, 15:29].max(axis=1) max_values_3 = df_grouped.iloc[:, 29:45].max(axis=1) max_values_4 = df_grouped.iloc[:, 45:62].max(axis=1) df_grouped.iloc[:, 1:15] = df_grouped_1.iloc[:, 1:15].div(max_values_1, axis=0).round(2) df_grouped.iloc[:, 15:29] = df_grouped_2.iloc[:, 15:29].div(max_values_2, axis=0).round(2) df_grouped.iloc[:, 29:45] = df_grouped_3.iloc[:, 29:45].div(max_values_3, axis=0).round(2) df_grouped.iloc[:, 45:62] = df_grouped_4.iloc[:, 45:62].div(max_values_4, axis=0).round(2)                  - get_feature_importance 분석                                          최종 모델 (행당 스케일링 학습 모델)             각 행당 카테고리 별로 나누어 스케일링 학습 모델                                                                                                     top_1 output_unique: 1336, origin_unique: 2361 correct: 1919, incorrect: 16637 accuracy: 10.34%             top_1 output_unique: 1473, origin_unique: 2361 correct: 1863, incorrect: 16693 accuracy: 10.04%                                   다양성 낮음 정확도 높음             다양성 높음 정확도 낮음                                   feature_importance는 서로 다른 결과가 나왔지만, feature_importance 리스트의 코사인 유사도는 0.97로 유사도가 높게 나왔습니다.  하지만 중요하게 여기는 피쳐들은 달랐고, 한 모델은 좀 더 다양한 값들이 리턴되는 반면 한 모델은 정확도가 조금이나마 우세한 결과가 반환되어 후에 앙상블 실험으로 이어지게 되었습니다.                 실수 했던 부분       max_values = df_grouped.iloc[:, 1:].max(axis=1) # 실수 했던 부분 df_grouped.iloc[:, 1:15] = df_grouped.iloc[:, 1:15].div(max_values, axis=0).round(2) df_grouped.iloc[:, 15:29] = df_grouped.iloc[:, 15:29].div(max_values, axis=0).round(2) df_grouped.iloc[:, 29:45] = df_grouped.iloc[:, 29:45].div(max_values, axis=0).round(2) df_grouped.iloc[:, 45:62] = df_grouped.iloc[:, 45:62].div(max_values, axis=0).round(2)                                                   최종 모델 (행당 스케일링 학습 모델)             데이터 전처리 과정에서 오류가 있었던 모델                                                                                                     초기에 실험하던 과정중에 카테고리 별로 독립적으로 스케일링하려 했지만, 위의 코드상의 에러로 의도하지 않은 데이터셋을 생성하게 되었고, TOP_1의 정확도도 기존의 모델보다 50% 정도가 떨어졌습니다.  당시에는 코드 오류 부분을 놓쳤었고, 해당 원인을 분석하는 도중에 왜 이런 결과가 나오게 되었는 지 생각을 정리하였는데 마냥 지우기에는 배울 점이 남아있는 것 같아 그대로 남겨둡니다.                  카테고리 관련 데이터는 사용자의 상호작용 유무로만 도출된 데이터이기 때문에 피처 중요도가 크게 차이나는 것에 의문을 가지고 분석을 진행 하였습니다.  중요도가 크게 차이나는 특성들이 있을 경우, 모델이 이러한 특성에 지나치게 의존하여 학습할 가능성이 있고, 이로 인해 트레이닝 데이터에는 잘 맞지만, 테스트 데이터에는 잘 일반화되지 않을 수가 있고 오버피팅으로 분류 될 수 있다는 것을 알게 되었습니다.  실제로 성능 측정을 해보았을 때, 같은 테스트 데이터를 집어 넣었을때 최종 모델은 top_1에서 2361개의 레시피중에서 1336가지의 레시피가 추천된 반면에 각 행당 카테고리별로 스케일링을 한 모델은 613가지의 레시피가 추천이 되었습니다.  이 점을 종합하여, 중요도 부분에서 불균형이 있었고, 이 때문에 편향이 나타나게 되어 테스트 성능이 떨어진 것으로 파악하고 있습니다.                위의 내용과 같이 초안을 작성하였지만, 학습 데이터를 재확인하던 중에  max_values로 나누게 된것을 확인하고 코드를 고친후에 다시 실험을 하였습니다. 결과적으로는 기존 모델과 새로 재학습 시킨 모델은 전처리 과정이 달랐어도 정확도가 비슷하게 수렴하였습니다.            05. 평가 Metrics           For each prediction in the “top k list”:                If the target is in the “top k list”, increment “Correct Predictions” by 1.         If the target is not in the “top k list”, increment “Incorrect Predictions” by 1.                 Accuracy Formula:  \\(\\text{Accuracy} = \\frac{\\text{Correct Predictions}}{\\text{Correct Predictions + Incorrect Predictions}} \\times 100\\)       cnt = 0 for y_test_value, top_k_value_list in zip(test_answer, converted_top_k_classes):   if y_test_value in top_k_value_list:     cnt += 1 accuracy = round(cnt / len(test_answer) * 100, 2)       서비스 환경에서는 여러개의 레시피를 추천하게 되고, 결과적으로 모델에서 높은 예측 확률을 가진 값들을 기반으로 레시피를 순서대로 반환하기 때문에 모델 결과 비교를 위해 위의 수식을 사용하여 실험을 했습니다.    06. 모델 앙상블      앙상블 방법을 2가지 방법으로 실험을 했습니다.  같은 순서로 정렬된 데이터셋을 고정 시드로 데이터를 나누고 학습을 시켰기 때문에 모델 클래스가 동일한 순서로 반환이 되었습니다.  그래서 각자의 확률 리스트로 합 연산이 가능했기 때문에 predict 결과를 합해주는 방식으로 앙상블을 진행했습니다.  스케일링 합 연산 앙상블의 경우 한쪽에서 확률이 평균적으로 높게 나와 분포가 균등하지 않을 경우가 있다고 생각해서 가장 큰 값이 1이 되도록 스케일링하여 클래스를 반환하였습니다.            합 연산 앙상블       # 테스트 세트에 대한 예측 확률 계산 probabilities_1 = loaded_model.predict_proba(X_test) probabilities_2 = loaded_model_2.predict_proba(X_test_2) probabilities = np.array(probabilities_1) + np.array(probabilities_2) # 상위 top_k개 클래스 레이블 반환 top_k_classes = probabilities.argsort(axis=1)[:, -TOP_K:].tolist()                   top_1  output_unique: 1412, origin_unique: 2361  correct: 1941, incorrect: 16615  accuracy: 10.46%  top_5  output_unique: 2123, origin_unique: 2361  correct: 4762, incorrect: 13794  accuracy: 25.66%  top_10  output_unique: 2296, origin_unique: 2361  correct: 6390, incorrect: 12166  accuracy: 34.44%                       스케일링 합 연산 앙상블       # 테스트 세트에 대한 예측 확률 계산 probabilities_1 = loaded_model.predict_proba(X_test) probabilities_2 = loaded_model_2.predict_proba(X_test_2) probabilities = np.array(probabilities_1) / np.amax(np.array(probabilities_1), axis=1).reshape(-1, 1) + np.array(probabilities_2) / np.amax(np.array(probabilities_2), axis=1).reshape(-1, 1) # 상위 top_k개 클래스 레이블 반환 top_k_classes = probabilities.argsort(axis=1)[:, -TOP_K:].tolist()                  top_1 output_unique: 1394, origin_unique: 2361 correct: 1946, incorrect: 16610 accuracy: 10.49% top_5 output_unique: 2116, origin_unique: 2361 correct: 4750, incorrect: 13806 accuracy: 25.6% top_10 output_unique: 2293, origin_unique: 2361 correct: 6381, incorrect: 12175 accuracy: 34.39%                  07. Result      Model_1 : 행 단위 스케일링 학습 모델  Model_2 : 행 단위 카테고리 분류별 스케일링 학습 모델  Model_3 : Model_1 + Model_2 합 연산 앙상블 모델  Model_4 : Model_1 + Model_2 스케일링 합 연산 앙상블 모델            TOP_1                                  TOP_1           output_unique           origin_unique           accuracy                                           Model_1           1336           2361           10.34%                             Model_2           1473           2361           10.04%                             Model_3           1412           2361           10.46%                             Model_4           1394           2361           10.49%                                TOP_5                                  TOP_1           output_unique           origin_unique           accuracy                                           Model_1           2096           2361           25.26%                             Model_2           2145           2361           25.27%                             Model_3           2123           2361           25.66%                             Model_4           2116           2361           25.6%                                TOP_10                                  TOP_1           output_unique           origin_unique           accuracy                                           Model_1           2280           2361           34.05%                             Model_2           2303           2361           34.02%                             Model_3           2296           2361           34.44%                             Model_4           2293           2361           34.39%                              Model_2에서는 레시피의 반환된 레시피의 종류가 가장 다양했습니다.  성능상으로는 Model_3가 가장 좋았습니다. 전체적으로 정리하면 Model_1 단일 모델보다는 Model_3 앙상블 모델이 더 다양한 레시피를 추천하는 반면에 성능도 더 높게 나왔습니다.  하지만 Model_3, Model_4 모델은 앙상블 모델이고 연산량도 더 많이 때문에 대규모 트래픽들을 처리해야 된다면, 단일 모델들도 성능상 크게 차이가 없기 때문에, 단일 모델 중에서 선택하여 서비스를 할 것 같습니다.    회고      결과적으로는 catboost 모델은 모델 학습보다는 데이터 처리가 중점이 된 것 같습니다.  마지막 프로젝트를 진행하면서 느낀점은 SOTA 모델과 같이 모델 알고리즘 자체의 성능도 중요하지만 추천시스템의 경우에는 주어진 문제가 어떤 것인지 정확히 이해하는 것이 가장 중요하다고 느꼈습니다.  CV, NLP와 같이 다른 트랙의 캠퍼들을 보았을 때에는 SOTA 모델을 사용하는 것이 성능으로 이어지는 것 같았습니다.  주어진 데이터 특성상 CV는 이미지 파일이 주어져 있고, NLP도 텍스트 데이터가 주어져 전처리 방식이 제한적으로 느껴졌습니다.  하지만 추천시스템의 경우에는 사람들의 로그 데이터에 가까운 데이터를 제공 받아, 문제를 해결하기 위해 데이터를 삭제 할 수도 있고, 합쳐 새로운 데이터를 만들어 내는 과정이 많았습니다.  그에 따라 모델 성능도 학습 데이터에 따라 크게 변화는 과정도 겪었습니다.  전체 과정을 마무리하며 든 생각은 가장 중요한 것은 풀고자하는 문제가 어떤 것인지 정확히 이해하는 것이라는 생각이 들었습니다. 동시에 모델의 예측 속도, 정확도와 같은 요소도 문제 과정에 큰 영향을 주었습니다.    ","categories": ["BoostCamp_AI"],
        "tags": ["Final Project"],
        "url": "/boostcamp_ai/final_project_catboost/",
        "teaser": null
      },{
        "title": "[sklearn] k-means clustering",
        "excerpt":"k-means clustering   1. k-means clustering이란     K-means 클러스터링은 비지도학습(Unsupervised Learning)의 한 종류로, 데이터를 그룹화하는 클러스터를 생성하는 알고리즘입니다. 주어진 데이터를 유사한 특성을 가진 클러스터로 나누는 것이 목표입니다. 예를 들어, 고객 구매 패턴이나 이미지 특성 분석 등 다양한 분야에서 활용됩니다.    2. 작동 방식   K-means 알고리즘은 아래와 같은 단계로 작동   단계 1: 초기 중심점 설정   사용자가 지정한 클러스터의 개수(K)에 따라 랜덤한 중심점을 설정   단계 2: 데이터 할당   각 데이터 포인트를 가장 가까운 중심점에 할당합니다. 이를 통해 각 데이터는 가장 가까운 클러스터에 속하게 됨   단계 3: 중심점 업데이트   각 클러스터에 할당된 데이터 포인트들의 평균을 계산하여 새로운 중심점을 업데이트   단계 4: 반복   위 단계 2와 3을 반복하면서 중심점과 할당을 조정합니다. 더 이상 중심점과 할당이 변경되지 않을 때까지 반복   3. K-means의 장단점   장점:      간단하고 빠른 알고리즘으로 대용량 데이터에도 적용 가능   데이터의 구조를 파악하고 그룹화할 수 있어 데이터 마이닝에 유용   클러스터의 개수(K)를 조정하여 원하는 결과를 얻을 수 있다   단점:      초기 중심점의 설정에 따라 결과가 달라질 수 있다   이상치(outlier)에 민감할 수 있다   클러스터의 모양이 원형이 아닐 경우 성능이 저하될 수 있다   4. 활용 사례   K-means 클러스터링은 다양한 분야에서 활용      고객 분류: 구매 패턴, 성향 등을 기반으로 고객을 그룹화하여 마케팅 전략 수립에 활용할 수 있습니다.   이미지 분류: 이미지 특성을 기반으로 비슷한 특징을 가진 이미지들을 그룹화합니다.   의료 데이터 분석: 환자 데이터를 분석하여 유사한 치료 결과를 가진 환자 그룹을 형성합니다.   5. 정리      K-means 클러스터링은 데이터 그룹화를 통해 데이터의 구조와 특성을 파악하는데 유용한 알고리즘   초기 중심점 설정에 따른 결과 변동이 있을 수 있으며, 데이터의 특성에 따라 조정이 필요한 알고리즘   데이터 마이닝, 고객 분류, 이미지 분류 등 다양한 분야에서 활용될 수 있어 중요한 알고리즘 중 하나로 꼽힘             Baseline   - 예시 데이터 [expanded_df]   MinMaxScaler() 처리한 데이터                          building_number       0       1       2       3       4       …       2034       2035       2036       2037       2038       2039                       0       1       0.0726       0.0638       0.0469       0.0420       0.0496       …       0.4265       0.4064       0.3346       0.2407       0.1430       0.2684                 1       2       0.2761       0.2671       0.2549       0.2517       0.2374       …       0.5766       0.5440       0.4774       0.4278       0.1909       0.3447                 2       3       0.0711       0.0573       0.0566       0.0516       0.0561       …       0.5522       0.6666       0.6451       0.4704       0.1479       0.1038                 …       …       …       …       …       …       …       …       …       …       …       …       …       …                 97       98       0.1315       0.1212       0.1166       0.0984       0.1367       …       0.4937       0.4841       0.4510       0.4010       0.3051       0.1922                 98       99       0.0887       0.0616       0.0465       0.0593       0.0563       …       0.5572       0.5381       0.5228       0.3756       0.2412       0.1638                 99       100       0.0757       0.0402       0.0285       0.0197       0.0210       …       0.7710       0.5440       0.4685       0.4926       0.3224       0.2305           100 rows × 2041 columns   - K-means cluster code   cluster_df = pd.DataFrame(expanded_df) X = cluster_df.drop('building_number', axis=1)  # building_number 열 제외한 데이터  # K-means 모델 생성 및 학습 num_clusters = 4 kmeans = KMeans(n_clusters=num_clusters, n_init=10, random_state=42) clusters = kmeans.fit_predict(X)  # 군집 결과를 데이터프레임에 추가 cluster_df['cluster'] = kmeans.labels_           결과 확인       print(cluster_df['cluster'].value_counts().sort_index())           # result cluster 0    16 1    11 2    32 3    41 Name: count, dtype: int64                실루엣 스코어       # 실루엣 스코어 계산 silhouette_avg = silhouette_score(X, cluster_df['cluster']) print(\"실루엣 스코어:\", silhouette_avg)           실루엣 스코어: 0.32784045970941667                클러스터 시각화       import matplotlib.pyplot as plt import seaborn as sns from sklearn.decomposition import PCA    # 데이터프레임에서 cluster 열을 제외한 데이터 추출 data = cluster_df.drop(['building_number','cluster'], axis=1)    # 주성분 분석을 통해 데이터 2차원으로 축소 pca = PCA(n_components=2) # n_components: 주성분 분석(PCA)에서 생성하려는 주성분의 개수를 지정하는 매개변수 data_2d = pca.fit_transform(data)    # 시각화 plt.figure(figsize=(5, 4)) sns.scatterplot(x=data_2d[:, 0], y=data_2d[:, 1], hue=cluster_df['cluster'], palette='Set1') plt.title('K-means Clustering Visualization') plt.xlabel('PCA 1') plt.ylabel('PCA 2') plt.legend() plt.show()                                   클러스터링 결과 시각화       # 클러스터 별로 subplot 생성 num_clusters = len(cluster_df['cluster'].unique()) fig, axes = plt.subplots(num_clusters, figsize=(30, 6 * num_clusters))    for cluster_num, ax in zip(range(num_clusters), axes):     cluster_data = cluster_df[cluster_df['cluster'] == cluster_num]     cluster_data = cluster_data.drop(['building_number', 'cluster'], axis=1)            for i, row in cluster_data.iterrows():         ax.plot(range(len(row)), row, alpha=0.5)        ax.set_title(f'Cluster {cluster_num} - {list(cluster_df[cluster_df[\"cluster\"] == cluster_num][\"building_number\"])}')     ax.set_xlabel('Index')     ax.set_ylabel('Change')        for i in range(0, len(expanded_df.loc[0].to_list()[1:]), 24):         ax.axvline(x=i, color='red', linestyle='--', linewidth=0.5)    plt.tight_layout() plt.show()                                   최적 매개변수 찾기       import matplotlib.pyplot as plt from sklearn.cluster import KMeans from sklearn.metrics import silhouette_score from sklearn.preprocessing import StandardScaler    # 데이터 준비 cluster_df = pd.DataFrame(expanded_df) X = cluster_df.drop('building_number', axis=1)    # 표준화 [스케일링이 된 데이터라면 생략] # scaler = StandardScaler() # X_scaled = scaler.fit_transform(X)    X_scaled = X # 스케이링이 된 데이터이므로 X 그대로 사용    # 엘보우 메서드를 통한 최적 군집 수 탐색 inertia_values = [] silhouette_scores = [] possible_k_values = range(2, 21)    for k in possible_k_values:     kmeans = KMeans(n_clusters=k, n_init=10,random_state=42)     kmeans.fit(X_scaled)     inertia_values.append(kmeans.inertia_)     silhouette_scores.append(silhouette_score(X_scaled, kmeans.labels_))    # 엘보우 메서드 시각화 plt.figure(figsize=(12, 6)) plt.subplot(1, 2, 1) plt.plot(possible_k_values, inertia_values, marker='o') plt.xlabel('Number of Clusters') plt.ylabel('Inertia') plt.title('Elbow Method - Inertia')    plt.subplot(1, 2, 2) plt.plot(possible_k_values, silhouette_scores, marker='o') plt.xlabel('Number of Clusters') plt.ylabel('Silhouette Score') plt.title('Elbow Method - Silhouette Score')    plt.tight_layout() plt.show()                                     Metrics   - Silhouette Score   실루엣 스코어(Silhouette Score)는 군집화의 품질을 평가하는 지표 중 하나로, 각 데이터 포인트가 자신의 군집 내부의 거리와 다른 군집과의 거리를 얼마나 잘 반영하는지를 나타내는 값  이 스코어는 -1에서 1 사이의 값을 가지며, 높을수록 군집화의 품질이 좋다고 판단   실루엣 스코어는 다음과 같은 방식으로 계산:      개별 데이터 포인트에 대해 군집 내부 평균 거리(a)와 가장 가까운 다른 군집의 평균 거리(b)를 계산합니다. 이 때, a는 해당 포인트가 속한 군집 내의 다른 모든 포인트 간의 평균 거리를 의미하며, b는 해당 포인트와 가장 가까운 다른 군집 내의 모든 포인트 간의 평균 거리를 의미   실루엣 스코어는 (b - a)를 더 큰 값인 b와 나눈 후, max(a, b)와의 차이를 계산   따라서, 스코어가 높을수록 해당 포인트가 자신의 군집 내부의 거리는 가깝고 다른 군집과의 거리는 먼 것이므로 군집화의 품질이 좋다고 볼 수 있다. 반대로, 스코어가 낮을수록 군집화의 품질이 나쁘다고 판단   전체 데이터셋의 실루엣 스코어는 각 데이터 포인트의 실루엣 스코어의 평균으로 계산     - 1에 가까운 값: 클러스터가 잘 구분되어 있음을 나타낸다  - 0에 가까운 값: 클러스터링 결과가 중첩되거나, 클러스터링이 잘 이루어지지 않은 상태를 나타낸다  - 음수 값: 잘못된 클러스터링을 나타낸다    일반적으로 0.5 이상의 스코어는 좋은 군집화 품질을 나타내며, 그 이하의 경우 군집화가 잘못되었거나 데이터가 겹치는 영역이 있을 수 있다   - Inertia   Inertia는 클러스터 내 데이터들과 해당 클러스터의 중심 간의 거리의 합을 나타내는 지표   K-means 알고리즘에서 클러스터 내 데이터들이 중심에 얼마나 가깝게 모여있는지를 나타내는 값  Inertia가 작을수록 클러스터 내 데이터들이 중심에 가깝게 모여있으며, 클러스터링이 잘 이루어졌다고 볼 수 있다   엘보우 메서드를 사용할 때, Inertia는 클러스터 수가 증가함에 따라 계속해서 감소  이는 클러스터 수가 늘어나면 각 클러스터의 중심과의 거리가 더욱 줄어들기 때문이다.   그러나 클러스터 수가 증가하면 모든 데이터가 개별적인 클러스터로 할당될 수 있으므로, Inertia가 0에 가까워질 수 있다.  따라서 적절한 군집 수를 찾기 위해서는 Inertia의 변화를 보면서 기울기가 급격히 감소하는 지점인 ‘엘보우 포인트’를 찾게 되며, 엘보우 포인트에서 Inertia의 변화가 감소하는 속도가 둔해지며, 클러스터 수를 선택하는데 도움을 준다.   Reference     [scikit-learn Clustering] - https://scikit-learn.org/stable/modules/clustering.html  [sklearn.cluster.KMeans] - https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html   + ChatGPT    ","categories": ["AI"],
        "tags": ["Clustering"],
        "url": "/ai/k_means_clustering/",
        "teaser": null
      },{
        "title": "[Setting] Ubuntu Server 22.04.3 LTS 초기 설정",
        "excerpt":"[Setting] Ubuntu Server 22.04.3 LTS 초기 설정   Ubuntu Server 설치           Reference              [Linux] 우분투 Ubuntu server 설치 - https://servermon.tistory.com/268         [[Linux] Ubuntu 20.04 server 설치 과정 - https://heroeswillnotdie.tistory.com/22         두 개의 블로그를 참고하여 설치 - Rufus 유틸을 사용해서 USB 드라이브 만들었습니다            ubuntu-drivers 설치           1. devices 확인       ubuntu-drivers devices                         만약 aplay 관련 error나 warning이 발생했을 때:         # aplay가 존재하는지 확인 which aplay # /usr/bin/aplay가 없을 경우 설치 sudo apt-get install alsa-utils                       2. drivers autoinstall              sudo ubuntu-drivers autoinstall 명령은 Ubuntu에서 사용되는 명령어로, 시스템에 설치된 하드웨어에 가장 잘 맞는 드라이버들을 자동으로 탐지하고 설치하는 기능을 수행         자세한 기능 설명은 다음과 같습니다:                  자동 탐지: ubuntu-drivers 도구는 현재 시스템에 연결된 하드웨어 (특히 그래픽 카드, 네트워크 카드, 사운드 카드 등)를 검사하여 해당 하드웨어에 최적화된 드라이버가 무엇인지 탐지         추천 드라이버 설치: autoinstall 옵션을 사용하면, 시스템에 가장 잘 맞는 드라이버를 자동으로 선택하고 설치  예를 들어, NVIDIA 그래픽 카드가 있는 경우, 가장 적절한 NVIDIA 드라이버 버전을 자동으로 선택하여 설치합니다.         의존성 관리: 드라이버를 설치할 때 필요한 의존성 패키지들도 자동으로 함께 설치         커널 모듈 및 서비스 관리: 드라이버 설치 후 필요한 커널 모듈이나 관련 서비스가 있다면, 이를 자동으로 로드하거나 시작                  sudo ubuntu-drivers autoinstall           (선택) apt 저장소 변경      설치 과정에서 Configure Ubuntu crchive mirror 페이지에서 별도로 주소를 설정하지 않았다면 kr.archive.ubuntu.com로 되어 있을 수 있다     mirror.kakao.com으로 변경하면 좀 더 빠르게 apt 설치를 할 수 있다    sudo vi /etc/apt/sources.list   :%s/kr.archive.ubuntu.com/mirror.kakao.com/      vi 문자열 변경 기능 사용 - 변경후 :wq [저장 후 나가기]               변경 확인       sudo apt-get update                  시스템 시간 변경      Ubuntu Server를 설치 할 때 English로 설치하면서 한국시간이 아닌 다른 나라 시간으로 잡혀있을 때 설정            1. 현재 시스템의 날짜 및 시간 정보 확인:       date                  timedatectl                       2. 시간대를 한국 시간으로 설정:       sudo timedatectl set-timezone Asia/Seoul                3. 변경 사항 확인:             ","categories": ["Ubuntu_Server"],
        "tags": ["Ubuntu Server","Setting"],
        "url": "/ubuntu_server/ubuntu/",
        "teaser": null
      },{
        "title": "[Setting] Ubuntu Server 22.04.3 LTS Wifi 설정 & Static IP",
        "excerpt":"[Setting] Ubuntu 22.04.3 LTS Wifi 설정 &amp; Static IP      netplan은 Ubuntu 18.04 버전부터 도입된 네트워크 설정 도구로, 여러 네트워크 설정을 YAML 파일 형식으로 정의하고 적용할 수 있게 해줍니다.            시스템에 있는 모든 네트워크 인터페이스의 정보 확인       ls /sys/class/net/                       Netplan 설정 파일 위치 확인              기본적으로 netplan 설정 파일은 /etc/netplan/ 디렉토리에 위치합니다. 이 디렉토리를 확인해보면 기본 설정 파일을 볼 수 있습니다.            ls /etc/netplan/                       Netplan 설정 파일 수정       sudo nano /etc/netplan/파일이름.yaml                YAML 형식으로 네트워크 설정              기본적인 정적 IP 설정 예제            # This is the network config written by 'subiquity' network:   version: 2   wifis:     wlp6s0: # wlan device 이름 입력       addresses:         - 192.168.0.100/24 # 고정 ip 주소 입력       routes:         - to: 0.0.0.0/0           via: 192.168.0.1 # gateway 주소 입력       nameservers:         addresses: [8.8.8.8, 8.8.4.4]       access-points:         test_wifi: # wifi 이름 입력           password: 00000000 # wifi 비밀 번호 입력       dhcp4: true                  띄어 쓰기 주의 : tab하고 스페이스가 섞이면 구문 에러 발생                 Netplan 설정 적용       sudo netplan apply                네트워크 설정 확인              ip a 또는 ip route 명령을 사용하여 네트워크 설정을 확인            ","categories": ["Ubuntu_Server"],
        "tags": ["Ubuntu Server","Setting"],
        "url": "/ubuntu_server/wifi_setting_static_ip/",
        "teaser": null
      },{
        "title": "[Troubleshooting] PasswordAuthentication no not work",
        "excerpt":"[Troubleshooting] PasswordAuthentication no not work      SSH(Secure Shell)는 원격으로 시스템에 접속하고 관리하는 데 사용되는 보안 프로토콜  “PasswordAuthentication no”라는 옵션은 SSH 서버 구성에서 중요한 부분으로, 이를 통해 비밀번호를 사용한 인증을 비활성화하는 것이 가능  설정하는 과정에서 PasswordAuthentication no를 추가해도 작동하지 않았고 이에 따른 해결 과정 기록            sshd_config 파일 열기       sudo vi /etc/ssh/sshd_config                PasswordAuthentication no 작성                   재시작       sudo service sshd restart           Troubleshooting      재시작을 완료 했는 데도 password로 계속 접속이 되는 현상 발생            폴더 확인       ls /etc/ssh/sshd_config.d           &gt; 50-cloud-init.conf                파일 수정       sudo vi /etc/ssh/sshd_config.d/50-cloud-init.conf                         yes가 되어 있는 것을 확인 no로 수정 후 저장 [:wq]                 재시작       sudo service sshd restart           Result         정확한 원인은 sshd_config 안에서 /sshd_config.d 폴더 안의 conf 파일들이 Include되는 과정에서 PasswordAuthentication yes가 먼저 선언 되었기 때문이였습니다.    ","categories": ["Ubuntu_Server"],
        "tags": ["Ubuntu Server","Troubleshooting"],
        "url": "/ubuntu_server/PasswordAuthentication/",
        "teaser": null
      },{
        "title": "[Troubleshooting] ubuntu server에서 wifi 인식이 안될 때",
        "excerpt":"[Troubleshooting] ubuntu server에서 wifi 인식이 안될 때      네트워크 연결하는 과정에서 랜카드는 인식이 되었는데 wlan이 인식이 안되는 문제가 발생했었습니다  해결하기 위해서 아래 그림처럼 우분투 미러 서버에서 직접 네트워크 관련 패키지들을 수동으로 설치 하여 문제 해결을 해보려 했지만 해결이 되지 않았습니다         - 해결 방법      윈도우와 메인보드의 빠른 부팅 설정을 해제하니 문제가 해결 되었습니다    1. 메인보드 바이오스 설정에서 빠른 부팅 설정 끄기   제조사 별로 바이오스 화면이 다르지만 바이오스 설정 진입 후 부팅 메뉴 - Fast Boost[빠른 부팅] 설정을 비활성화 합니다   2. 윈도우 전원 관리 빠른 부팅 설정 끄기           윈도우 + R로 “실행”창을 켠 후에 powercfg.cpl을 입력합니다:                   전원 단추 작동 설정으로 이동합니다:                   현재 사용할 수 있는 설정 변경을 클릭하여, 빠른 시작을 끄로 변경 내용 저장을 합니다:                   Reference - https://askubuntu.com/questions/1234118/wifi-adapter-not-found-in-ubuntu-20-04          Ubuntu와 Windows 10을 설치하면 Windows 시스템이 Wi-Fi 카드를 차단한다고 합니다,   ","categories": ["Ubuntu_Server"],
        "tags": ["Ubuntu Server","Troubleshooting"],
        "url": "/ubuntu_server/fast_booting/",
        "teaser": null
      },{
        "title": "[Setting] python / pyenv 설치",
        "excerpt":"[Setting] python / pyenv 설치   - python 설치      Ubuntu 서버는 기본적으로 Python을 포함하고 있지만, 최신 버전을 설치하거나 확인하려면 아래 과정을 거치면 됩니다.            python 버전 확인:       python3 --version                  sudo apt update sudo apt install python-is-python3 python --version               python-is-python3 패키지를 설치하여 python 명령어를 python3로 연결할 수 있습니다.                 패키지 관리자를 사용한 기본 Python 설치:       sudo apt update sudo apt install python3           - pip 설치   sudo apt-get install python3-pip   - pyenv 설치      pyenv는 여러 버전의 파이썬을 간편하게 설치하고 관리할 수 있는 툴입니다.            패키지의 최신 목록 업데이트 및 사전 준비       sudo apt update # 필요한 의존성 및 라이브러리를 설치 # pyenv를 사용하여 파이썬을 컴파일할 때 필요한 패키지들이 포함 sudo apt install -y build-essential libssl-dev zlib1g-dev libbz2-dev libreadline-dev libsqlite3-dev wget curl llvm libncurses5-dev libncursesw5-dev xz-utils tk-dev libffi-dev liblzma-dev python3-openssl git                pyenv 설치       curl https://pyenv.run | bash           WARNING: seems you still have not added 'pyenv' to the load path. # Load pyenv automatically by appending # the following to  ~/.bash_profile if it exists, otherwise ~/.profile (for login shells) and ~/.bashrc (for interactive shells) :    export PYENV_ROOT=\"$HOME/.pyenv\" command -v pyenv &gt;/dev/null || export PATH=\"$PYENV_ROOT/bin:$PATH\" eval \"$(pyenv init -)\"    # Restart your shell for the changes to take effect.    # Load pyenv-virtualenv automatically by adding # the following to ~/.bashrc:    eval \"$(pyenv virtualenv-init -)\"                  설치를 완료하면 pyenv와 pyenv-virtualenv가 정상적으로 작동하기 위해 필요한 환경 변수와 초기화 스크립트를 쉘 설정 파일에 추가하는 방법 안내 메세지가 출력됩니다.                 pyenv 환경 변수 설정       - vi 편집기로 추가       vi ~/.bashrc # or (~/.bash_profile, ~/.zshrc)    # ~/.bashrc 파일에 아래의 명령어 추가 export PYENV_ROOT=\"$HOME/.pyenv\" command -v pyenv &gt;/dev/null || export PATH=\"$PYENV_ROOT/bin:$PATH\" eval \"$(pyenv init -)\" eval \"$(pyenv virtualenv-init -)\"       # 위의 명령어를 ~/.bashrc에 입력 후 저장 source ~/.bashrc # or ( source ~/.bash_profile, source ~/.zshrc) # shell 재실행 exec $SHELL           - echo로 한번에 추가 [~/.bashrc]         echo '# ---------- pyenv setting ---------- #' &gt;&gt; ~/.bashrc   echo 'export PYENV_ROOT=\"$HOME/.pyenv\"' &gt;&gt; ~/.bashrc   echo 'command -v pyenv &gt;/dev/null || export PATH=\"$PYENV_ROOT/bin:$PATH\"' &gt;&gt; ~/.bashrc   echo 'eval \"$(pyenv init -)\"' &gt;&gt; ~/.bashrc   echo 'eval \"$(pyenv virtualenv-init -)\"' &gt;&gt; ~/.bashrc   echo '# ----------------------------------- #' &gt;&gt; ~/.bashrc   source ~/.bashrc   exec $SHELL                pyenv 버전확인       pyenv --version                  정상적으로 pyenv의 버전이 출력된다면 설치가 완료된 것입니다.            - pyenv 설정 및 가상환경 생성     이동하기 - [Pyenv] pyenv 가상환경 생성   ","categories": ["Ubuntu_Server"],
        "tags": ["Ubuntu Server","Setting"],
        "url": "/ubuntu_server/python_pyenv_setting/",
        "teaser": null
      },{
        "title": "[Pyenv] pyenv 가상환경 생성",
        "excerpt":"[Pyenv] pyenv 가상환경 생성      pyenv 설치 후 진행하는 환경 설정입니다     설치 방법 - [Setting] python / pyenv 설치    - 설치된 python 버전 확인   pyenv versions      pyenv를 통해 설치 한 파이썬이 없다면 * system (set by ...)으로 한 줄만 출력됩니다    - 설치 가능 python 버전 확인   pyenv install --list      설치 가능한 파이썬 버전의 리스트가 출력됩니다  원하는 버전이 있다면 확인후에 아래 절차대로 진행합니다    - python 설치   # pyenv install &lt;version&gt; pyenv install 3.11      pyenv install &lt;버전&gt;을 입력하면 가장 최신의 3.11.x 버전을 설치합니다    - pyenv 가상 환경 생성           현재 설치된 파이썬 버전 확인        pyenv versions  # example output  &gt; * system (set by /home/xxx/.pyenv/version)  &gt; 3.11.4                virtualenv 생성       # pyenv virtualenv &lt;version&gt; &lt;name&gt; pyenv virtualenv 3.11.4 myenv           - pyenv 주요 명령어           시스템 전체의 파이썬 버전 설정:       pyenv global &lt;PYTHON_VERSION&gt;                현재 디렉토리의 파이썬 버전 설정 (로컬 설정):       pyenv local &lt;PYTHON_VERSION&gt;                설치된 파이썬 버전 삭제:       pyenv uninstall &lt;PYTHON_VERSION&gt;                현재 사용 중인 파이썬 버전 확인:       pyenv version           - pyenv-virtualenv 관련 명령어           가상환경 활성화 / 비활성화:       # pyenv activate &lt;name&gt; pyenv activate myenv           pyenv deactivate                설치된 가상 환경 목록 확인:       pyenv virtualenvs                가상 환경 제거:       # pyenv uninstall &lt;NAME&gt; pyenv uninstall myenv          ","categories": ["DevTools"],
        "tags": ["DevTools","Pyenv"],
        "url": "/devtools/pyenv_setting/",
        "teaser": null
      },{
        "title": "[sklearn] cosine_similarity",
        "excerpt":"cosine_similarity [코사인 유사도]             Cosine Similarity [코사인 유사도]              코사인 유사도는 벡터 간의 유사성을 측정하는 방법 중 하나로 두 벡터 간의 각도에 기반하여 유사성을 측정하며, 벡터가 얼마나 비슷한 방향을 가지는지를 판단합니다.    그림 - https://ko.wikipedia.org/wiki/코사인_유사도                 Euclidean distance [유클리드 거리]              유클리드 거리(Euclidean Distance)는 유사성 측정 방법 중 하나로 사용자 간 또는 아이템 간의 거리를 기반으로 유사성을 측정하는 방법입니다.    그림 - https://ko.wikipedia.org/wiki/유클리드거리              01. CF에서의 Cosine Similarity      Collaborative Filtering을 통한 추천 시스템을 구현할 때, 여러 유사성 메트릭 중 코사인 유사도를 통해서 구현하였습니다.  먼저 Collaborative Filtering이란 :          Collaborative Filtering은 사용자의 행동 패턴을 기반으로 추천을 수행하는 기술입니다.     주로 사용자-아이템 행렬(User-Item Matrix)을 기반으로 작동합니다. 이 행렬은 사용자와 아이템 간의 상호 작용을 표현한 것으로, 보통 사용자가 아이템에 대한 평가 또는 구매 등의 활동을 나타냅니다.       그래서 Collaborative Filtering에서 왜 코사인 유사도를 메트릭으로 사용 했는지에 대해서 공부한 내용입니다.            코사인 유사도를 사용한 이유                     코사인 유사도는 계산 과정에서 두 벡터 간의 각도에 기반하여 유사성을 측정하게 됩니다. 그래서 코사인 유사도를 통해서는 각도가 더 유사한 C_1을 유사하다고 판단하게 됩니다.  반면에 유클리드 거리 기반 유사도는 거리를 기반으로 유사도를 측정하기 때문에 apple이 더 많은 빈도를 보이지만 거리 상으로 가까운 banana banana를 더 유사하다고 판단하게 됩니다.  그래서 유클리드 거리는 벡터의 크기에 민감하게 되고 스케일이 다른 특성들을 가진 데이터에서는 유클리드 거리를 사용하는 것이 적절하지 않게 됩니다.  또한 이런 특성으로 인해 CF에서는 사용자-아이템 행렬이 희소한 경우가 많은데, 유클리드 거리를 사용하면 희소한 데이터에 대한 정확한 유사성 측정이 어려지게 됩니다.  그래서 두 벡터 간 크기가 아닌 각도로 유사성을 판단하는 코사인 유사도가 좋은 성능을 보이는 경향이 있습니다.            02. 코사인 유사도 계산      MovieLens small 데이터를 사용하여 진행하였습니다  link - https://grouplens.org/datasets/movielens/latest/    TRAIN_PATH = \"/home/brother_gyu/dev/movie_rec/data\" RATING_FILE_NAME = \"ratings.csv\" ratings_df = pd.read_csv(os.path.join(TRAIN_PATH, RATING_FILE_NAME)) pivot_df = ratings_df.pivot_table(index='userId', columns='movieId', values='rating', fill_value=0)           02_01 sklearn 라이브러리 사용   from sklearn.metrics.pairwise import cosine_similarity user_similarity = cosine_similarity(pivot_df) pd.DataFrame(user_similarity)         time: 0.0388s  sklearn 라이브러리를 활용하여 간단하게 구현 할 수 있습니다.    # sklearn 라이브러리의 cosine_similarity 함수 def cosine_similarity(X, Y=None, dense_output=True):     # 입력 데이터 X와 Y가 유효한 pairwise 배열인지 확인     X, Y = check_pairwise_arrays(X, Y) \t   \t# 입력 데이터를 정규화     X_normalized = normalize(X, copy=True)     if X is Y:         Y_normalized = X_normalized     else:         Y_normalized = normalize(Y, copy=True) \t\t# safe_sparse_dot을 사용하여 코사인 유사도를 계산     # X_normalized @ Y_normalized.T     K = safe_sparse_dot(X_normalized, Y_normalized.T, dense_output=dense_output)      return K      sklearn 라이브러리의 cosine_similarity 함수는 위와 같이 구성되어 있습니다.  다양한 데이터 형식에 대응할수 있게 작성되어 있습니다.            코드 나누어서 파악하기       # check_pairwise_arrays() X, Y = check_pairwise_arrays(np.array([[1,2,3]]), np.array([[4,5,6]]))    # output X, Y &gt; (array([[1., 2., 3.]]), array([[4., 5., 6.]]))                  유효한 pairwise 배열인지 확인을 하게 됩니다.  만약 유효하지 않다면 ValueError: Expected 2D array, got 1D array instead: array=[4. 5. 6.].과 같은 ValueError를 일으킵니다.            # normalize() X_normalized = normalize(X, copy=True) if X is Y:     Y_normalized = X_normalized else:     Y_normalized = normalize(Y, copy=True)        # output X_normalized, Y_normalized &gt; (array([[0.26726124, 0.53452248, 0.80178373]]),    array([[0.45584231, 0.56980288, 0.68376346]]))                  normalize()를 통해 정규화를 하게 됩니다.  추가로 조건문에서 X와 Y가 같다면 X_normalized 값을 Y_normalized로 선언하면서 시간을 단축하고자 한 것을 확인 할 수 있습니다.            # safe_sparse_dot() K = safe_sparse_dot(X_normalized, Y_normalized.T, dense_output=dense_output)    # output K &gt; array([[0.97463185]])                  safe_sparse_dot()은 행렬 곱셈을 처리하는 유틸리티 함수입니다.        X_normalized @ Y_normalized.T # output &gt; array([[0.97463185]])               위 코드와 같이 행렬 곱을 반환합니다. 하지만 safe_sparse_dot() 함수는 dense_output 매개변수를 통해 희소 행렬과 밀집 행렬 간의 곱셈을 효율적으로 처리하고, 밀집 배열과 밀집 배열 간의 곱셈도 처리할 수 있습니다.            02_02 코사인 유사도 함수 구현   import numpy as np from collections import defaultdict  def cosine_similarity_numpy(vector1, vector2):     dot_product = np.dot(vector1, vector2)     norm_vector1 = np.linalg.norm(vector1)     norm_vector2 = np.linalg.norm(vector2)     similarity = dot_product / (norm_vector1 * norm_vector2)     return similarity  pivot_matrix = pivot_df.to_numpy()  user_similarity_dict = defaultdict(list) for idx, x in enumerate(pivot_matrix):     for y in pivot_matrix:         user_similarity_dict[idx].append(cosine_similarity_numpy(x,y))         time: 13.0641s  코사인 유사도 공식을 함수로 만들어 유사도 행렬을 만드는 코드입니다.  결과는 sklearn 라이브러리의 코사인 유사도 행렬과 동일한 것을 확인할 수 있었지만, 400배에 가까운 시간이 더 소요되는 것을 확인할 수 있었습니다.    02_03 함수 시간 개선   def cosine_similarity_numpy_fast(X):   \t# 유사도 행렬 계산     user_similarity_matrix = np.dot(X, X.T)     # 각 사용자의 벡터 길이(norm) 계산     norms = np.linalg.norm(X, axis=1)     # 벡터 길이(norm)를 외적하여 벡터 길이(norm) 행렬 생성     norms_matrix = np.outer(norms, norms)     # 코사인 유사도 행렬을 각 벡터 길이(norm) 행렬로 나눠서 정규화     user_similarity_matrix /= norms_matrix     return user_similarity_matrix    similarity_matrix = cosine_similarity_numpy_fast(pivot_matrix)  # output pd.DataFrame(similarity_matrix)         time: 0.0291s  행렬 연산으로 처리하면 sklearn 라이브러리처럼 빠른 속도로 처리할 수 있었습니다.    마치며      유클리드 거리와 코사인 유사도만 해도 같은 유사도 측정 메트릭이지만 정반대의 결과를 보였습니다.  데이터의 형태와 목적에 따라 적절한 메트릭을 판단하는 능력을 기르는 것이 중요하다고 느껴졌고, 유사도 함수를 구현해도 단순 공식대로 구현하는 것보다 행렬 연산을 활용하여 구성하는 것이 처리 속도면에서도 크게 차이가 났습니다.  메트릭 수식을 행렬 연산을 결합하는 것이 쉽지는 않지만 노력해야 되는 부분인것 같고, 평소에 별다른 생각 없이 사용만 하던 sklearn 코사인 유사도 코드를 파악해볼 수 있는 좋은 기회였습니다.   ","categories": ["AI"],
        "tags": ["similarity"],
        "url": "/ai/cosine_similarity/",
        "teaser": null
      },{
        "title": "[Desktop Setting] Ram Overclock",
        "excerpt":"[Desktop Setting] Ram Overclock      램 오버클럭 수치 기록    - Hardware      Mainboard - ASUS TUF GAMING B550-PLUS (WI-FI)     CPU - AMD Ryzen 5 5600X 6-Core Processor [Stepping: B0]     Ram - Samsung DDR4 16GB 3200 [2Rx8 PC4 C다이]    - TestMem5 v0.12 Advanced 5 - 10주기 통과                          설정값                       Memory Frequency       DDR4-3600Mhz                 FCLK Frequency       1800Mhz                 DRAM CAS# Latency       18                 Trcdrd       21                 Trcdwr       21                 DRAM RAS# PRE Time       21                 DRAM RAS# ACT Time       44                 Command Rate       1T                 DRAM Voltage       1.35           ","categories": ["ETC"],
        "tags": ["Desktop Setting"],
        "url": "/etc/ram_overclock/",
        "teaser": null
      },{
        "title": "[Util] 알면 편한 유틸리티들",
        "excerpt":"[Util] 알면 편한 유틸리티들   - htop        top 명령어보다 더 직관적인 인터페이스로 시스템의 프로세스와 리소스 사용량을 모니터링            설치       sudo apt-get install htop                사용법       htop           - tumx        터미널 세션을 관리하고 여러 윈도우와 패널을 사용할 수 있게 해주는 터미널 멀티플렉서 터미널 다중 작업을 지원            설치       sudo apt-get install tmux                사용법              블로그 정리 - [linux] tmux - terminal multiplexer            - tree        우분투와 다른 리눅스 배포판에서 사용할 수 있는 명령행 유틸리티로, 디렉토리와 그 하위의 디렉토리 및 파일들을 트리 구조로 시각적으로 출력       설치     sudo apt-get install tree                사용법       tree or tree [디렉토리 경로]                출력 결과       . └── catboost_recipe_classifier     ├── config.yaml     ├── data     │   ├── category_onehot.csv     │   └── train_data.csv     ├── model     │   └── catboost_model.bin     ├── notebook     │   ├── catboost_test_prototype.ipynb     │   └── eda.ipynb     ├── predict.py     ├── README.md     ├── requirements.txt     └── train.py    4 directories, 10 files           ","categories": ["Ubuntu_Server"],
        "tags": ["Ubuntu Server","Util"],
        "url": "/ubuntu_server/userful_util/",
        "teaser": null
      },{
        "title": "[Setting] cuda/cudnn 설치하기",
        "excerpt":"[Setting] cuda/cudnn 설치하기      Ubuntu 환경에서 cuda/cudnn 설치하는 법을 다룹니다.    - CUDA 설치           사전 준비:       sudo apt update sudo apt upgrade                NVIDIA 드라이버 설치:              NVIDIA GPU 드라이버가 설치되어 있지 않다면, Ubuntu의 기본 리포지토리를 사용하여 설치 가능         nvidia-smi 명령어가 작동한다면 3. CUDA 설치로 바로 이동하면 됩니다.            # sudo apt install nvidia-driver-xxx sudo apt install nvidia-driver-535  # 혹은 최신 버전으로 or sudo ubuntu-drivers autoinstall # 자동 설치           - 재부팅:       sudo reboot           - 설치된 드라이버 확인:       nvidia-smi                CUDA 설치:              CUDA Toolkit - https://developer.nvidia.com/cuda-downloads                            첫 페이지                                   설치 환경에 맞추어서 선택                                   해당 base installer의 내용을 복사후 터미널에 입력                                   환경변수 설정:              .bashrc나 .zshrc 파일에 아래 내용을 추가            echo '# ---------- cuda setting ---------- #' &gt;&gt; ~/.bashrc # or ~/.zshrc echo 'export PATH=/usr/local/cuda/bin:$PATH' &gt;&gt; ~/.bashrc # or ~/.zshrc echo 'export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH' &gt;&gt; ~/.bashrc # or ~/.zshrc echo '# ----------------------------------- #' &gt;&gt; ~/.bashrc # or ~/.zshrc source ~/.bashrc # or ~/.zshrc exec $SHELL                설치 확인:       nvcc --version           # example &gt; nvcc: NVIDIA (R) Cuda compiler driver &gt; Copyright (c) 2005-2023 NVIDIA Corporation &gt; Built on Tue_Jul_11_02:20:44_PDT_2023 &gt; Cuda compilation tools, release 12.2, V12.2.128 &gt; Build cuda_12.2.r12.2/compiler.33053471_0           - cuDNN 설치           파일 다운로드:              cuDNN Download - https://developer.nvidia.com/rdp/cudnn-download  환경에 맞는 파일을 다운합니다                        cuDNN 설치:       # sudo chown _apt: &lt;file_path&gt; # sudo apt install &lt;codnn-*.deb file&gt; sudo chown _apt: /home/brother_gyu/dev/cudnn-local-repo-ubuntu2204-8.9.4.25_1.0-1_amd64.deb sudo apt install ./cudnn-local-repo-ubuntu2204-8.9.4.25_1.0-1_amd64.deb                       sudo apt update 에러 발생할 경우:              # ls /var/cudnn-local-repo-ubuntuxxxx-x.x.x.xx/ ls /var/cudnn-local-repo-ubuntu2204-8.9.4.25/                  ls /usr/share/keyrings/                        ls /var/cudnn-local-repo-ubuntuxxxx-x.x.x.xx/ 폴더 내의 cudnn-local-*-keyring.gpg 파일을 /usr/share/keyrings/에 넣어줘야 합니다.            # sudo cp /var/cudnn-local-repo-ubuntuxxxx-x.x.x.xx/cudnn-local-xxxxxxxx-keyring.gpg /usr/share/keyrings/ sudo cp /var/cudnn-local-repo-ubuntu2204-8.9.4.25/cudnn-local-3C3A81D3-keyring.gpg /usr/share/keyrings/                           이제 해결된 것을 확인할 수 있습니다.                 라이브러리 설치       # sudo apt-get install libcudnn8=8.x.x.x-1+cudaX.Y # sudo apt-get install libcudnn8-dev=8.x.x.x-1+cudaX.Y # sudo apt-get install libcudnn8-samples=8.x.x.x-1+cudaX.Y     sudo apt-get install libcudnn8=8.9.4.25-1+cuda12.2 sudo apt-get install libcudnn8-dev=8.9.4.25-1+cuda12.2 sudo apt-get install libcudnn8-samples=8.9.4.25-1+cuda12.2             - 정상 설치 테스트           pytorch 설치       pip install torch torchvision torchaudio                test 코드       import torch print(torch.cuda.is_available()) # True t = torch.rand(100, 100).cuda() print(t.device) # cuda:*           - 예시 결과       (dev_3.11.4) brother_gyu@rtx4060:~/dev$ python Python 3.11.4 (main, Aug 25 2023, 00:30:45) [GCC 11.4.0] on linux Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. &gt;&gt;&gt; import torch &gt;&gt;&gt; print(torch.cuda.is_available()) True &gt;&gt;&gt; t = torch.rand(100, 100).cuda() &gt;&gt;&gt; print(t.device) cuda:0           Reference        cuDNN 설치법 - https://docs.nvidia.com/deeplearning/cudnn/install-guide/index.html#installlinux-deb     cuda_install.md [unofficial] - https://gist.github.com/denguir/b21aa66ae7fb1089655dd9de8351a202    ","categories": ["Ubuntu_Server"],
        "tags": ["Ubuntu Server","Setting"],
        "url": "/ubuntu_server/cuda_install/",
        "teaser": null
      }]
